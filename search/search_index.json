{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ShapleyX Documentation Welcome to the ShapleyX documentation! ShapleyX is a Python package for global sensitivity analysis using Sparse Random Sampling - High Dimensional Model Representation (HDMR) with Group Method of Data Handling (GMDH) for parameter selection and linear regression for parameter refinement. Documentation Sections Getting Started : Installation and basic setup Tutorials : Step-by-step guides for common tasks How-to Guides : Solutions to specific problems Reference : Complete API documentation Explanation : Background and theory import shapleyx # Initialize RS-HDMR analyzer analyzer = shapleyx . rshdmr ( data_file = 'input_data.csv' , polys = [ 10 , 5 ], method = 'ard' ) # Run the entire analysis pipeline sobol_indices , shapley_effects , total_index = analyzer . run_all ()","title":"Home"},{"location":"#shapleyx-documentation","text":"Welcome to the ShapleyX documentation! ShapleyX is a Python package for global sensitivity analysis using Sparse Random Sampling - High Dimensional Model Representation (HDMR) with Group Method of Data Handling (GMDH) for parameter selection and linear regression for parameter refinement.","title":"ShapleyX Documentation"},{"location":"#documentation-sections","text":"Getting Started : Installation and basic setup Tutorials : Step-by-step guides for common tasks How-to Guides : Solutions to specific problems Reference : Complete API documentation Explanation : Background and theory import shapleyx # Initialize RS-HDMR analyzer analyzer = shapleyx . rshdmr ( data_file = 'input_data.csv' , polys = [ 10 , 5 ], method = 'ard' ) # Run the entire analysis pipeline sobol_indices , shapley_effects , total_index = analyzer . run_all ()","title":"Documentation Sections"},{"location":"explanation/theory/","text":"Theoretical Background Shapley Values in Sensitivity Analysis Shapley values originate from cooperative game theory and provide a principled way to: Fairly distribute the \"payout\" (output variance) among \"players\" (input parameters) Account for all possible interaction effects Provide unique attribution under certain axioms Mathematical Formulation For a model output \\( Y = f(X_1, X_2, \\dots, X_d)\\) , the Shapley effect \\(\\phi_i\\) for parameter \\(X_i\\) is: \\[ \\phi_i = \\sum_{S \\subseteq D \\setminus \\{i\\}} \\frac{|S|!(d-|S|-1)!}{d!} \\left[\\text{Var}\\big(E[Y|X_S \\cup \\{i\\}]\\big) - \\text{Var}\\big(E[Y|X_S]\\big)\\right] \\] where: - \\(D\\) is the set of all parameters - \\(S\\) is a subset of parameters excluding \\(i\\) - \\(X_S\\) represents the parameters in subset \\(S\\) For a model output $$ Y = f(X_1, X_2, \\ldots, X_d) $$, the Shapley effect \\( \\(\\phi_i\\) \\) for parameter \\( \\(X_i\\) \\) is: \\[ \\phi_i = \\sum_{S \\subseteq D \\setminus \\{i\\}} \\frac{|S|!(d-|S|-1)!}{d!} [\\text{Var}(E[Y|X_S \\cup \\{i\\}]) - \\text{Var}(E[Y|X_S])] \\] where: - \\(D\\) is the set of all parameters - \\(S\\) is a subset of parameters excluding \\(i\\) - \\(X_S\\) represents the parameters in subset \\(S\\) Relationship to Sobol Indices Shapley effects generalize Sobol indices by: - Combining all order effects involving a parameter - Providing a complete decomposition where: - \\(\\sum_{i=1}^d \\phi_i = \\text{Var}(Y)\\) - Each \\(\\phi_i \\geq 0\\) Advantages Complete Attribution : Accounts for all interactions Additivity : Effects sum to total variance Interpretability : Direct measure of importance Robustness : Works well with correlated inputs Implementation in ShapleyX The package uses: - Polynomial chaos expansions for efficient computation - Automatic Relevance Determination (ARD) for robust estimation - Legendre polynomials for orthogonal basis functions","title":"Theory"},{"location":"explanation/theory/#theoretical-background","text":"","title":"Theoretical Background"},{"location":"explanation/theory/#shapley-values-in-sensitivity-analysis","text":"Shapley values originate from cooperative game theory and provide a principled way to: Fairly distribute the \"payout\" (output variance) among \"players\" (input parameters) Account for all possible interaction effects Provide unique attribution under certain axioms","title":"Shapley Values in Sensitivity Analysis"},{"location":"explanation/theory/#mathematical-formulation","text":"For a model output \\( Y = f(X_1, X_2, \\dots, X_d)\\) , the Shapley effect \\(\\phi_i\\) for parameter \\(X_i\\) is: \\[ \\phi_i = \\sum_{S \\subseteq D \\setminus \\{i\\}} \\frac{|S|!(d-|S|-1)!}{d!} \\left[\\text{Var}\\big(E[Y|X_S \\cup \\{i\\}]\\big) - \\text{Var}\\big(E[Y|X_S]\\big)\\right] \\] where: - \\(D\\) is the set of all parameters - \\(S\\) is a subset of parameters excluding \\(i\\) - \\(X_S\\) represents the parameters in subset \\(S\\) For a model output $$ Y = f(X_1, X_2, \\ldots, X_d) $$, the Shapley effect \\( \\(\\phi_i\\) \\) for parameter \\( \\(X_i\\) \\) is: \\[ \\phi_i = \\sum_{S \\subseteq D \\setminus \\{i\\}} \\frac{|S|!(d-|S|-1)!}{d!} [\\text{Var}(E[Y|X_S \\cup \\{i\\}]) - \\text{Var}(E[Y|X_S])] \\] where: - \\(D\\) is the set of all parameters - \\(S\\) is a subset of parameters excluding \\(i\\) - \\(X_S\\) represents the parameters in subset \\(S\\)","title":"Mathematical Formulation"},{"location":"explanation/theory/#relationship-to-sobol-indices","text":"Shapley effects generalize Sobol indices by: - Combining all order effects involving a parameter - Providing a complete decomposition where: - \\(\\sum_{i=1}^d \\phi_i = \\text{Var}(Y)\\) - Each \\(\\phi_i \\geq 0\\)","title":"Relationship to Sobol Indices"},{"location":"explanation/theory/#advantages","text":"Complete Attribution : Accounts for all interactions Additivity : Effects sum to total variance Interpretability : Direct measure of importance Robustness : Works well with correlated inputs","title":"Advantages"},{"location":"explanation/theory/#implementation-in-shapleyx","text":"The package uses: - Polynomial chaos expansions for efficient computation - Automatic Relevance Determination (ARD) for robust estimation - Legendre polynomials for orthogonal basis functions","title":"Implementation in ShapleyX"},{"location":"getting-started/deployment/","text":"Deployment Guide Automated Deployment (Recommended) Push changes to GitHub The workflow in .github/workflows/gh-pages.yml will automatically: Build the documentation Deploy to the gh-pages branch Make it available at https://[your-username].github.io/shapleyx/ Manual Deployment (Fallback) If automated deployment fails: Install requirements: pip install mkdocs-material mkdocstrings [ python ] Build and deploy: mkdocs gh-deploy --force If permission errors persist: git config --global user.name \"Your Name\" git config --global user.email \"your@email.com\" mkdocs gh-deploy --force --remote-branch gh-pages Troubleshooting Permission Errors Ensure you have push access to the repository Verify your GitHub token has repo permissions Try creating a personal access token with repo scope Build Errors Check for broken links in your docs Verify all Python modules referenced in API docs exist Run mkdocs serve locally to test before deploying","title":"Deployment Guide"},{"location":"getting-started/deployment/#deployment-guide","text":"","title":"Deployment Guide"},{"location":"getting-started/deployment/#automated-deployment-recommended","text":"Push changes to GitHub The workflow in .github/workflows/gh-pages.yml will automatically: Build the documentation Deploy to the gh-pages branch Make it available at https://[your-username].github.io/shapleyx/","title":"Automated Deployment (Recommended)"},{"location":"getting-started/deployment/#manual-deployment-fallback","text":"If automated deployment fails: Install requirements: pip install mkdocs-material mkdocstrings [ python ] Build and deploy: mkdocs gh-deploy --force If permission errors persist: git config --global user.name \"Your Name\" git config --global user.email \"your@email.com\" mkdocs gh-deploy --force --remote-branch gh-pages","title":"Manual Deployment (Fallback)"},{"location":"getting-started/deployment/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"getting-started/deployment/#permission-errors","text":"Ensure you have push access to the repository Verify your GitHub token has repo permissions Try creating a personal access token with repo scope","title":"Permission Errors"},{"location":"getting-started/deployment/#build-errors","text":"Check for broken links in your docs Verify all Python modules referenced in API docs exist Run mkdocs serve locally to test before deploying","title":"Build Errors"},{"location":"getting-started/installation/","text":"Installation Prerequisites Python 3.7 or higher pip package manager Installing ShapleyX You can install ShapleyX directly from PyPI: pip install shapleyx Or install from source: git clone https://github.com/frbennett/shapleyx.git cd shapleyx pip install . Dependencies ShapleyX requires the following Python packages: numpy scipy pandas scikit-learn matplotlib seaborn These will be installed automatically when installing ShapleyX. Verifying Installation After installation, you can verify it works by running: ```python import shapleyx print(shapleyx. version )","title":"Installation"},{"location":"getting-started/installation/#installation","text":"","title":"Installation"},{"location":"getting-started/installation/#prerequisites","text":"Python 3.7 or higher pip package manager","title":"Prerequisites"},{"location":"getting-started/installation/#installing-shapleyx","text":"You can install ShapleyX directly from PyPI: pip install shapleyx Or install from source: git clone https://github.com/frbennett/shapleyx.git cd shapleyx pip install .","title":"Installing ShapleyX"},{"location":"getting-started/installation/#dependencies","text":"ShapleyX requires the following Python packages: numpy scipy pandas scikit-learn matplotlib seaborn These will be installed automatically when installing ShapleyX.","title":"Dependencies"},{"location":"getting-started/installation/#verifying-installation","text":"After installation, you can verify it works by running: ```python import shapleyx print(shapleyx. version )","title":"Verifying Installation"},{"location":"getting-started/quickstart/","text":"Quickstart Guide This guide will walk you through a basic sensitivity analysis using ShapleyX. Loading Data First, prepare your data in CSV format with columns for input parameters and one column for output (named 'Y'): import pandas as pd # Load your data data = pd . read_csv ( 'input_data.csv' ) Running Analysis from shapleyx import rshdmr # Initialize analyzer analyzer = rshdmr ( data_file = 'input_data.csv' , # or pass DataFrame directly polys = [ 10 , 5 ], # polynomial orders method = 'ard' , # regression method verbose = True # show progress ) # Run complete analysis pipeline sobol_indices , shapley_effects , total_index = analyzer . run_all () Viewing Results # Sobol indices print ( \"Sobol Indices:\" ) print ( sobol_indices ) # Shapley effects print ( \" \\n Shapley Effects:\" ) print ( shapley_effects ) # Total indices print ( \" \\n Total Indices:\" ) print ( total_index ) Plotting Results # Plot predicted vs actual analyzer . plot_hdmr () Next Steps See Tutorials for more detailed examples Explore How-to Guides for customization options","title":"Quickstart"},{"location":"getting-started/quickstart/#quickstart-guide","text":"This guide will walk you through a basic sensitivity analysis using ShapleyX.","title":"Quickstart Guide"},{"location":"getting-started/quickstart/#loading-data","text":"First, prepare your data in CSV format with columns for input parameters and one column for output (named 'Y'): import pandas as pd # Load your data data = pd . read_csv ( 'input_data.csv' )","title":"Loading Data"},{"location":"getting-started/quickstart/#running-analysis","text":"from shapleyx import rshdmr # Initialize analyzer analyzer = rshdmr ( data_file = 'input_data.csv' , # or pass DataFrame directly polys = [ 10 , 5 ], # polynomial orders method = 'ard' , # regression method verbose = True # show progress ) # Run complete analysis pipeline sobol_indices , shapley_effects , total_index = analyzer . run_all ()","title":"Running Analysis"},{"location":"getting-started/quickstart/#viewing-results","text":"# Sobol indices print ( \"Sobol Indices:\" ) print ( sobol_indices ) # Shapley effects print ( \" \\n Shapley Effects:\" ) print ( shapley_effects ) # Total indices print ( \" \\n Total Indices:\" ) print ( total_index )","title":"Viewing Results"},{"location":"getting-started/quickstart/#plotting-results","text":"# Plot predicted vs actual analyzer . plot_hdmr ()","title":"Plotting Results"},{"location":"getting-started/quickstart/#next-steps","text":"See Tutorials for more detailed examples Explore How-to Guides for customization options","title":"Next Steps"},{"location":"how-to-guides/common-tasks/","text":"Common Tasks Handling Large Datasets # Process data in chunks analyzer = rshdmr ( data_file = 'large_data.csv' , chunksize = 10000 , # Process 10,000 rows at a time polys = [ 5 , 3 ], # Lower polynomial orders for large datasets method = 'ard' ) Customizing Polynomial Orders # Set different polynomial orders for each input analyzer = rshdmr ( data_file = 'data.csv' , polys = [ 10 , 5 , 8 , 3 , 6 ], # Specific orders for each parameter method = 'ard' ) Saving and Loading Results # Save results to file import pickle with open ( 'sensitivity_results.pkl' , 'wb' ) as f : pickle . dump ({ 'sobol' : sobol_indices , 'shapley' : shapley_effects , 'total' : total_index }, f ) # Load results with open ( 'sensitivity_results.pkl' , 'rb' ) as f : results = pickle . load ( f ) Comparing Different Methods # Compare ARD and OLS methods analyzer_ard = rshdmr ( data_file = 'data.csv' , method = 'ard' ) analyzer_ols = rshdmr ( data_file = 'data.csv' , method = 'ols' ) ard_results = analyzer_ard . run_all () ols_results = analyzer_ols . run_all () Troubleshooting Common Issues Memory Errors Reduce polynomial orders Use smaller chunksize Filter less important parameters Convergence Issues Check data quality Try different polynomial orders Normalize input data","title":"Common Tasks"},{"location":"how-to-guides/common-tasks/#common-tasks","text":"","title":"Common Tasks"},{"location":"how-to-guides/common-tasks/#handling-large-datasets","text":"# Process data in chunks analyzer = rshdmr ( data_file = 'large_data.csv' , chunksize = 10000 , # Process 10,000 rows at a time polys = [ 5 , 3 ], # Lower polynomial orders for large datasets method = 'ard' )","title":"Handling Large Datasets"},{"location":"how-to-guides/common-tasks/#customizing-polynomial-orders","text":"# Set different polynomial orders for each input analyzer = rshdmr ( data_file = 'data.csv' , polys = [ 10 , 5 , 8 , 3 , 6 ], # Specific orders for each parameter method = 'ard' )","title":"Customizing Polynomial Orders"},{"location":"how-to-guides/common-tasks/#saving-and-loading-results","text":"# Save results to file import pickle with open ( 'sensitivity_results.pkl' , 'wb' ) as f : pickle . dump ({ 'sobol' : sobol_indices , 'shapley' : shapley_effects , 'total' : total_index }, f ) # Load results with open ( 'sensitivity_results.pkl' , 'rb' ) as f : results = pickle . load ( f )","title":"Saving and Loading Results"},{"location":"how-to-guides/common-tasks/#comparing-different-methods","text":"# Compare ARD and OLS methods analyzer_ard = rshdmr ( data_file = 'data.csv' , method = 'ard' ) analyzer_ols = rshdmr ( data_file = 'data.csv' , method = 'ols' ) ard_results = analyzer_ard . run_all () ols_results = analyzer_ols . run_all ()","title":"Comparing Different Methods"},{"location":"how-to-guides/common-tasks/#troubleshooting-common-issues","text":"","title":"Troubleshooting Common Issues"},{"location":"how-to-guides/common-tasks/#memory-errors","text":"Reduce polynomial orders Use smaller chunksize Filter less important parameters","title":"Memory Errors"},{"location":"how-to-guides/common-tasks/#convergence-issues","text":"Check data quality Try different polynomial orders Normalize input data","title":"Convergence Issues"},{"location":"reference/api/","text":"API Reference shapleyx . rshdmr ( data_file , polys = [ 10 , 5 ], n_jobs =- 1 , test_size = 0.25 , limit = 2.0 , k_best = 1 , p_average = 2 , n_iter = 300 , verbose = False , method = 'ard' , starting_iter = 5 , resampling = True , CI = 95.0 , number_of_resamples = 1000 , cv_tol = 0.05 ) Global Sensitivity Analysis using RS-HDMR with GMDH and linear regression. Examples: This class implements a global sensitivity analysis framework combining: Sparse Random Sampling (SRS) High Dimensional Model Representation (HDMR) Group Method of Data Handling (GMDH) for parameter selection Linear regression for parameter refinement Parameters: data_file ( str or DataFrame ) \u2013 Input data file path or DataFrame. polys ( list , default: [10, 5] ) \u2013 Polynomial orders for expansion. Defaults to [10, 5]. n_jobs ( int , default: -1 ) \u2013 Number of parallel jobs. Defaults to -1. test_size ( float , default: 0.25 ) \u2013 Test set size ratio. Defaults to 0.25. limit ( float , default: 2.0 ) \u2013 Coefficient limit. Defaults to 2.0. k_best ( int , default: 1 ) \u2013 Number of best features. Defaults to 1. p_average ( int , default: 2 ) \u2013 Parameter for averaging. Defaults to 2. n_iter ( int , default: 300 ) \u2013 Number of iterations. Defaults to 300. verbose ( bool , default: False ) \u2013 Verbosity flag. Defaults to False. method ( str , default: 'ard' ) \u2013 Regression method ('ard', 'omp', etc.). Defaults to 'ard'. can take values 'ard' - Automatic Relevance Determination 'omp' - Orthogonal Matching Pursuit from sklearn.linear_model 'ard_cv' - Automatic Relevance Determination with cross-validation 'omp_cv' - Orthogonal Matching Pursuit with cross-validation from sklearn.linear_model 'ard_sk' - Automatic Relevance Determination from sklearn.linear_model starting_iter ( int , default: 5 ) \u2013 Starting iteration. Defaults to 5. resampling ( bool , default: True ) \u2013 Enable bootstrap resampling. Defaults to True. CI ( float , default: 95.0 ) \u2013 Confidence interval percentage. Defaults to 95.0. number_of_resamples ( int , default: 1000 ) \u2013 Number of bootstrap samples. Defaults to 1000. cv_tol ( float , default: 0.05 ) \u2013 Cross-validation tolerance. Defaults to 0.05. Attributes: X ( DataFrame ) \u2013 Input features dataframe. Y ( Series ) \u2013 Target variable series. X_T ( DataFrame ) \u2013 Transformed features in unit hypercube. ranges ( list ) \u2013 Ranges of transformed data. X_T_L ( DataFrame ) \u2013 Expanded features with Legendre polynomials. coef_ ( array ) \u2013 Regression coefficients. y_pred ( array ) \u2013 Predicted values. evs ( dict ) \u2013 Model evaluation statistics. results ( DataFrame ) \u2013 Sobol indices results. non_zero_coefficients ( DataFrame ) \u2013 Non-zero coefficients. shap ( DataFrame ) \u2013 Shapley effects. total ( DataFrame ) \u2013 Total sensitivity indices. surrogate_model ( DataFrame ) \u2013 Trained surrogate model for predictions. primitive_variables ( DataFrame ) \u2013 Primitive variables from Legendre expansion. poly_orders ( DataFrame ) \u2013 Polynomial orders used in Legendre expansion. delta_instance ( DataFrame ) \u2013 Instance of pawn.DeltaX or pawn.hX for delta/h indices calculation. Examples: # Initialize analyzer analyzer = rshdmr ( data_file = 'input.csv' , polys = [ 10 , 5 ], method = 'ard' ) # Run analysis sobol , shapley , total = analyzer . run_all () # Make predictions predictions = analyzer . predict ( new_data ) # Get sensitivity indices pawn_results = analyzer . get_pawn ( S = 10 ) Todo: Improve memory management for large expansions Source code in shapleyx\\shapleyx.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def __init__ ( self , data_file , polys = [ 10 , 5 ], n_jobs = - 1 , test_size = 0.25 , limit = 2.0 , k_best = 1 , p_average = 2 , n_iter = 300 , verbose = False , method = 'ard' , starting_iter = 5 , resampling = True , CI = 95.0 , number_of_resamples = 1000 , cv_tol = 0.05 ): self . read_data ( data_file ) self . n_jobs = n_jobs self . test_size = test_size self . limit = limit self . k_best = k_best self . p_average = p_average self . polys = polys self . max_1st = max ( polys ) self . n_iter = n_iter self . verbose = verbose self . method = method self . starting_iter = starting_iter self . resampling = resampling self . CI = CI self . number_of_resamples = number_of_resamples self . cv_tol = cv_tol eval_all_indices () Evaluates Sobol indices, Shapley effects, and total sensitivity indices. Uses the indicies.eval_indices utility. Updates the following attributes self.results (pd.DataFrame): DataFrame containing Sobol indices results. self.non_zero_coefficients (pd.DataFrame): DataFrame of non-zero coefficients. self.shap (pd.DataFrame): DataFrame containing Shapley effects. self.total (pd.DataFrame): DataFrame containing total sensitivity indices. Source code in shapleyx\\shapleyx.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def eval_all_indices ( self ): \"\"\"Evaluates Sobol indices, Shapley effects, and total sensitivity indices. Uses the `indicies.eval_indices` utility. Updates the following attributes: self.results (pd.DataFrame): DataFrame containing Sobol indices results. self.non_zero_coefficients (pd.DataFrame): DataFrame of non-zero coefficients. self.shap (pd.DataFrame): DataFrame containing Shapley effects. self.total (pd.DataFrame): DataFrame containing total sensitivity indices. \"\"\" eval_indicies = indicies . eval_indices ( self . X_T_L , self . Y , self . coef_ , self . evs ) self . results = eval_indicies . get_sobol_indicies () self . non_zero_coefficients = eval_indicies . get_non_zero_coefficients () self . shap = eval_indicies . eval_shapley ( self . X . columns ) self . total = eval_indicies . eval_total_index ( self . X . columns ) get_deltax ( num_unconditioned , delta_samples ) Calculate delta indices for the given number of unconditioned variables and delta samples. This method initializes a DeltaX instance using the provided data and parameters, then computes the delta indices based on the specified number of unconditioned variables and delta samples. Parameters: num_unconditioned ( int ) \u2013 The number of unconditioned variables. delta_samples ( int ) \u2013 The number of delta samples to generate. Returns: DataFrame \u2013 pd.DataFrame: A DataFrame containing the computed delta indices. Source code in shapleyx\\shapleyx.py 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 def get_deltax ( self , num_unconditioned : int , delta_samples : int ) -> pd . DataFrame : \"\"\" Calculate delta indices for the given number of unconditioned variables and delta samples. This method initializes a DeltaX instance using the provided data and parameters, then computes the delta indices based on the specified number of unconditioned variables and delta samples. Args: num_unconditioned (int): The number of unconditioned variables. delta_samples (int): The number of delta samples to generate. Returns: pd.DataFrame: A DataFrame containing the computed delta indices. \"\"\" self . delta_instance = pawn . DeltaX ( self . X , self . Y , self . ranges , self . non_zero_coefficients ) delta_indices = self . delta_instance . get_deltax ( num_unconditioned , delta_samples ) return delta_indices get_hx ( num_unconditioned , delta_samples ) Calculate delta indices for the given number of unconditioned variables and delta samples. This method initializes a DeltaX instance using the provided data and parameters, then computes the delta indices based on the specified number of unconditioned variables and delta samples. Parameters: num_unconditioned ( int ) \u2013 The number of unconditioned variables. delta_samples ( int ) \u2013 The number of delta samples to generate. Returns: DataFrame \u2013 pd.DataFrame: A DataFrame containing the computed delta indices. Source code in shapleyx\\shapleyx.py 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 def get_hx ( self , num_unconditioned : int , delta_samples : int ) -> pd . DataFrame : \"\"\" Calculate delta indices for the given number of unconditioned variables and delta samples. This method initializes a DeltaX instance using the provided data and parameters, then computes the delta indices based on the specified number of unconditioned variables and delta samples. Args: num_unconditioned (int): The number of unconditioned variables. delta_samples (int): The number of delta samples to generate. Returns: pd.DataFrame: A DataFrame containing the computed delta indices. \"\"\" self . delta_instance = pawn . hX ( self . X , self . Y , self . ranges , self . non_zero_coefficients ) delta_indices = self . delta_instance . get_hx ( num_unconditioned , delta_samples ) return delta_indices get_pawn ( S = 10 ) Estimates PAWN sensitivity indices directly from data. Uses the pawn.estimate_pawn utility. Parameters: S ( int , default: 10 ) \u2013 Number of slices/intervals for the PAWN estimation. Defaults to 10. Returns: dict \u2013 Dictionary containing the PAWN sensitivity indices for each feature. Source code in shapleyx\\shapleyx.py 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def get_pawn ( self , S = 10 ) : \"\"\"Estimates PAWN sensitivity indices directly from data. Uses the `pawn.estimate_pawn` utility. Args: S (int, optional): Number of slices/intervals for the PAWN estimation. Defaults to 10. Returns: dict: Dictionary containing the PAWN sensitivity indices for each feature. \"\"\" num_features = self . X . shape [ 1 ] pawn_results = pawn . estimate_pawn ( self . X . columns , num_features , self . X . values , self . Y , S = S ) return pawn_results get_pawnx ( num_unconditioned , num_conditioned , num_ks_samples , alpha = 0.05 ) Calculates PAWN sensitivity indices using the surrogate model. Uses the pawn.pawnx utility. Parameters: num_unconditioned ( int ) \u2013 Number of unconditioned samples for PAWN. num_conditioned ( int ) \u2013 Number of conditioned samples for PAWN. num_ks_samples ( int ) \u2013 Number of samples for the Kolmogorov-Smirnov test. alpha ( float , default: 0.05 ) \u2013 Significance level for the KS test. Defaults to 0.05. Returns: DataFrame \u2013 pd.DataFrame: DataFrame containing the PAWN sensitivity indices. Source code in shapleyx\\shapleyx.py 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 def get_pawnx ( self , num_unconditioned : int , num_conditioned : int , num_ks_samples : int , alpha : float = 0.05 ) -> pd . DataFrame : \"\"\"Calculates PAWN sensitivity indices using the surrogate model. Uses the `pawn.pawnx` utility. Args: num_unconditioned (int): Number of unconditioned samples for PAWN. num_conditioned (int): Number of conditioned samples for PAWN. num_ks_samples (int): Number of samples for the Kolmogorov-Smirnov test. alpha (float, optional): Significance level for the KS test. Defaults to 0.05. Returns: pd.DataFrame: DataFrame containing the PAWN sensitivity indices. \"\"\" pawn_instance = pawn . pawnx ( self . X , self . Y , self . ranges , self . non_zero_coefficients ) pawn_indices = pawn_instance . get_pawnx ( num_unconditioned , num_conditioned , num_ks_samples , alpha ) return pawn_indices get_pruned_data () Generates a pruned dataset containing only the features with non-zero coefficients. This method creates a new DataFrame that includes only the columns from the original dataset ( X_T_L ) that correspond to the labels with non-zero coefficients. Additionally, it includes the target variable ( Y ). Returns: \u2013 pd.DataFrame: A DataFrame containing the pruned data with selected features and the target variable. Source code in shapleyx\\shapleyx.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def get_pruned_data ( self ): \"\"\" Generates a pruned dataset containing only the features with non-zero coefficients. This method creates a new DataFrame that includes only the columns from the original dataset (`X_T_L`) that correspond to the labels with non-zero coefficients. Additionally, it includes the target variable (`Y`). Returns: pd.DataFrame: A DataFrame containing the pruned data with selected features and the target variable. \"\"\" pruned_data = pd . DataFrame () for label in self . non_zero_coefficients [ 'labels' ] : pruned_data [ label ] = self . X_T_L [ label ] pruned_data [ 'Y' ] = self . Y return pruned_data legendre_expand () Performs Legendre polynomial expansion on the transformed data self.X_T . Uses the legendre.legendre_expand utility. Updates the following attributes self.primitive_variables: Primitive variables from the expansion. self.poly_orders: Polynomial orders used in the expansion. self.X_T_L (pd.DataFrame): The expanded data matrix including Legendre terms. Source code in shapleyx\\shapleyx.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def legendre_expand ( self ): \"\"\"Performs Legendre polynomial expansion on the transformed data `self.X_T`. Uses the `legendre.legendre_expand` utility. Updates the following attributes: self.primitive_variables: Primitive variables from the expansion. self.poly_orders: Polynomial orders used in the expansion. self.X_T_L (pd.DataFrame): The expanded data matrix including Legendre terms. \"\"\" expansion_data = legendre . legendre_expand ( self . X_T , self . polys ) expansion_data . build_basis_set () self . primitive_variables = expansion_data . get_primitive_variables () self . poly_orders = expansion_data . get_poly_orders () self . X_T_L = expansion_data . get_expanded () plot_hdmr () Plots the High-Dimensional Model Representation (HDMR) of the model's predictions. This method uses the plot_hdmr function from the stats module to visualize the HDMR of the actual values ( self.Y ) against the predicted values ( self.y_pred ). Returns: \u2013 None Source code in shapleyx\\shapleyx.py 244 245 246 247 248 249 250 251 252 253 254 def plot_hdmr ( self ): \"\"\" Plots the High-Dimensional Model Representation (HDMR) of the model's predictions. This method uses the `plot_hdmr` function from the `stats` module to visualize the HDMR of the actual values (`self.Y`) against the predicted values (`self.y_pred`). Returns: None \"\"\" stats . plot_hdmr ( self . Y , self . y_pred ) predict ( X ) Predicts output for new input data using the trained surrogate model. If the surrogate model ( self.surrogate_model ) doesn't exist, it first creates and fits one using predictor.surrogate . Parameters: X ( array - like ) \u2013 Input data for which predictions are to be made. Should have the same features as the original training data. Returns: \u2013 array-like: Predicted output values. Source code in shapleyx\\shapleyx.py 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def predict ( self , X ): \"\"\"Predicts output for new input data using the trained surrogate model. If the surrogate model (`self.surrogate_model`) doesn't exist, it first creates and fits one using `predictor.surrogate`. Args: X (array-like): Input data for which predictions are to be made. Should have the same features as the original training data. Returns: array-like: Predicted output values. \"\"\" if not hasattr ( self , 'surrogate_model' ): self . surrogate_model = predictor . surrogate ( self . non_zero_coefficients , self . ranges ) self . surrogate_model . fit ( self . X , self . Y ) return self . surrogate_model . predict ( X ) read_data ( data_file ) Reads data from a file or DataFrame. Initializes the self.X (features) and self.Y (target) attributes. Parameters: data_file ( str or DataFrame ) \u2013 Path to the data file (CSV) or a pandas DataFrame. Raises: ValueError \u2013 If data_file is not a string path or a DataFrame. Source code in shapleyx\\shapleyx.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def read_data ( self , data_file ): \"\"\"Reads data from a file or DataFrame. Initializes the `self.X` (features) and `self.Y` (target) attributes. Args: data_file (str or pd.DataFrame): Path to the data file (CSV) or a pandas DataFrame. Raises: ValueError: If `data_file` is not a string path or a DataFrame. \"\"\" if isinstance ( data_file , pd . DataFrame ): print ( 'Found a DataFrame' ) df = data_file elif isinstance ( data_file , str ): df = pd . read_csv ( data_file ) else : raise ValueError ( \"data_file must be either a pandas DataFrame or a file path (str).\" ) self . Y = df [ 'Y' ] self . X = df . drop ( 'Y' , axis = 1 ) # Clean up the original DataFrame to save memory del df run_all () Execute a complete sequence of steps for RS-HDMR (Random Sampling High-Dimensional Model Representation) analysis. This method performs the following steps in sequence: Transforms the input data to a unit hypercube. Builds basis functions using Legendre polynomials. Runs regression analysis to fit the model. Calculates and displays RS-HDMR model performance statistics. Evaluates Sobol indices to quantify the contribution of each input variable to the output variance. Calculates Shapley effects to measure the importance of each input variable. Computes the total index to assess the overall impact of input variables. If resampling is enabled, performs bootstrap resampling to estimate confidence intervals for Sobol indices and Shapley effects. Prints a completion message with a randomly selected quote. Returns: tuple \u2013 A tuple containing three elements: sobol_indices (pd.DataFrame): A DataFrame containing Sobol indices for each input variable. shapley_effects (pd.DataFrame): A DataFrame containing Shapley effects for each input variable. total_index (pd.DataFrame): A DataFrame containing the total index for each input variable. Notes The method assumes that the necessary data and configurations are already set in the class instance. If resampling is enabled ( self.resampling is True), confidence intervals (CIs) are calculated for Sobol indices and Shapley effects. The method uses helper functions like transform_data , legendre_expand , run_regression , stats , plot_hdmr , eval_sobol_indices , get_shapley , and get_total_index to perform specific tasks. The completion message includes a randomly selected quote for a touch of inspiration. Example sobol_indices, shapley_effects, total_index = instance.run_all() print(sobol_indices) print(shapley_effects) print(total_index) Source code in shapleyx\\shapleyx.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def run_all ( self ): \"\"\" Execute a complete sequence of steps for RS-HDMR (Random Sampling High-Dimensional Model Representation) analysis. This method performs the following steps in sequence: 1. Transforms the input data to a unit hypercube. 2. Builds basis functions using Legendre polynomials. 3. Runs regression analysis to fit the model. 4. Calculates and displays RS-HDMR model performance statistics. 5. Evaluates Sobol indices to quantify the contribution of each input variable to the output variance. 6. Calculates Shapley effects to measure the importance of each input variable. 7. Computes the total index to assess the overall impact of input variables. 8. If resampling is enabled, performs bootstrap resampling to estimate confidence intervals for Sobol indices and Shapley effects. 9. Prints a completion message with a randomly selected quote. Returns: tuple: A tuple containing three elements: - sobol_indices (pd.DataFrame): A DataFrame containing Sobol indices for each input variable. - shapley_effects (pd.DataFrame): A DataFrame containing Shapley effects for each input variable. - total_index (pd.DataFrame): A DataFrame containing the total index for each input variable. Notes: - The method assumes that the necessary data and configurations are already set in the class instance. - If resampling is enabled (`self.resampling` is True), confidence intervals (CIs) are calculated for Sobol indices and Shapley effects. - The method uses helper functions like `transform_data`, `legendre_expand`, `run_regression`, `stats`, `plot_hdmr`, `eval_sobol_indices`, `get_shapley`, and `get_total_index` to perform specific tasks. - The completion message includes a randomly selected quote for a touch of inspiration. Example: >>> sobol_indices, shapley_effects, total_index = instance.run_all() >>> print(sobol_indices) >>> print(shapley_effects) >>> print(total_index) \"\"\" # Define a helper function to print headings def print_step ( step_name ): print_heading ( step_name ) # Step 1: Transform data to unit hypercube print_step ( 'Transforming data to unit hypercube' ) self . transform_data () # Step 2: Build basis functions print_step ( 'Building basis functions' ) self . legendre_expand () # Step 3: Run regression analysis print_step ( 'Running regression analysis' ) self . run_regression () # Step 4: Calculate RS-HDMR model performance statistics print_step ( 'RS-HDMR model performance statistics' ) self . stats () print () self . plot_hdmr () # Step 5: Evaluate Sobol indices self . eval_all_indices () sobol_indices = self . results . drop ( columns = [ 'labels' , 'coeff' ]) # Step 6: Calculate Shapley effects shapley_effects = self . shap # Step 7: Calculate total index total_index = self . total # Step 8: Perform resampling if enabled if self . resampling : print_step ( f 'Running bootstrap resampling { self . number_of_resamples } samples for { self . CI } % CI' ) do_resampling = resampling . resampling ( self . get_pruned_data (), self . number_of_resamples , self . X . columns ) do_resampling . do_resampling () sobol_indices = do_resampling . get_sobol_quantiles ( sobol_indices , self . CI ) # Calculate quantiles for Shapley effects shapley_effects = do_resampling . get_shap_quantiles ( shapley_effects , self . CI ) print_step ( 'Completed bootstrap resampling' ) # Step 9: Print completion message with a quote quote = quotes . get_quote () message = ( \" Completed all analysis \\n \" \" ------------------------ \\n\\n \" f \" { textwrap . fill ( quote , 58 ) } \" ) print_step ( message ) return sobol_indices , shapley_effects , total_index run_regression () Runs the regression analysis using the specified method. Uses the regression.regression utility based on self.method . Updates the following attributes self.coef_ (np.array): The regression coefficients obtained from the fit. self.y_pred (np.array): The predicted values based on the fitted model. Source code in shapleyx\\shapleyx.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def run_regression ( self ): \"\"\"Runs the regression analysis using the specified method. Uses the `regression.regression` utility based on `self.method`. Updates the following attributes: self.coef_ (np.array): The regression coefficients obtained from the fit. self.y_pred (np.array): The predicted values based on the fitted model. \"\"\" regression_instance = regression . regression ( X_T_L = self . X_T_L , Y = self . Y , method = self . method , n_iter = self . n_iter , verbose = self . verbose , cv_tol = self . cv_tol , starting_iter = self . starting_iter ) self . coef_ , self . y_pred = regression_instance . run_regression () stats () Calculates and stores evaluation statistics for the fitted model. Uses the stats.stats utility. Updates the following attributes self.evs (dict): A dictionary containing evaluation statistics (e.g., R^2, MSE). Source code in shapleyx\\shapleyx.py 234 235 236 237 238 239 240 241 242 def stats ( self ): \"\"\"Calculates and stores evaluation statistics for the fitted model. Uses the `stats.stats` utility. Updates the following attributes: self.evs (dict): A dictionary containing evaluation statistics (e.g., R^2, MSE). \"\"\" self . evs = stats . stats ( self . Y , self . y_pred , self . coef_ ) transform_data () Transforms the input data self.X into a unit hypercube. Updates the following attributes self.ranges (list): The ranges (min, max) of the original data features. self.X_T (pd.DataFrame): The transformed data matrix within the unit hypercube. Source code in shapleyx\\shapleyx.py 183 184 185 186 187 188 189 190 191 192 193 def transform_data ( self ): \"\"\"Transforms the input data `self.X` into a unit hypercube. Updates the following attributes: self.ranges (list): The ranges (min, max) of the original data features. self.X_T (pd.DataFrame): The transformed data matrix within the unit hypercube. \"\"\" transformed_data = transformation . transformation ( self . X ) transformed_data . do_transform () self . ranges = transformed_data . get_ranges () self . X_T = transformed_data . get_X_T () shapleyx.ARD RegressionARD ( n_iter = 300 , tol = 0.001 , fit_intercept = True , copy_X = True , verbose = False , cv_tol = 0.1 , cv = False ) Bases: RegressorMixin , LinearModel Regression with Automatic Relevance Determination (ARD) using Sparse Bayesian Learning. This class implements a fast version of ARD regression, which is a Bayesian approach to regression that automatically determines the relevance of each feature. It is based on the Sparse Bayesian Learning (SBL) algorithm, which promotes sparsity in the model by estimating the precision of the coefficients. Parameters n_iter : int, optional (default=300) Maximum number of iterations for the optimization algorithm. float, optional (default=1e-3) Convergence threshold. If the absolute change in the precision parameter for the weights is below this threshold, the algorithm terminates. bool, optional (default=True) Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (e.g., data is expected to be already centered). bool, optional (default=True) If True, X will be copied; else, it may be overwritten. bool, optional (default=False) If True, the algorithm will print progress messages during fitting. float, optional (default=0.1) Tolerance for cross-validation. If the percentage change in cross-validation score is below this threshold, the algorithm terminates. bool, optional (default=False) If True, cross-validation will be used to determine the optimal number of features. Attributes coef_ : array, shape (n_features,) Coefficients of the regression model (mean of the posterior distribution). float Estimated precision of the noise. array, dtype=bool, shape (n_features,) Boolean array indicating which features are active (non-zero coefficients). array, shape (n_features,) Estimated precisions of the coefficients. array, shape (n_features, n_features) Estimated covariance matrix of the weights, computed only for non-zero coefficients. list List of cross-validation scores if cv is True. Methods fit(X, y) Fit the ARD regression model to the data. predict_dist(X) Compute the predictive distribution for the test set. _center_data(X, y) Center the data by subtracting the mean. _posterior_dist(A, beta, XX, XY, full_covar=False) Calculate the mean and covariance matrix of the posterior distribution of coefficients. _sparsity_quality(XX, XXd, XY, XYa, Aa, Ri, active, beta, cholesky) Calculate sparsity and quality parameters for each feature. References [1] Tipping, M. E., & Faul, A. C. (2003). Fast marginal likelihood maximisation for sparse Bayesian models. In Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics (pp. 276-283). [2] Tipping, M. E., & Faul, A. C. (2001). Analysis of sparse Bayesian learning. In Advances in Neural Information Processing Systems (pp. 383-389). Source code in shapleyx\\ARD.py 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , n_iter = 300 , tol = 1e-3 , fit_intercept = True , copy_X = True , verbose = False , cv_tol = 0.1 , cv = False ): self . n_iter = n_iter self . tol = tol self . scores_ = list () self . fit_intercept = fit_intercept self . copy_X = copy_X self . verbose = verbose self . cv = cv self . cv_tol = cv_tol fit ( X , y ) Fit the ARD regression model to the data. Parameters X : {array-like, sparse matrix}, shape (n_samples, n_features) Training data, matrix of explanatory variables. array-like, shape (n_samples,) Target values. Returns self : object Returns the instance itself. Source code in shapleyx\\ARD.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def fit ( self , X , y ): ''' Fit the ARD regression model to the data. Parameters ---------- X : {array-like, sparse matrix}, shape (n_samples, n_features) Training data, matrix of explanatory variables. y : array-like, shape (n_samples,) Target values. Returns ------- self : object Returns the instance itself. ''' X , y = check_X_y ( X , y , dtype = np . float64 , y_numeric = True ) X , y , X_mean , y_mean , X_std = self . _center_data ( X , y ) n_samples , n_features = X . shape cv_list = [] cv_score_history = [] current_r = 0 # precompute X'*Y , X'*X for faster iterations & allocate memory for # sparsity & quality vectors XY = np . dot ( X . T , y ) XX = np . dot ( X . T , X ) XXd = np . diag ( XX ) # initialise precision of noise & and coefficients var_y = np . var ( y ) # check that variance is non zero !!! if var_y == 0 : beta = 1e-2 else : beta = 1. / np . var ( y ) A = np . PINF * np . ones ( n_features ) active = np . zeros ( n_features , dtype = bool ) # in case of almost perfect multicollinearity between some features # start from feature 0 if np . sum ( XXd - X_mean ** 2 < np . finfo ( np . float32 ) . eps ) > 0 : A [ 0 ] = np . finfo ( np . float16 ) . eps active [ 0 ] = True else : # start from a single basis vector with largest projection on targets proj = XY ** 2 / XXd start = np . argmax ( proj ) active [ start ] = True A [ start ] = XXd [ start ] / ( proj [ start ] - var_y ) warning_flag = 0 for i in range ( self . n_iter ): XXa = XX [ active ,:][:, active ] XYa = XY [ active ] Aa = A [ active ] # mean & covariance of posterior distribution Mn , Ri , cholesky = self . _posterior_dist ( Aa , beta , XXa , XYa ) if cholesky : Sdiag = np . sum ( Ri ** 2 , 0 ) else : Sdiag = np . copy ( np . diag ( Ri )) warning_flag += 1 # raise warning in case cholesky failes if warning_flag == 1 : warnings . warn (( \"Cholesky decomposition failed ! Algorithm uses pinvh, \" \"which is significantly slower, if you use RVR it \" \"is advised to change parameters of kernel\" )) # compute quality & sparsity parameters s , q , S , Q = self . _sparsity_quality ( XX , XXd , XY , XYa , Aa , Ri , active , beta , cholesky ) # update precision parameter for noise distribution rss = np . sum ( ( y - np . dot ( X [:, active ] , Mn ) ) ** 2 ) beta = n_samples - np . sum ( active ) + np . sum ( Aa * Sdiag ) beta /= ( rss + np . finfo ( np . float32 ) . eps ) # update precision parameters of coefficients A , converged = update_precisions ( Q , S , q , s , A , active , self . tol , n_samples , False ) # *************************************** if self . cv : # Select features based on the 'active' mask # Assumes X is a numpy array for efficient slicing X_active = X [:, active ] # Define the model for cross-validation (instantiated fresh each time) cv_model = linear_model . Ridge () try : # Perform 10-fold cross-validation, explicitly using R^2 scoring # Ensure 'y' corresponds correctly to 'X_active' cv_scores = cross_val_score ( cv_model , X_active , y , cv = 10 , scoring = 'r2' ) new_cv_score = np . mean ( cv_scores ) # Use numpy mean for clarity # Calculate percentage change, handling division by zero # Assumes current_cv_score is initialized (e.g., to None or 0.0) before the loop if current_cv_score is not None and current_cv_score != 0 : percentage_change = ( new_cv_score - current_cv_score ) / current_cv_score * 100 elif new_cv_score == 0 and ( current_cv_score is None or current_cv_score == 0 ): percentage_change = 0.0 # No change if both old and new scores are zero else : # Handle cases where current_cv_score is None (first iteration) or zero percentage_change = np . inf # Indicate a large change if starting from zero/None # Optional: Replace print with logging for better control in applications # Assumes 'i' is an iteration counter from an outer loop print ( f \"Iteration { i } : CV Score = { new_cv_score : .4f } , % Change = { percentage_change : .2f } %\" ) # Check for convergence based on the absolute percentage change # Assumes cv_tol is a positive threshold for the magnitude of change # Assumes 'converged' is initialized (e.g., to False) before the loop if current_cv_score is not None and abs ( percentage_change ) < self . cv_tol : converged = True # Consider adding a 'break' here if the loop should terminate immediately upon convergence # Update the current score and history # Assumes cv_score_history is initialized (e.g., as []) before the loop current_cv_score = new_cv_score cv_score_history . append ( new_cv_score ) except ValueError as ve : # Catch specific errors, e.g., if X_active becomes empty or has incompatible dimensions print ( f \"Warning: Cross-validation failed at iteration { i } due to ValueError: { ve } \" ) # Decide how to handle: stop, skip, assign default score? # Example: Treat as no improvement or break percentage_change = np . nan # Mark as invalid # converged = True # Option: Stop if CV fails except Exception as e : # Catch other potential errors during cross-validation print ( f \"Warning: Cross-validation failed unexpectedly at iteration { i } : { e } \" ) percentage_change = np . nan # converged = True # Option: Stop if CV fails # Calculate active features once per iteration num_active_features = np . sum ( active ) if self . verbose : # Use logging (assuming logger is configured) and f-string for iteration progress # import logging # Ensure logging is imported at the top of the file logging . info ( f \"Iteration: { i } , Active Features: { num_active_features } \" ) # Check for convergence or max iterations to terminate if converged or i == self . n_iter - 1 : # Construct the final status message final_status = f \"Finished at Iteration: { i } , Active Features: { num_active_features } .\" if converged : log_level = logging . INFO # Normal convergence final_status += \" Algorithm converged.\" # The original code printed \"Algorithm converged !\" only if verbose. # Logging INFO level covers this sufficiently. Add DEBUG if more detail needed. # if self.verbose: # logging.debug(\"Convergence details: ...\") else : # i == self.n_iter - 1 log_level = logging . WARNING # Reached max iterations without converging final_status += f \" Reached maximum iterations ( { self . n_iter } ).\" # Log the final status logging . log ( log_level , final_status ) break # Exit the loop #print(('Iteration: {0}, number of features ' # 'in the model: {1}').format(i,np.sum(active))) # after last update of alpha & beta update parameters # of posterior distribution XXa , XYa , Aa = XX [ active ,:][:, active ], XY [ active ], A [ active ] Mn , Sn , cholesky = self . _posterior_dist ( Aa , beta , XXa , XYa , True ) self . coef_ = np . zeros ( n_features ) self . coef_ [ active ] = Mn self . sigma_ = Sn self . active_ = active self . lambda_ = A self . alpha_ = beta self . _set_intercept ( X_mean , y_mean , X_std ) if self . cv : # print(max(enumerate(cv_list), key=lambda x: x[1])) print (( 'Iteration: {0} , number of features ' 'in the model: {1} ' ) . format ( i , np . sum ( active ))) return self predict_dist ( X ) Computes predictive distribution for test set. Predictive distribution for each data point is one dimensional Gaussian and therefore is characterised by mean and variance. Parameters X : {array-like, sparse matrix}, shape (n_samples_test, n_features) Test data, matrix of explanatory variables. Returns y_hat : array, shape (n_samples_test,) Estimated values of targets on the test set (mean of the predictive distribution). array, shape (n_samples_test,) Variance of the predictive distribution. Source code in shapleyx\\ARD.py 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 def predict_dist ( self , X ): ''' Computes predictive distribution for test set. Predictive distribution for each data point is one dimensional Gaussian and therefore is characterised by mean and variance. Parameters ---------- X : {array-like, sparse matrix}, shape (n_samples_test, n_features) Test data, matrix of explanatory variables. Returns ------- y_hat : array, shape (n_samples_test,) Estimated values of targets on the test set (mean of the predictive distribution). var_hat : array, shape (n_samples_test,) Variance of the predictive distribution. ''' y_hat = self . _decision_function ( X ) var_hat = 1. / self . alpha_ var_hat += np . sum ( np . dot ( X [:, self . active_ ], self . sigma_ ) * X [:, self . active_ ], axis = 1 ) return y_hat , var_hat update_precisions ( Q , S , q , s , A , active , tol , n_samples , clf_bias ) Updates the precision parameters (alpha) for features in a sparse Bayesian learning model by selecting a feature to add, recompute, or delete based on its impact on the log marginal likelihood. The function also checks for convergence. Parameters: Q : numpy.ndarray Quality parameters for all features. S : numpy.ndarray Sparsity parameters for all features. q : numpy.ndarray Quality parameters for features currently in the model. s : numpy.ndarray Sparsity parameters for features currently in the model. A : numpy.ndarray Precision parameters (alpha) for all features. active : numpy.ndarray (bool) Boolean array indicating whether each feature is currently in the model. tol : float Tolerance threshold for determining convergence based on changes in precision. n_samples : int Number of samples in the dataset, used to normalize the change in log marginal likelihood. clf_bias : bool Flag indicating whether the model includes a bias term (used in classification tasks). Returns: list A list containing two elements: - Updated precision parameters (A) for all features. - A boolean flag indicating whether the model has converged. Notes: The function performs the following steps: 1. Computes the change in log marginal likelihood for adding, recomputing, or deleting features. 2. Identifies the feature that causes the largest change in likelihood. 3. Updates the precision parameter (alpha) for the selected feature. 4. Checks for convergence based on whether no features are added/deleted and changes in precision are below the specified tolerance. 5. Returns the updated precision parameters and convergence status. Convergence is determined by two conditions: - No features are added or deleted. - The change in precision for features already in the model is below the tolerance threshold. The function ensures that the bias term is not removed in classification tasks. Source code in shapleyx\\ARD.py 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 def update_precisions ( Q , S , q , s , A , active , tol , n_samples , clf_bias ): ''' Updates the precision parameters (alpha) for features in a sparse Bayesian learning model by selecting a feature to add, recompute, or delete based on its impact on the log marginal likelihood. The function also checks for convergence. Parameters: ----------- Q : numpy.ndarray Quality parameters for all features. S : numpy.ndarray Sparsity parameters for all features. q : numpy.ndarray Quality parameters for features currently in the model. s : numpy.ndarray Sparsity parameters for features currently in the model. A : numpy.ndarray Precision parameters (alpha) for all features. active : numpy.ndarray (bool) Boolean array indicating whether each feature is currently in the model. tol : float Tolerance threshold for determining convergence based on changes in precision. n_samples : int Number of samples in the dataset, used to normalize the change in log marginal likelihood. clf_bias : bool Flag indicating whether the model includes a bias term (used in classification tasks). Returns: -------- list A list containing two elements: - Updated precision parameters (A) for all features. - A boolean flag indicating whether the model has converged. Notes: ------ The function performs the following steps: 1. Computes the change in log marginal likelihood for adding, recomputing, or deleting features. 2. Identifies the feature that causes the largest change in likelihood. 3. Updates the precision parameter (alpha) for the selected feature. 4. Checks for convergence based on whether no features are added/deleted and changes in precision are below the specified tolerance. 5. Returns the updated precision parameters and convergence status. Convergence is determined by two conditions: - No features are added or deleted. - The change in precision for features already in the model is below the tolerance threshold. The function ensures that the bias term is not removed in classification tasks. ''' # initialise vector holding changes in log marginal likelihood deltaL = np . zeros ( Q . shape [ 0 ]) # identify features that can be added , recomputed and deleted in model theta = q ** 2 - s add = ( theta > 0 ) * ( active == False ) recompute = ( theta > 0 ) * ( active == True ) delete = ~ ( add + recompute ) # compute sparsity & quality parameters corresponding to features in # three groups identified above Qadd , Sadd = Q [ add ], S [ add ] Qrec , Srec , Arec = Q [ recompute ], S [ recompute ], A [ recompute ] Qdel , Sdel , Adel = Q [ delete ], S [ delete ], A [ delete ] # compute new alpha's (precision parameters) for features that are # currently in model and will be recomputed Anew = s [ recompute ] ** 2 / ( theta [ recompute ] + np . finfo ( np . float32 ) . eps ) delta_alpha = ( 1. / Anew - 1. / Arec ) # compute change in log marginal likelihood deltaL [ add ] = ( Qadd ** 2 - Sadd ) / Sadd + np . log ( Sadd / Qadd ** 2 ) deltaL [ recompute ] = Qrec ** 2 / ( Srec + 1. / delta_alpha ) - np . log ( 1 + Srec * delta_alpha ) deltaL [ delete ] = Qdel ** 2 / ( Sdel - Adel ) - np . log ( 1 - Sdel / Adel ) deltaL = deltaL / n_samples # find feature which caused largest change in likelihood feature_index = np . argmax ( deltaL ) # no deletions or additions same_features = np . sum ( theta [ ~ recompute ] > 0 ) == 0 # changes in precision for features already in model is below threshold no_delta = np . sum ( abs ( Anew - Arec ) > tol ) == 0 # check convergence: if no features to add or delete and small change in # precision for current features then terminate converged = False if same_features and no_delta : converged = True return [ A , converged ] # if not converged update precision parameter of weights and return if theta [ feature_index ] > 0 : A [ feature_index ] = s [ feature_index ] ** 2 / theta [ feature_index ] if active [ feature_index ] == False : active [ feature_index ] = True else : # at least two active features if active [ feature_index ] == True and np . sum ( active ) >= 2 : # do not remove bias term in classification # (in regression it is factored in through centering) if not ( feature_index == 0 and clf_bias ): active [ feature_index ] = False A [ feature_index ] = np . PINF return [ A , converged ] shapleyx.xsampler xsampler ( num_samples , ranges ) Generate a Latin Hypercube sample scaled to the specified ranges. Parameters: num_samples ( int ) \u2013 Number of samples to generate. ranges ( dict ) \u2013 A dictionary where keys are feature names and values are tuples of (lower, upper) bounds. Returns: ndarray \u2013 np.ndarray: A scaled Latin Hypercube sample of shape (num_samples, num_features). Source code in shapleyx\\xsampler.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def xsampler ( num_samples : int , ranges : dict ) -> np . ndarray : \"\"\" Generate a Latin Hypercube sample scaled to the specified ranges. Args: num_samples (int): Number of samples to generate. ranges (dict): A dictionary where keys are feature names and values are tuples of (lower, upper) bounds. Returns: np.ndarray: A scaled Latin Hypercube sample of shape (num_samples, num_features). \"\"\" num_features = len ( ranges ) # Extract lower and upper bounds from the ranges dictionary lower_bounds = [ bounds [ 0 ] for bounds in ranges . values ()] upper_bounds = [ bounds [ 1 ] for bounds in ranges . values ()] # Generate Latin Hypercube sample sampler = qmc . LatinHypercube ( d = num_features ) sample = sampler . random ( n = num_samples ) # Scale the sample to the specified ranges sample_scaled = qmc . scale ( sample , lower_bounds , upper_bounds ) return sample_scaled","title":"API"},{"location":"reference/api/#api-reference","text":"","title":"API Reference"},{"location":"reference/api/#shapleyx.rshdmr","text":"Global Sensitivity Analysis using RS-HDMR with GMDH and linear regression. Examples: This class implements a global sensitivity analysis framework combining: Sparse Random Sampling (SRS) High Dimensional Model Representation (HDMR) Group Method of Data Handling (GMDH) for parameter selection Linear regression for parameter refinement Parameters: data_file ( str or DataFrame ) \u2013 Input data file path or DataFrame. polys ( list , default: [10, 5] ) \u2013 Polynomial orders for expansion. Defaults to [10, 5]. n_jobs ( int , default: -1 ) \u2013 Number of parallel jobs. Defaults to -1. test_size ( float , default: 0.25 ) \u2013 Test set size ratio. Defaults to 0.25. limit ( float , default: 2.0 ) \u2013 Coefficient limit. Defaults to 2.0. k_best ( int , default: 1 ) \u2013 Number of best features. Defaults to 1. p_average ( int , default: 2 ) \u2013 Parameter for averaging. Defaults to 2. n_iter ( int , default: 300 ) \u2013 Number of iterations. Defaults to 300. verbose ( bool , default: False ) \u2013 Verbosity flag. Defaults to False. method ( str , default: 'ard' ) \u2013 Regression method ('ard', 'omp', etc.). Defaults to 'ard'. can take values 'ard' - Automatic Relevance Determination 'omp' - Orthogonal Matching Pursuit from sklearn.linear_model 'ard_cv' - Automatic Relevance Determination with cross-validation 'omp_cv' - Orthogonal Matching Pursuit with cross-validation from sklearn.linear_model 'ard_sk' - Automatic Relevance Determination from sklearn.linear_model starting_iter ( int , default: 5 ) \u2013 Starting iteration. Defaults to 5. resampling ( bool , default: True ) \u2013 Enable bootstrap resampling. Defaults to True. CI ( float , default: 95.0 ) \u2013 Confidence interval percentage. Defaults to 95.0. number_of_resamples ( int , default: 1000 ) \u2013 Number of bootstrap samples. Defaults to 1000. cv_tol ( float , default: 0.05 ) \u2013 Cross-validation tolerance. Defaults to 0.05. Attributes: X ( DataFrame ) \u2013 Input features dataframe. Y ( Series ) \u2013 Target variable series. X_T ( DataFrame ) \u2013 Transformed features in unit hypercube. ranges ( list ) \u2013 Ranges of transformed data. X_T_L ( DataFrame ) \u2013 Expanded features with Legendre polynomials. coef_ ( array ) \u2013 Regression coefficients. y_pred ( array ) \u2013 Predicted values. evs ( dict ) \u2013 Model evaluation statistics. results ( DataFrame ) \u2013 Sobol indices results. non_zero_coefficients ( DataFrame ) \u2013 Non-zero coefficients. shap ( DataFrame ) \u2013 Shapley effects. total ( DataFrame ) \u2013 Total sensitivity indices. surrogate_model ( DataFrame ) \u2013 Trained surrogate model for predictions. primitive_variables ( DataFrame ) \u2013 Primitive variables from Legendre expansion. poly_orders ( DataFrame ) \u2013 Polynomial orders used in Legendre expansion. delta_instance ( DataFrame ) \u2013 Instance of pawn.DeltaX or pawn.hX for delta/h indices calculation. Examples: # Initialize analyzer analyzer = rshdmr ( data_file = 'input.csv' , polys = [ 10 , 5 ], method = 'ard' ) # Run analysis sobol , shapley , total = analyzer . run_all () # Make predictions predictions = analyzer . predict ( new_data ) # Get sensitivity indices pawn_results = analyzer . get_pawn ( S = 10 ) Todo: Improve memory management for large expansions Source code in shapleyx\\shapleyx.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def __init__ ( self , data_file , polys = [ 10 , 5 ], n_jobs = - 1 , test_size = 0.25 , limit = 2.0 , k_best = 1 , p_average = 2 , n_iter = 300 , verbose = False , method = 'ard' , starting_iter = 5 , resampling = True , CI = 95.0 , number_of_resamples = 1000 , cv_tol = 0.05 ): self . read_data ( data_file ) self . n_jobs = n_jobs self . test_size = test_size self . limit = limit self . k_best = k_best self . p_average = p_average self . polys = polys self . max_1st = max ( polys ) self . n_iter = n_iter self . verbose = verbose self . method = method self . starting_iter = starting_iter self . resampling = resampling self . CI = CI self . number_of_resamples = number_of_resamples self . cv_tol = cv_tol","title":"rshdmr"},{"location":"reference/api/#shapleyx.rshdmr.eval_all_indices","text":"Evaluates Sobol indices, Shapley effects, and total sensitivity indices. Uses the indicies.eval_indices utility. Updates the following attributes self.results (pd.DataFrame): DataFrame containing Sobol indices results. self.non_zero_coefficients (pd.DataFrame): DataFrame of non-zero coefficients. self.shap (pd.DataFrame): DataFrame containing Shapley effects. self.total (pd.DataFrame): DataFrame containing total sensitivity indices. Source code in shapleyx\\shapleyx.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def eval_all_indices ( self ): \"\"\"Evaluates Sobol indices, Shapley effects, and total sensitivity indices. Uses the `indicies.eval_indices` utility. Updates the following attributes: self.results (pd.DataFrame): DataFrame containing Sobol indices results. self.non_zero_coefficients (pd.DataFrame): DataFrame of non-zero coefficients. self.shap (pd.DataFrame): DataFrame containing Shapley effects. self.total (pd.DataFrame): DataFrame containing total sensitivity indices. \"\"\" eval_indicies = indicies . eval_indices ( self . X_T_L , self . Y , self . coef_ , self . evs ) self . results = eval_indicies . get_sobol_indicies () self . non_zero_coefficients = eval_indicies . get_non_zero_coefficients () self . shap = eval_indicies . eval_shapley ( self . X . columns ) self . total = eval_indicies . eval_total_index ( self . X . columns )","title":"eval_all_indices"},{"location":"reference/api/#shapleyx.rshdmr.get_deltax","text":"Calculate delta indices for the given number of unconditioned variables and delta samples. This method initializes a DeltaX instance using the provided data and parameters, then computes the delta indices based on the specified number of unconditioned variables and delta samples. Parameters: num_unconditioned ( int ) \u2013 The number of unconditioned variables. delta_samples ( int ) \u2013 The number of delta samples to generate. Returns: DataFrame \u2013 pd.DataFrame: A DataFrame containing the computed delta indices. Source code in shapleyx\\shapleyx.py 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 def get_deltax ( self , num_unconditioned : int , delta_samples : int ) -> pd . DataFrame : \"\"\" Calculate delta indices for the given number of unconditioned variables and delta samples. This method initializes a DeltaX instance using the provided data and parameters, then computes the delta indices based on the specified number of unconditioned variables and delta samples. Args: num_unconditioned (int): The number of unconditioned variables. delta_samples (int): The number of delta samples to generate. Returns: pd.DataFrame: A DataFrame containing the computed delta indices. \"\"\" self . delta_instance = pawn . DeltaX ( self . X , self . Y , self . ranges , self . non_zero_coefficients ) delta_indices = self . delta_instance . get_deltax ( num_unconditioned , delta_samples ) return delta_indices","title":"get_deltax"},{"location":"reference/api/#shapleyx.rshdmr.get_hx","text":"Calculate delta indices for the given number of unconditioned variables and delta samples. This method initializes a DeltaX instance using the provided data and parameters, then computes the delta indices based on the specified number of unconditioned variables and delta samples. Parameters: num_unconditioned ( int ) \u2013 The number of unconditioned variables. delta_samples ( int ) \u2013 The number of delta samples to generate. Returns: DataFrame \u2013 pd.DataFrame: A DataFrame containing the computed delta indices. Source code in shapleyx\\shapleyx.py 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 def get_hx ( self , num_unconditioned : int , delta_samples : int ) -> pd . DataFrame : \"\"\" Calculate delta indices for the given number of unconditioned variables and delta samples. This method initializes a DeltaX instance using the provided data and parameters, then computes the delta indices based on the specified number of unconditioned variables and delta samples. Args: num_unconditioned (int): The number of unconditioned variables. delta_samples (int): The number of delta samples to generate. Returns: pd.DataFrame: A DataFrame containing the computed delta indices. \"\"\" self . delta_instance = pawn . hX ( self . X , self . Y , self . ranges , self . non_zero_coefficients ) delta_indices = self . delta_instance . get_hx ( num_unconditioned , delta_samples ) return delta_indices","title":"get_hx"},{"location":"reference/api/#shapleyx.rshdmr.get_pawn","text":"Estimates PAWN sensitivity indices directly from data. Uses the pawn.estimate_pawn utility. Parameters: S ( int , default: 10 ) \u2013 Number of slices/intervals for the PAWN estimation. Defaults to 10. Returns: dict \u2013 Dictionary containing the PAWN sensitivity indices for each feature. Source code in shapleyx\\shapleyx.py 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def get_pawn ( self , S = 10 ) : \"\"\"Estimates PAWN sensitivity indices directly from data. Uses the `pawn.estimate_pawn` utility. Args: S (int, optional): Number of slices/intervals for the PAWN estimation. Defaults to 10. Returns: dict: Dictionary containing the PAWN sensitivity indices for each feature. \"\"\" num_features = self . X . shape [ 1 ] pawn_results = pawn . estimate_pawn ( self . X . columns , num_features , self . X . values , self . Y , S = S ) return pawn_results","title":"get_pawn"},{"location":"reference/api/#shapleyx.rshdmr.get_pawnx","text":"Calculates PAWN sensitivity indices using the surrogate model. Uses the pawn.pawnx utility. Parameters: num_unconditioned ( int ) \u2013 Number of unconditioned samples for PAWN. num_conditioned ( int ) \u2013 Number of conditioned samples for PAWN. num_ks_samples ( int ) \u2013 Number of samples for the Kolmogorov-Smirnov test. alpha ( float , default: 0.05 ) \u2013 Significance level for the KS test. Defaults to 0.05. Returns: DataFrame \u2013 pd.DataFrame: DataFrame containing the PAWN sensitivity indices. Source code in shapleyx\\shapleyx.py 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 def get_pawnx ( self , num_unconditioned : int , num_conditioned : int , num_ks_samples : int , alpha : float = 0.05 ) -> pd . DataFrame : \"\"\"Calculates PAWN sensitivity indices using the surrogate model. Uses the `pawn.pawnx` utility. Args: num_unconditioned (int): Number of unconditioned samples for PAWN. num_conditioned (int): Number of conditioned samples for PAWN. num_ks_samples (int): Number of samples for the Kolmogorov-Smirnov test. alpha (float, optional): Significance level for the KS test. Defaults to 0.05. Returns: pd.DataFrame: DataFrame containing the PAWN sensitivity indices. \"\"\" pawn_instance = pawn . pawnx ( self . X , self . Y , self . ranges , self . non_zero_coefficients ) pawn_indices = pawn_instance . get_pawnx ( num_unconditioned , num_conditioned , num_ks_samples , alpha ) return pawn_indices","title":"get_pawnx"},{"location":"reference/api/#shapleyx.rshdmr.get_pruned_data","text":"Generates a pruned dataset containing only the features with non-zero coefficients. This method creates a new DataFrame that includes only the columns from the original dataset ( X_T_L ) that correspond to the labels with non-zero coefficients. Additionally, it includes the target variable ( Y ). Returns: \u2013 pd.DataFrame: A DataFrame containing the pruned data with selected features and the target variable. Source code in shapleyx\\shapleyx.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def get_pruned_data ( self ): \"\"\" Generates a pruned dataset containing only the features with non-zero coefficients. This method creates a new DataFrame that includes only the columns from the original dataset (`X_T_L`) that correspond to the labels with non-zero coefficients. Additionally, it includes the target variable (`Y`). Returns: pd.DataFrame: A DataFrame containing the pruned data with selected features and the target variable. \"\"\" pruned_data = pd . DataFrame () for label in self . non_zero_coefficients [ 'labels' ] : pruned_data [ label ] = self . X_T_L [ label ] pruned_data [ 'Y' ] = self . Y return pruned_data","title":"get_pruned_data"},{"location":"reference/api/#shapleyx.rshdmr.legendre_expand","text":"Performs Legendre polynomial expansion on the transformed data self.X_T . Uses the legendre.legendre_expand utility. Updates the following attributes self.primitive_variables: Primitive variables from the expansion. self.poly_orders: Polynomial orders used in the expansion. self.X_T_L (pd.DataFrame): The expanded data matrix including Legendre terms. Source code in shapleyx\\shapleyx.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def legendre_expand ( self ): \"\"\"Performs Legendre polynomial expansion on the transformed data `self.X_T`. Uses the `legendre.legendre_expand` utility. Updates the following attributes: self.primitive_variables: Primitive variables from the expansion. self.poly_orders: Polynomial orders used in the expansion. self.X_T_L (pd.DataFrame): The expanded data matrix including Legendre terms. \"\"\" expansion_data = legendre . legendre_expand ( self . X_T , self . polys ) expansion_data . build_basis_set () self . primitive_variables = expansion_data . get_primitive_variables () self . poly_orders = expansion_data . get_poly_orders () self . X_T_L = expansion_data . get_expanded ()","title":"legendre_expand"},{"location":"reference/api/#shapleyx.rshdmr.plot_hdmr","text":"Plots the High-Dimensional Model Representation (HDMR) of the model's predictions. This method uses the plot_hdmr function from the stats module to visualize the HDMR of the actual values ( self.Y ) against the predicted values ( self.y_pred ). Returns: \u2013 None Source code in shapleyx\\shapleyx.py 244 245 246 247 248 249 250 251 252 253 254 def plot_hdmr ( self ): \"\"\" Plots the High-Dimensional Model Representation (HDMR) of the model's predictions. This method uses the `plot_hdmr` function from the `stats` module to visualize the HDMR of the actual values (`self.Y`) against the predicted values (`self.y_pred`). Returns: None \"\"\" stats . plot_hdmr ( self . Y , self . y_pred )","title":"plot_hdmr"},{"location":"reference/api/#shapleyx.rshdmr.predict","text":"Predicts output for new input data using the trained surrogate model. If the surrogate model ( self.surrogate_model ) doesn't exist, it first creates and fits one using predictor.surrogate . Parameters: X ( array - like ) \u2013 Input data for which predictions are to be made. Should have the same features as the original training data. Returns: \u2013 array-like: Predicted output values. Source code in shapleyx\\shapleyx.py 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def predict ( self , X ): \"\"\"Predicts output for new input data using the trained surrogate model. If the surrogate model (`self.surrogate_model`) doesn't exist, it first creates and fits one using `predictor.surrogate`. Args: X (array-like): Input data for which predictions are to be made. Should have the same features as the original training data. Returns: array-like: Predicted output values. \"\"\" if not hasattr ( self , 'surrogate_model' ): self . surrogate_model = predictor . surrogate ( self . non_zero_coefficients , self . ranges ) self . surrogate_model . fit ( self . X , self . Y ) return self . surrogate_model . predict ( X )","title":"predict"},{"location":"reference/api/#shapleyx.rshdmr.read_data","text":"Reads data from a file or DataFrame. Initializes the self.X (features) and self.Y (target) attributes. Parameters: data_file ( str or DataFrame ) \u2013 Path to the data file (CSV) or a pandas DataFrame. Raises: ValueError \u2013 If data_file is not a string path or a DataFrame. Source code in shapleyx\\shapleyx.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def read_data ( self , data_file ): \"\"\"Reads data from a file or DataFrame. Initializes the `self.X` (features) and `self.Y` (target) attributes. Args: data_file (str or pd.DataFrame): Path to the data file (CSV) or a pandas DataFrame. Raises: ValueError: If `data_file` is not a string path or a DataFrame. \"\"\" if isinstance ( data_file , pd . DataFrame ): print ( 'Found a DataFrame' ) df = data_file elif isinstance ( data_file , str ): df = pd . read_csv ( data_file ) else : raise ValueError ( \"data_file must be either a pandas DataFrame or a file path (str).\" ) self . Y = df [ 'Y' ] self . X = df . drop ( 'Y' , axis = 1 ) # Clean up the original DataFrame to save memory del df","title":"read_data"},{"location":"reference/api/#shapleyx.rshdmr.run_all","text":"Execute a complete sequence of steps for RS-HDMR (Random Sampling High-Dimensional Model Representation) analysis. This method performs the following steps in sequence: Transforms the input data to a unit hypercube. Builds basis functions using Legendre polynomials. Runs regression analysis to fit the model. Calculates and displays RS-HDMR model performance statistics. Evaluates Sobol indices to quantify the contribution of each input variable to the output variance. Calculates Shapley effects to measure the importance of each input variable. Computes the total index to assess the overall impact of input variables. If resampling is enabled, performs bootstrap resampling to estimate confidence intervals for Sobol indices and Shapley effects. Prints a completion message with a randomly selected quote. Returns: tuple \u2013 A tuple containing three elements: sobol_indices (pd.DataFrame): A DataFrame containing Sobol indices for each input variable. shapley_effects (pd.DataFrame): A DataFrame containing Shapley effects for each input variable. total_index (pd.DataFrame): A DataFrame containing the total index for each input variable. Notes The method assumes that the necessary data and configurations are already set in the class instance. If resampling is enabled ( self.resampling is True), confidence intervals (CIs) are calculated for Sobol indices and Shapley effects. The method uses helper functions like transform_data , legendre_expand , run_regression , stats , plot_hdmr , eval_sobol_indices , get_shapley , and get_total_index to perform specific tasks. The completion message includes a randomly selected quote for a touch of inspiration. Example sobol_indices, shapley_effects, total_index = instance.run_all() print(sobol_indices) print(shapley_effects) print(total_index) Source code in shapleyx\\shapleyx.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def run_all ( self ): \"\"\" Execute a complete sequence of steps for RS-HDMR (Random Sampling High-Dimensional Model Representation) analysis. This method performs the following steps in sequence: 1. Transforms the input data to a unit hypercube. 2. Builds basis functions using Legendre polynomials. 3. Runs regression analysis to fit the model. 4. Calculates and displays RS-HDMR model performance statistics. 5. Evaluates Sobol indices to quantify the contribution of each input variable to the output variance. 6. Calculates Shapley effects to measure the importance of each input variable. 7. Computes the total index to assess the overall impact of input variables. 8. If resampling is enabled, performs bootstrap resampling to estimate confidence intervals for Sobol indices and Shapley effects. 9. Prints a completion message with a randomly selected quote. Returns: tuple: A tuple containing three elements: - sobol_indices (pd.DataFrame): A DataFrame containing Sobol indices for each input variable. - shapley_effects (pd.DataFrame): A DataFrame containing Shapley effects for each input variable. - total_index (pd.DataFrame): A DataFrame containing the total index for each input variable. Notes: - The method assumes that the necessary data and configurations are already set in the class instance. - If resampling is enabled (`self.resampling` is True), confidence intervals (CIs) are calculated for Sobol indices and Shapley effects. - The method uses helper functions like `transform_data`, `legendre_expand`, `run_regression`, `stats`, `plot_hdmr`, `eval_sobol_indices`, `get_shapley`, and `get_total_index` to perform specific tasks. - The completion message includes a randomly selected quote for a touch of inspiration. Example: >>> sobol_indices, shapley_effects, total_index = instance.run_all() >>> print(sobol_indices) >>> print(shapley_effects) >>> print(total_index) \"\"\" # Define a helper function to print headings def print_step ( step_name ): print_heading ( step_name ) # Step 1: Transform data to unit hypercube print_step ( 'Transforming data to unit hypercube' ) self . transform_data () # Step 2: Build basis functions print_step ( 'Building basis functions' ) self . legendre_expand () # Step 3: Run regression analysis print_step ( 'Running regression analysis' ) self . run_regression () # Step 4: Calculate RS-HDMR model performance statistics print_step ( 'RS-HDMR model performance statistics' ) self . stats () print () self . plot_hdmr () # Step 5: Evaluate Sobol indices self . eval_all_indices () sobol_indices = self . results . drop ( columns = [ 'labels' , 'coeff' ]) # Step 6: Calculate Shapley effects shapley_effects = self . shap # Step 7: Calculate total index total_index = self . total # Step 8: Perform resampling if enabled if self . resampling : print_step ( f 'Running bootstrap resampling { self . number_of_resamples } samples for { self . CI } % CI' ) do_resampling = resampling . resampling ( self . get_pruned_data (), self . number_of_resamples , self . X . columns ) do_resampling . do_resampling () sobol_indices = do_resampling . get_sobol_quantiles ( sobol_indices , self . CI ) # Calculate quantiles for Shapley effects shapley_effects = do_resampling . get_shap_quantiles ( shapley_effects , self . CI ) print_step ( 'Completed bootstrap resampling' ) # Step 9: Print completion message with a quote quote = quotes . get_quote () message = ( \" Completed all analysis \\n \" \" ------------------------ \\n\\n \" f \" { textwrap . fill ( quote , 58 ) } \" ) print_step ( message ) return sobol_indices , shapley_effects , total_index","title":"run_all"},{"location":"reference/api/#shapleyx.rshdmr.run_regression","text":"Runs the regression analysis using the specified method. Uses the regression.regression utility based on self.method . Updates the following attributes self.coef_ (np.array): The regression coefficients obtained from the fit. self.y_pred (np.array): The predicted values based on the fitted model. Source code in shapleyx\\shapleyx.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def run_regression ( self ): \"\"\"Runs the regression analysis using the specified method. Uses the `regression.regression` utility based on `self.method`. Updates the following attributes: self.coef_ (np.array): The regression coefficients obtained from the fit. self.y_pred (np.array): The predicted values based on the fitted model. \"\"\" regression_instance = regression . regression ( X_T_L = self . X_T_L , Y = self . Y , method = self . method , n_iter = self . n_iter , verbose = self . verbose , cv_tol = self . cv_tol , starting_iter = self . starting_iter ) self . coef_ , self . y_pred = regression_instance . run_regression ()","title":"run_regression"},{"location":"reference/api/#shapleyx.rshdmr.stats","text":"Calculates and stores evaluation statistics for the fitted model. Uses the stats.stats utility. Updates the following attributes self.evs (dict): A dictionary containing evaluation statistics (e.g., R^2, MSE). Source code in shapleyx\\shapleyx.py 234 235 236 237 238 239 240 241 242 def stats ( self ): \"\"\"Calculates and stores evaluation statistics for the fitted model. Uses the `stats.stats` utility. Updates the following attributes: self.evs (dict): A dictionary containing evaluation statistics (e.g., R^2, MSE). \"\"\" self . evs = stats . stats ( self . Y , self . y_pred , self . coef_ )","title":"stats"},{"location":"reference/api/#shapleyx.rshdmr.transform_data","text":"Transforms the input data self.X into a unit hypercube. Updates the following attributes self.ranges (list): The ranges (min, max) of the original data features. self.X_T (pd.DataFrame): The transformed data matrix within the unit hypercube. Source code in shapleyx\\shapleyx.py 183 184 185 186 187 188 189 190 191 192 193 def transform_data ( self ): \"\"\"Transforms the input data `self.X` into a unit hypercube. Updates the following attributes: self.ranges (list): The ranges (min, max) of the original data features. self.X_T (pd.DataFrame): The transformed data matrix within the unit hypercube. \"\"\" transformed_data = transformation . transformation ( self . X ) transformed_data . do_transform () self . ranges = transformed_data . get_ranges () self . X_T = transformed_data . get_X_T ()","title":"transform_data"},{"location":"reference/api/#shapleyx.ARD","text":"","title":"ARD"},{"location":"reference/api/#shapleyx.ARD.RegressionARD","text":"Bases: RegressorMixin , LinearModel Regression with Automatic Relevance Determination (ARD) using Sparse Bayesian Learning. This class implements a fast version of ARD regression, which is a Bayesian approach to regression that automatically determines the relevance of each feature. It is based on the Sparse Bayesian Learning (SBL) algorithm, which promotes sparsity in the model by estimating the precision of the coefficients.","title":"RegressionARD"},{"location":"reference/api/#shapleyx.ARD.RegressionARD--parameters","text":"n_iter : int, optional (default=300) Maximum number of iterations for the optimization algorithm. float, optional (default=1e-3) Convergence threshold. If the absolute change in the precision parameter for the weights is below this threshold, the algorithm terminates. bool, optional (default=True) Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (e.g., data is expected to be already centered). bool, optional (default=True) If True, X will be copied; else, it may be overwritten. bool, optional (default=False) If True, the algorithm will print progress messages during fitting. float, optional (default=0.1) Tolerance for cross-validation. If the percentage change in cross-validation score is below this threshold, the algorithm terminates. bool, optional (default=False) If True, cross-validation will be used to determine the optimal number of features.","title":"Parameters"},{"location":"reference/api/#shapleyx.ARD.RegressionARD--attributes","text":"coef_ : array, shape (n_features,) Coefficients of the regression model (mean of the posterior distribution). float Estimated precision of the noise. array, dtype=bool, shape (n_features,) Boolean array indicating which features are active (non-zero coefficients). array, shape (n_features,) Estimated precisions of the coefficients. array, shape (n_features, n_features) Estimated covariance matrix of the weights, computed only for non-zero coefficients. list List of cross-validation scores if cv is True.","title":"Attributes"},{"location":"reference/api/#shapleyx.ARD.RegressionARD--methods","text":"fit(X, y) Fit the ARD regression model to the data. predict_dist(X) Compute the predictive distribution for the test set. _center_data(X, y) Center the data by subtracting the mean. _posterior_dist(A, beta, XX, XY, full_covar=False) Calculate the mean and covariance matrix of the posterior distribution of coefficients. _sparsity_quality(XX, XXd, XY, XYa, Aa, Ri, active, beta, cholesky) Calculate sparsity and quality parameters for each feature.","title":"Methods"},{"location":"reference/api/#shapleyx.ARD.RegressionARD--references","text":"[1] Tipping, M. E., & Faul, A. C. (2003). Fast marginal likelihood maximisation for sparse Bayesian models. In Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics (pp. 276-283). [2] Tipping, M. E., & Faul, A. C. (2001). Analysis of sparse Bayesian learning. In Advances in Neural Information Processing Systems (pp. 383-389). Source code in shapleyx\\ARD.py 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , n_iter = 300 , tol = 1e-3 , fit_intercept = True , copy_X = True , verbose = False , cv_tol = 0.1 , cv = False ): self . n_iter = n_iter self . tol = tol self . scores_ = list () self . fit_intercept = fit_intercept self . copy_X = copy_X self . verbose = verbose self . cv = cv self . cv_tol = cv_tol","title":"References"},{"location":"reference/api/#shapleyx.ARD.RegressionARD.fit","text":"Fit the ARD regression model to the data.","title":"fit"},{"location":"reference/api/#shapleyx.ARD.RegressionARD.fit--parameters","text":"X : {array-like, sparse matrix}, shape (n_samples, n_features) Training data, matrix of explanatory variables. array-like, shape (n_samples,) Target values.","title":"Parameters"},{"location":"reference/api/#shapleyx.ARD.RegressionARD.fit--returns","text":"self : object Returns the instance itself. Source code in shapleyx\\ARD.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def fit ( self , X , y ): ''' Fit the ARD regression model to the data. Parameters ---------- X : {array-like, sparse matrix}, shape (n_samples, n_features) Training data, matrix of explanatory variables. y : array-like, shape (n_samples,) Target values. Returns ------- self : object Returns the instance itself. ''' X , y = check_X_y ( X , y , dtype = np . float64 , y_numeric = True ) X , y , X_mean , y_mean , X_std = self . _center_data ( X , y ) n_samples , n_features = X . shape cv_list = [] cv_score_history = [] current_r = 0 # precompute X'*Y , X'*X for faster iterations & allocate memory for # sparsity & quality vectors XY = np . dot ( X . T , y ) XX = np . dot ( X . T , X ) XXd = np . diag ( XX ) # initialise precision of noise & and coefficients var_y = np . var ( y ) # check that variance is non zero !!! if var_y == 0 : beta = 1e-2 else : beta = 1. / np . var ( y ) A = np . PINF * np . ones ( n_features ) active = np . zeros ( n_features , dtype = bool ) # in case of almost perfect multicollinearity between some features # start from feature 0 if np . sum ( XXd - X_mean ** 2 < np . finfo ( np . float32 ) . eps ) > 0 : A [ 0 ] = np . finfo ( np . float16 ) . eps active [ 0 ] = True else : # start from a single basis vector with largest projection on targets proj = XY ** 2 / XXd start = np . argmax ( proj ) active [ start ] = True A [ start ] = XXd [ start ] / ( proj [ start ] - var_y ) warning_flag = 0 for i in range ( self . n_iter ): XXa = XX [ active ,:][:, active ] XYa = XY [ active ] Aa = A [ active ] # mean & covariance of posterior distribution Mn , Ri , cholesky = self . _posterior_dist ( Aa , beta , XXa , XYa ) if cholesky : Sdiag = np . sum ( Ri ** 2 , 0 ) else : Sdiag = np . copy ( np . diag ( Ri )) warning_flag += 1 # raise warning in case cholesky failes if warning_flag == 1 : warnings . warn (( \"Cholesky decomposition failed ! Algorithm uses pinvh, \" \"which is significantly slower, if you use RVR it \" \"is advised to change parameters of kernel\" )) # compute quality & sparsity parameters s , q , S , Q = self . _sparsity_quality ( XX , XXd , XY , XYa , Aa , Ri , active , beta , cholesky ) # update precision parameter for noise distribution rss = np . sum ( ( y - np . dot ( X [:, active ] , Mn ) ) ** 2 ) beta = n_samples - np . sum ( active ) + np . sum ( Aa * Sdiag ) beta /= ( rss + np . finfo ( np . float32 ) . eps ) # update precision parameters of coefficients A , converged = update_precisions ( Q , S , q , s , A , active , self . tol , n_samples , False ) # *************************************** if self . cv : # Select features based on the 'active' mask # Assumes X is a numpy array for efficient slicing X_active = X [:, active ] # Define the model for cross-validation (instantiated fresh each time) cv_model = linear_model . Ridge () try : # Perform 10-fold cross-validation, explicitly using R^2 scoring # Ensure 'y' corresponds correctly to 'X_active' cv_scores = cross_val_score ( cv_model , X_active , y , cv = 10 , scoring = 'r2' ) new_cv_score = np . mean ( cv_scores ) # Use numpy mean for clarity # Calculate percentage change, handling division by zero # Assumes current_cv_score is initialized (e.g., to None or 0.0) before the loop if current_cv_score is not None and current_cv_score != 0 : percentage_change = ( new_cv_score - current_cv_score ) / current_cv_score * 100 elif new_cv_score == 0 and ( current_cv_score is None or current_cv_score == 0 ): percentage_change = 0.0 # No change if both old and new scores are zero else : # Handle cases where current_cv_score is None (first iteration) or zero percentage_change = np . inf # Indicate a large change if starting from zero/None # Optional: Replace print with logging for better control in applications # Assumes 'i' is an iteration counter from an outer loop print ( f \"Iteration { i } : CV Score = { new_cv_score : .4f } , % Change = { percentage_change : .2f } %\" ) # Check for convergence based on the absolute percentage change # Assumes cv_tol is a positive threshold for the magnitude of change # Assumes 'converged' is initialized (e.g., to False) before the loop if current_cv_score is not None and abs ( percentage_change ) < self . cv_tol : converged = True # Consider adding a 'break' here if the loop should terminate immediately upon convergence # Update the current score and history # Assumes cv_score_history is initialized (e.g., as []) before the loop current_cv_score = new_cv_score cv_score_history . append ( new_cv_score ) except ValueError as ve : # Catch specific errors, e.g., if X_active becomes empty or has incompatible dimensions print ( f \"Warning: Cross-validation failed at iteration { i } due to ValueError: { ve } \" ) # Decide how to handle: stop, skip, assign default score? # Example: Treat as no improvement or break percentage_change = np . nan # Mark as invalid # converged = True # Option: Stop if CV fails except Exception as e : # Catch other potential errors during cross-validation print ( f \"Warning: Cross-validation failed unexpectedly at iteration { i } : { e } \" ) percentage_change = np . nan # converged = True # Option: Stop if CV fails # Calculate active features once per iteration num_active_features = np . sum ( active ) if self . verbose : # Use logging (assuming logger is configured) and f-string for iteration progress # import logging # Ensure logging is imported at the top of the file logging . info ( f \"Iteration: { i } , Active Features: { num_active_features } \" ) # Check for convergence or max iterations to terminate if converged or i == self . n_iter - 1 : # Construct the final status message final_status = f \"Finished at Iteration: { i } , Active Features: { num_active_features } .\" if converged : log_level = logging . INFO # Normal convergence final_status += \" Algorithm converged.\" # The original code printed \"Algorithm converged !\" only if verbose. # Logging INFO level covers this sufficiently. Add DEBUG if more detail needed. # if self.verbose: # logging.debug(\"Convergence details: ...\") else : # i == self.n_iter - 1 log_level = logging . WARNING # Reached max iterations without converging final_status += f \" Reached maximum iterations ( { self . n_iter } ).\" # Log the final status logging . log ( log_level , final_status ) break # Exit the loop #print(('Iteration: {0}, number of features ' # 'in the model: {1}').format(i,np.sum(active))) # after last update of alpha & beta update parameters # of posterior distribution XXa , XYa , Aa = XX [ active ,:][:, active ], XY [ active ], A [ active ] Mn , Sn , cholesky = self . _posterior_dist ( Aa , beta , XXa , XYa , True ) self . coef_ = np . zeros ( n_features ) self . coef_ [ active ] = Mn self . sigma_ = Sn self . active_ = active self . lambda_ = A self . alpha_ = beta self . _set_intercept ( X_mean , y_mean , X_std ) if self . cv : # print(max(enumerate(cv_list), key=lambda x: x[1])) print (( 'Iteration: {0} , number of features ' 'in the model: {1} ' ) . format ( i , np . sum ( active ))) return self","title":"Returns"},{"location":"reference/api/#shapleyx.ARD.RegressionARD.predict_dist","text":"Computes predictive distribution for test set. Predictive distribution for each data point is one dimensional Gaussian and therefore is characterised by mean and variance.","title":"predict_dist"},{"location":"reference/api/#shapleyx.ARD.RegressionARD.predict_dist--parameters","text":"X : {array-like, sparse matrix}, shape (n_samples_test, n_features) Test data, matrix of explanatory variables.","title":"Parameters"},{"location":"reference/api/#shapleyx.ARD.RegressionARD.predict_dist--returns","text":"y_hat : array, shape (n_samples_test,) Estimated values of targets on the test set (mean of the predictive distribution). array, shape (n_samples_test,) Variance of the predictive distribution. Source code in shapleyx\\ARD.py 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 def predict_dist ( self , X ): ''' Computes predictive distribution for test set. Predictive distribution for each data point is one dimensional Gaussian and therefore is characterised by mean and variance. Parameters ---------- X : {array-like, sparse matrix}, shape (n_samples_test, n_features) Test data, matrix of explanatory variables. Returns ------- y_hat : array, shape (n_samples_test,) Estimated values of targets on the test set (mean of the predictive distribution). var_hat : array, shape (n_samples_test,) Variance of the predictive distribution. ''' y_hat = self . _decision_function ( X ) var_hat = 1. / self . alpha_ var_hat += np . sum ( np . dot ( X [:, self . active_ ], self . sigma_ ) * X [:, self . active_ ], axis = 1 ) return y_hat , var_hat","title":"Returns"},{"location":"reference/api/#shapleyx.ARD.update_precisions","text":"Updates the precision parameters (alpha) for features in a sparse Bayesian learning model by selecting a feature to add, recompute, or delete based on its impact on the log marginal likelihood. The function also checks for convergence.","title":"update_precisions"},{"location":"reference/api/#shapleyx.ARD.update_precisions--parameters","text":"Q : numpy.ndarray Quality parameters for all features. S : numpy.ndarray Sparsity parameters for all features. q : numpy.ndarray Quality parameters for features currently in the model. s : numpy.ndarray Sparsity parameters for features currently in the model. A : numpy.ndarray Precision parameters (alpha) for all features. active : numpy.ndarray (bool) Boolean array indicating whether each feature is currently in the model. tol : float Tolerance threshold for determining convergence based on changes in precision. n_samples : int Number of samples in the dataset, used to normalize the change in log marginal likelihood. clf_bias : bool Flag indicating whether the model includes a bias term (used in classification tasks).","title":"Parameters:"},{"location":"reference/api/#shapleyx.ARD.update_precisions--returns","text":"list A list containing two elements: - Updated precision parameters (A) for all features. - A boolean flag indicating whether the model has converged.","title":"Returns:"},{"location":"reference/api/#shapleyx.ARD.update_precisions--notes","text":"The function performs the following steps: 1. Computes the change in log marginal likelihood for adding, recomputing, or deleting features. 2. Identifies the feature that causes the largest change in likelihood. 3. Updates the precision parameter (alpha) for the selected feature. 4. Checks for convergence based on whether no features are added/deleted and changes in precision are below the specified tolerance. 5. Returns the updated precision parameters and convergence status. Convergence is determined by two conditions: - No features are added or deleted. - The change in precision for features already in the model is below the tolerance threshold. The function ensures that the bias term is not removed in classification tasks. Source code in shapleyx\\ARD.py 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 def update_precisions ( Q , S , q , s , A , active , tol , n_samples , clf_bias ): ''' Updates the precision parameters (alpha) for features in a sparse Bayesian learning model by selecting a feature to add, recompute, or delete based on its impact on the log marginal likelihood. The function also checks for convergence. Parameters: ----------- Q : numpy.ndarray Quality parameters for all features. S : numpy.ndarray Sparsity parameters for all features. q : numpy.ndarray Quality parameters for features currently in the model. s : numpy.ndarray Sparsity parameters for features currently in the model. A : numpy.ndarray Precision parameters (alpha) for all features. active : numpy.ndarray (bool) Boolean array indicating whether each feature is currently in the model. tol : float Tolerance threshold for determining convergence based on changes in precision. n_samples : int Number of samples in the dataset, used to normalize the change in log marginal likelihood. clf_bias : bool Flag indicating whether the model includes a bias term (used in classification tasks). Returns: -------- list A list containing two elements: - Updated precision parameters (A) for all features. - A boolean flag indicating whether the model has converged. Notes: ------ The function performs the following steps: 1. Computes the change in log marginal likelihood for adding, recomputing, or deleting features. 2. Identifies the feature that causes the largest change in likelihood. 3. Updates the precision parameter (alpha) for the selected feature. 4. Checks for convergence based on whether no features are added/deleted and changes in precision are below the specified tolerance. 5. Returns the updated precision parameters and convergence status. Convergence is determined by two conditions: - No features are added or deleted. - The change in precision for features already in the model is below the tolerance threshold. The function ensures that the bias term is not removed in classification tasks. ''' # initialise vector holding changes in log marginal likelihood deltaL = np . zeros ( Q . shape [ 0 ]) # identify features that can be added , recomputed and deleted in model theta = q ** 2 - s add = ( theta > 0 ) * ( active == False ) recompute = ( theta > 0 ) * ( active == True ) delete = ~ ( add + recompute ) # compute sparsity & quality parameters corresponding to features in # three groups identified above Qadd , Sadd = Q [ add ], S [ add ] Qrec , Srec , Arec = Q [ recompute ], S [ recompute ], A [ recompute ] Qdel , Sdel , Adel = Q [ delete ], S [ delete ], A [ delete ] # compute new alpha's (precision parameters) for features that are # currently in model and will be recomputed Anew = s [ recompute ] ** 2 / ( theta [ recompute ] + np . finfo ( np . float32 ) . eps ) delta_alpha = ( 1. / Anew - 1. / Arec ) # compute change in log marginal likelihood deltaL [ add ] = ( Qadd ** 2 - Sadd ) / Sadd + np . log ( Sadd / Qadd ** 2 ) deltaL [ recompute ] = Qrec ** 2 / ( Srec + 1. / delta_alpha ) - np . log ( 1 + Srec * delta_alpha ) deltaL [ delete ] = Qdel ** 2 / ( Sdel - Adel ) - np . log ( 1 - Sdel / Adel ) deltaL = deltaL / n_samples # find feature which caused largest change in likelihood feature_index = np . argmax ( deltaL ) # no deletions or additions same_features = np . sum ( theta [ ~ recompute ] > 0 ) == 0 # changes in precision for features already in model is below threshold no_delta = np . sum ( abs ( Anew - Arec ) > tol ) == 0 # check convergence: if no features to add or delete and small change in # precision for current features then terminate converged = False if same_features and no_delta : converged = True return [ A , converged ] # if not converged update precision parameter of weights and return if theta [ feature_index ] > 0 : A [ feature_index ] = s [ feature_index ] ** 2 / theta [ feature_index ] if active [ feature_index ] == False : active [ feature_index ] = True else : # at least two active features if active [ feature_index ] == True and np . sum ( active ) >= 2 : # do not remove bias term in classification # (in regression it is factored in through centering) if not ( feature_index == 0 and clf_bias ): active [ feature_index ] = False A [ feature_index ] = np . PINF return [ A , converged ]","title":"Notes:"},{"location":"reference/api/#shapleyx.xsampler","text":"","title":"xsampler"},{"location":"reference/api/#shapleyx.xsampler.xsampler","text":"Generate a Latin Hypercube sample scaled to the specified ranges. Parameters: num_samples ( int ) \u2013 Number of samples to generate. ranges ( dict ) \u2013 A dictionary where keys are feature names and values are tuples of (lower, upper) bounds. Returns: ndarray \u2013 np.ndarray: A scaled Latin Hypercube sample of shape (num_samples, num_features). Source code in shapleyx\\xsampler.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def xsampler ( num_samples : int , ranges : dict ) -> np . ndarray : \"\"\" Generate a Latin Hypercube sample scaled to the specified ranges. Args: num_samples (int): Number of samples to generate. ranges (dict): A dictionary where keys are feature names and values are tuples of (lower, upper) bounds. Returns: np.ndarray: A scaled Latin Hypercube sample of shape (num_samples, num_features). \"\"\" num_features = len ( ranges ) # Extract lower and upper bounds from the ranges dictionary lower_bounds = [ bounds [ 0 ] for bounds in ranges . values ()] upper_bounds = [ bounds [ 1 ] for bounds in ranges . values ()] # Generate Latin Hypercube sample sampler = qmc . LatinHypercube ( d = num_features ) sample = sampler . random ( n = num_samples ) # Scale the sample to the specified ranges sample_scaled = qmc . scale ( sample , lower_bounds , upper_bounds ) return sample_scaled","title":"xsampler"},{"location":"tutorials/basic-usage/","text":"Basic Usage Tutorial This tutorial walks through a complete sensitivity analysis workflow with ShapleyX. Step 1: Prepare Your Data import pandas as pd import numpy as np # Generate sample data np . random . seed ( 42 ) X = np . random . rand ( 1000 , 5 ) # 1000 samples, 5 parameters Y = X [:, 0 ] ** 2 + 2 * X [:, 1 ] * X [:, 2 ] + np . sin ( X [:, 3 ]) + X [:, 4 ] # Create DataFrame data = pd . DataFrame ( X , columns = [ 'x1' , 'x2' , 'x3' , 'x4' , 'x5' ]) data [ 'Y' ] = Y data . to_csv ( 'sample_data.csv' , index = False ) Step 2: Initialize the Analyzer from shapleyx import rshdmr analyzer = rshdmr ( data_file = 'sample_data.csv' , polys = [ 10 , 5 ], # Polynomial orders method = 'ard' , # Automatic Relevance Determination verbose = True ) Step 3: Run the Analysis # Run complete analysis pipeline sobol_indices , shapley_effects , total_index = analyzer . run_all () # View results print ( \"Sobol Indices:\" ) print ( sobol_indices ) print ( \" \\n Shapley Effects:\" ) print ( shapley_effects ) print ( \" \\n Total Indices:\" ) print ( total_index ) Step 4: Visualize Results # Plot predicted vs actual analyzer . plot_hdmr () # Plot sensitivity indices analyzer . plot_indices () Next Steps Try with your own dataset Experiment with different polynomial orders Explore advanced configuration options","title":"Basic Usage"},{"location":"tutorials/basic-usage/#basic-usage-tutorial","text":"This tutorial walks through a complete sensitivity analysis workflow with ShapleyX.","title":"Basic Usage Tutorial"},{"location":"tutorials/basic-usage/#step-1-prepare-your-data","text":"import pandas as pd import numpy as np # Generate sample data np . random . seed ( 42 ) X = np . random . rand ( 1000 , 5 ) # 1000 samples, 5 parameters Y = X [:, 0 ] ** 2 + 2 * X [:, 1 ] * X [:, 2 ] + np . sin ( X [:, 3 ]) + X [:, 4 ] # Create DataFrame data = pd . DataFrame ( X , columns = [ 'x1' , 'x2' , 'x3' , 'x4' , 'x5' ]) data [ 'Y' ] = Y data . to_csv ( 'sample_data.csv' , index = False )","title":"Step 1: Prepare Your Data"},{"location":"tutorials/basic-usage/#step-2-initialize-the-analyzer","text":"from shapleyx import rshdmr analyzer = rshdmr ( data_file = 'sample_data.csv' , polys = [ 10 , 5 ], # Polynomial orders method = 'ard' , # Automatic Relevance Determination verbose = True )","title":"Step 2: Initialize the Analyzer"},{"location":"tutorials/basic-usage/#step-3-run-the-analysis","text":"# Run complete analysis pipeline sobol_indices , shapley_effects , total_index = analyzer . run_all () # View results print ( \"Sobol Indices:\" ) print ( sobol_indices ) print ( \" \\n Shapley Effects:\" ) print ( shapley_effects ) print ( \" \\n Total Indices:\" ) print ( total_index )","title":"Step 3: Run the Analysis"},{"location":"tutorials/basic-usage/#step-4-visualize-results","text":"# Plot predicted vs actual analyzer . plot_hdmr () # Plot sensitivity indices analyzer . plot_indices ()","title":"Step 4: Visualize Results"},{"location":"tutorials/basic-usage/#next-steps","text":"Try with your own dataset Experiment with different polynomial orders Explore advanced configuration options","title":"Next Steps"}]}