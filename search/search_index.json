{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ShapleyX Documentation","text":"<p>Welcome to the ShapleyX documentation! </p> <p>ShapleyX is a Python package for global sensitivity analysis using Sparse Random Sampling - High Dimensional Model Representation (HDMR) with Group Method of Data Handling (GMDH) for parameter selection and linear regression for parameter refinement.</p>"},{"location":"#documentation-sections","title":"Documentation Sections","text":"<ul> <li>Getting Started: Installation and basic setup</li> <li>Tutorials: Step-by-step guides for common tasks</li> <li>How-to Guides: Solutions to specific problems</li> <li>Reference: Complete API documentation</li> <li>Explanation: Background and theory</li> </ul> <pre><code>import shapleyx\n\n# Initialize RS-HDMR analyzer\nanalyzer = shapleyx.rshdmr(data_file='input_data.csv', polys=[10, 5], method='ard')\n\n# Run the entire analysis pipeline\nsobol_indices, shapley_effects, total_index = analyzer.run_all()\n</code></pre>"},{"location":"explanation/theory/","title":"Theoretical Background","text":""},{"location":"explanation/theory/#shapley-values-in-sensitivity-analysis","title":"Shapley Values in Sensitivity Analysis","text":"<p>Shapley values originate from cooperative game theory and provide a principled way to:</p> <ol> <li>Fairly distribute the \"payout\" (output variance) among \"players\" (input parameters)</li> <li>Account for all possible interaction effects</li> <li>Provide unique attribution under certain axioms</li> </ol>"},{"location":"explanation/theory/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>For a model output \\( Y = f(X_1, X_2, \\dots, X_d)\\) , the Shapley effect \\(\\phi_i\\) for parameter \\(X_i\\) is:</p> \\[ \\phi_i = \\sum_{S \\subseteq D \\setminus \\{i\\}} \\frac{|S|!(d-|S|-1)!}{d!} \\left[\\text{Var}\\big(E[Y|X_S \\cup \\{i\\}]\\big) - \\text{Var}\\big(E[Y|X_S]\\big)\\right] \\] <p>where: - \\(D\\) is the set of all parameters - \\(S\\) is a subset of parameters excluding \\(i\\) - \\(X_S\\) represents the parameters in subset \\(S\\) For a model output $$ Y = f(X_1, X_2, \\ldots, X_d) $$, the Shapley effect \\(\\(\\phi_i\\)\\) for parameter \\(\\(X_i\\)\\) is:</p> \\[ \\phi_i = \\sum_{S \\subseteq D \\setminus \\{i\\}} \\frac{|S|!(d-|S|-1)!}{d!} [\\text{Var}(E[Y|X_S \\cup \\{i\\}]) - \\text{Var}(E[Y|X_S])] \\] <p>where: - \\(D\\) is the set of all parameters - \\(S\\) is a subset of parameters excluding \\(i\\) - \\(X_S\\) represents the parameters in subset \\(S\\)</p>"},{"location":"explanation/theory/#relationship-to-sobol-indices","title":"Relationship to Sobol Indices","text":"<p>Shapley effects generalize Sobol indices by: - Combining all order effects involving a parameter - Providing a complete decomposition where:   - \\(\\sum_{i=1}^d \\phi_i = \\text{Var}(Y)\\)   - Each \\(\\phi_i \\geq 0\\)</p>"},{"location":"explanation/theory/#advantages","title":"Advantages","text":"<ol> <li>Complete Attribution: Accounts for all interactions</li> <li>Additivity: Effects sum to total variance</li> <li>Interpretability: Direct measure of importance</li> <li>Robustness: Works well with correlated inputs</li> </ol>"},{"location":"explanation/theory/#implementation-in-shapleyx","title":"Implementation in ShapleyX","text":"<p>The package uses: - Polynomial chaos expansions for efficient computation - Automatic Relevance Determination (ARD) for robust estimation - Legendre polynomials for orthogonal basis functions</p>"},{"location":"getting-started/deployment/","title":"Deployment Guide","text":""},{"location":"getting-started/deployment/#automated-deployment-recommended","title":"Automated Deployment (Recommended)","text":"<ol> <li>Push changes to GitHub</li> <li>The workflow in <code>.github/workflows/gh-pages.yml</code> will automatically:</li> <li>Build the documentation</li> <li>Deploy to the <code>gh-pages</code> branch</li> <li>Make it available at <code>https://[your-username].github.io/shapleyx/</code></li> </ol>"},{"location":"getting-started/deployment/#manual-deployment-fallback","title":"Manual Deployment (Fallback)","text":"<p>If automated deployment fails:</p> <ol> <li> <p>Install requirements: <pre><code>pip install mkdocs-material mkdocstrings[python]\n</code></pre></p> </li> <li> <p>Build and deploy: <pre><code>mkdocs gh-deploy --force\n</code></pre></p> </li> <li> <p>If permission errors persist: <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your@email.com\"\nmkdocs gh-deploy --force --remote-branch gh-pages\n</code></pre></p> </li> </ol>"},{"location":"getting-started/deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/deployment/#permission-errors","title":"Permission Errors","text":"<ul> <li>Ensure you have push access to the repository</li> <li>Verify your GitHub token has repo permissions</li> <li>Try creating a personal access token with repo scope</li> </ul>"},{"location":"getting-started/deployment/#build-errors","title":"Build Errors","text":"<ul> <li>Check for broken links in your docs</li> <li>Verify all Python modules referenced in API docs exist</li> <li>Run <code>mkdocs serve</code> locally to test before deploying</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.7 or higher</li> <li>pip package manager</li> </ul>"},{"location":"getting-started/installation/#installing-shapleyx","title":"Installing ShapleyX","text":"<p>You can install ShapleyX directly from PyPI:</p> <pre><code>pip install shapleyx\n</code></pre> <p>Or install from source:</p> <pre><code>git clone https://github.com/frederickbennett/shapleyx.git\ncd shapleyx\npip install .\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>ShapleyX requires the following Python packages:</p> <ul> <li>numpy</li> <li>scipy</li> <li>pandas</li> <li>scikit-learn</li> <li>matplotlib</li> <li>seaborn</li> </ul> <p>These will be installed automatically when installing ShapleyX.</p>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, you can verify it works by running:</p> <p>```python import shapleyx print(shapleyx.version)</p>"},{"location":"getting-started/quickstart/","title":"Quickstart Guide","text":"<p>This guide will walk you through a basic sensitivity analysis using ShapleyX.</p>"},{"location":"getting-started/quickstart/#loading-data","title":"Loading Data","text":"<p>First, prepare your data in CSV format with columns for input parameters and one column for output (named 'Y'):</p> <pre><code>import pandas as pd\n\n# Load your data\ndata = pd.read_csv('input_data.csv')\n</code></pre>"},{"location":"getting-started/quickstart/#running-analysis","title":"Running Analysis","text":"<pre><code>from shapleyx import rshdmr\n\n# Initialize analyzer\nanalyzer = rshdmr(\n    data_file='input_data.csv',  # or pass DataFrame directly\n    polys=[10, 5],              # polynomial orders\n    method='ard',               # regression method\n    verbose=True                # show progress\n)\n\n# Run complete analysis pipeline\nsobol_indices, shapley_effects, total_index = analyzer.run_all()\n</code></pre>"},{"location":"getting-started/quickstart/#viewing-results","title":"Viewing Results","text":"<pre><code># Sobol indices\nprint(\"Sobol Indices:\")\nprint(sobol_indices)\n\n# Shapley effects  \nprint(\"\\nShapley Effects:\")\nprint(shapley_effects)\n\n# Total indices\nprint(\"\\nTotal Indices:\")\nprint(total_index)\n</code></pre>"},{"location":"getting-started/quickstart/#plotting-results","title":"Plotting Results","text":"<pre><code># Plot predicted vs actual\nanalyzer.plot_hdmr()\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>See Tutorials for more detailed examples</li> <li>Explore How-to Guides for customization options</li> </ul>"},{"location":"how-to-guides/common-tasks/","title":"Common Tasks","text":""},{"location":"how-to-guides/common-tasks/#handling-large-datasets","title":"Handling Large Datasets","text":"<pre><code># Process data in chunks\nanalyzer = rshdmr(\n    data_file='large_data.csv',\n    chunksize=10000,  # Process 10,000 rows at a time\n    polys=[5, 3],     # Lower polynomial orders for large datasets\n    method='ard'\n)\n</code></pre>"},{"location":"how-to-guides/common-tasks/#customizing-polynomial-orders","title":"Customizing Polynomial Orders","text":"<pre><code># Set different polynomial orders for each input\nanalyzer = rshdmr(\n    data_file='data.csv',\n    polys=[10, 5, 8, 3, 6],  # Specific orders for each parameter\n    method='ard'\n)\n</code></pre>"},{"location":"how-to-guides/common-tasks/#saving-and-loading-results","title":"Saving and Loading Results","text":"<pre><code># Save results to file\nimport pickle\nwith open('sensitivity_results.pkl', 'wb') as f:\n    pickle.dump({\n        'sobol': sobol_indices,\n        'shapley': shapley_effects,\n        'total': total_index\n    }, f)\n\n# Load results\nwith open('sensitivity_results.pkl', 'rb') as f:\n    results = pickle.load(f)\n</code></pre>"},{"location":"how-to-guides/common-tasks/#comparing-different-methods","title":"Comparing Different Methods","text":"<pre><code># Compare ARD and OLS methods\nanalyzer_ard = rshdmr(data_file='data.csv', method='ard')\nanalyzer_ols = rshdmr(data_file='data.csv', method='ols')\n\nard_results = analyzer_ard.run_all()\nols_results = analyzer_ols.run_all()\n</code></pre>"},{"location":"how-to-guides/common-tasks/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"how-to-guides/common-tasks/#memory-errors","title":"Memory Errors","text":"<ul> <li>Reduce polynomial orders</li> <li>Use smaller chunksize</li> <li>Filter less important parameters</li> </ul>"},{"location":"how-to-guides/common-tasks/#convergence-issues","title":"Convergence Issues","text":"<ul> <li>Check data quality</li> <li>Try different polynomial orders</li> <li>Normalize input data</li> </ul>"},{"location":"reference/api/","title":"API Reference","text":""},{"location":"reference/api/#shapleyx.rshdmr","title":"<code>shapleyx.rshdmr</code>","text":"<p>Global Sensitivity Analysis using Sparse Random Sampling - High Dimensional  Model Representation (HDMR) with Group Method of Data Handling (GMDH) for  parameter selection and linear regression for parameter refinement.</p> <p>This module implements a global sensitivity analysis (GSA) framework using  Sparse Random Sampling (SRS) combined with High Dimensional Model Representation  (HDMR). The method employs the Group Method of Data Handling (GMDH) for parameter  selection and linear regression for parameter refinement. The framework is designed  to analyze the sensitivity of model outputs to input parameters, providing insights  into the relative importance of each parameter and their interactions.</p> <p>The module includes functionality for: - Reading and preprocessing input data. - Transforming data to a unit hypercube. - Building basis functions using Legendre polynomials. - Running regression analysis using various methods (ARD, OMP, etc.). - Evaluating Sobol indices, Shapley effects, and total indices. - Performing bootstrap resampling for confidence intervals. - Calculating PAWN indices for sensitivity analysis. - Predicting model outputs based on input parameters.</p> <p>Author: Frederick Bennett</p> <p>Classes:     rshdmr: Main class for performing global sensitivity analysis using RS-HDMR.</p> <p>Methods:     init: Initializes the RS-HDMR object with input data and parameters.     read_data: Reads and preprocesses input data.     transform_data: Transforms input data to a unit hypercube.     legendre_expand: Expands the input data using Legendre polynomials.     run_regression: Runs regression analysis using specified method.     stats: Computes and prints model performance statistics.     plot_hdmr: Plots predicted vs. experimental values.     eval_sobol_indices: Evaluates Sobol indices for sensitivity analysis.     get_shapley: Computes Shapley effects for sensitivity analysis.     get_total_index: Computes total sensitivity indices.     get_pruned_data: Returns pruned dataset based on non-zero coefficients.     get_pawn: Computes PAWN indices for sensitivity analysis.     run_all: Runs the entire RS-HDMR analysis pipeline.     predict: Predicts model outputs based on input parameters.     get_pawnx: Computes PAWN indices with additional statistical analysis.</p> <p>Example:     # Initialize RS-HDMR object     analyzer = rshdmr(data_file='input_data.csv', polys=[10, 5], method='ard')</p> <pre><code># Run the entire analysis pipeline\nsobol_indices, shapley_effects, total_index = analyzer.run_all()\n\n# Predict model outputs for new input data\npredictions = analyzer.predict(new_input_data)\n\n# Compute PAWN indices\npawn_results = analyzer.get_pawn(S=10)\n</code></pre> Source code in <code>shapleyx\\shapleyx.py</code> <pre><code>class rshdmr():\n\n    \"\"\"\n    *******************************************************************************\n    Global Sensitivity Analysis using Sparse Random Sampling - High Dimensional \n    Model Representation (HDMR) with Group Method of Data Handling (GMDH) for \n    parameter selection and linear regression for parameter refinement.\n    *******************************************************************************\n\n    This module implements a global sensitivity analysis (GSA) framework using \n    Sparse Random Sampling (SRS) combined with High Dimensional Model Representation \n    (HDMR). The method employs the Group Method of Data Handling (GMDH) for parameter \n    selection and linear regression for parameter refinement. The framework is designed \n    to analyze the sensitivity of model outputs to input parameters, providing insights \n    into the relative importance of each parameter and their interactions.\n\n    The module includes functionality for:\n    - Reading and preprocessing input data.\n    - Transforming data to a unit hypercube.\n    - Building basis functions using Legendre polynomials.\n    - Running regression analysis using various methods (ARD, OMP, etc.).\n    - Evaluating Sobol indices, Shapley effects, and total indices.\n    - Performing bootstrap resampling for confidence intervals.\n    - Calculating PAWN indices for sensitivity analysis.\n    - Predicting model outputs based on input parameters.\n\n    Author: Frederick Bennett\n\n    Classes:\n        rshdmr: Main class for performing global sensitivity analysis using RS-HDMR.\n\n    Methods:\n        __init__: Initializes the RS-HDMR object with input data and parameters.\n        read_data: Reads and preprocesses input data.\n        transform_data: Transforms input data to a unit hypercube.\n        legendre_expand: Expands the input data using Legendre polynomials.\n        run_regression: Runs regression analysis using specified method.\n        stats: Computes and prints model performance statistics.\n        plot_hdmr: Plots predicted vs. experimental values.\n        eval_sobol_indices: Evaluates Sobol indices for sensitivity analysis.\n        get_shapley: Computes Shapley effects for sensitivity analysis.\n        get_total_index: Computes total sensitivity indices.\n        get_pruned_data: Returns pruned dataset based on non-zero coefficients.\n        get_pawn: Computes PAWN indices for sensitivity analysis.\n        run_all: Runs the entire RS-HDMR analysis pipeline.\n        predict: Predicts model outputs based on input parameters.\n        get_pawnx: Computes PAWN indices with additional statistical analysis.\n\n    Example:\n        # Initialize RS-HDMR object\n        analyzer = rshdmr(data_file='input_data.csv', polys=[10, 5], method='ard')\n\n        # Run the entire analysis pipeline\n        sobol_indices, shapley_effects, total_index = analyzer.run_all()\n\n        # Predict model outputs for new input data\n        predictions = analyzer.predict(new_input_data)\n\n        # Compute PAWN indices\n        pawn_results = analyzer.get_pawn(S=10)\n    \"\"\"\n\n    # Rest of the code...\n\n\n    def __init__(self,data_file, polys = [10, 5],\n                 n_jobs = -1,\n                 test_size = 0.25,\n                 limit = 2.0,\n                 k_best = 1,\n                 p_average = 2,\n                 n_iter = 300,\n                 verbose = False,\n                 method = 'ard',\n                 starting_iter = 5,\n                 resampling = True,\n                 CI=95.0,\n                 number_of_resamples=1000,\n                 cv_tol = 0.05):\n\n        self.read_data(data_file)\n        self.n_jobs = n_jobs\n        self.test_size = test_size \n        self.limit = limit \n        self.k_best = k_best \n        self.p_average =  p_average\n        self.polys =  polys\n        self.max_1st = max(polys) \n        self.n_iter = n_iter\n        self.verbose = verbose\n        self.method = method\n        self.starting_iter = starting_iter\n        self.resampling = resampling\n        self.CI = CI\n        self.number_of_resamples = number_of_resamples\n        self.cv_tol = cv_tol \n\n    def read_data(self, data_file):\n        \"\"\"\n        Reads data from a file or DataFrame and initializes the X and Y attributes.\n        \"\"\"\n        if isinstance(data_file, pd.DataFrame):\n            print('Found a DataFrame')\n            df = data_file\n        elif isinstance(data_file, str):\n            df = pd.read_csv(data_file)\n        else:\n            raise ValueError(\"data_file must be either a pandas DataFrame or a file path (str).\")\n\n        self.Y = df['Y']\n        self.X = df.drop('Y', axis=1)\n\n        # Clean up the original DataFrame to save memory\n        del df\n\n\n    def transform_data(self):\n        \"\"\"\n        Transforms the data using into a unit hypercube.\n\n        This method applies a transformation to the data stored in `self.X`, \n        updates the transformed data, and retrieves the ranges and transformed \n        data matrix.\n\n        Attributes:\n            self.X (DataFrame or ndarray): The original data to be transformed.\n            self.ranges (list): The ranges of the transformed data.\n            self.X_T (DataFrame or ndarray): The transformed data matrix.\n\n        Returns:\n            None\n        \"\"\"\n        transformed_data = transformation.transformation(self.X)\n        transformed_data.do_transform()\n        self.ranges = transformed_data.get_ranges()\n        self.X_T = transformed_data.get_X_T()\n\n\n    def legendre_expand(self):\n        \"\"\"\n        Perform Legendre expansion on the input data.\n        This method uses the `legendre_expand` function from the `legendre` module to\n        expand the input data `X` and `X_T` up to the specified maximum order `max_1st`\n        using the provided polynomial basis `polys` and target values `Y`.\n        The expanded data is then stored in the instance variables:\n        - `primitive_variables`: The primitive variables obtained from the expansion.\n        - `poly_orders`: The polynomial orders used in the expansion.\n        - `X_T_L`: The expanded data.\n        Returns:\n            None\n        \"\"\"\n        expansion_data = legendre.legendre_expand(self.X_T, self.polys)\n        expansion_data.build_basis_set() \n\n        self.primitive_variables = expansion_data.get_primitive_variables() \n        self.poly_orders = expansion_data.get_poly_orders()\n        self.X_T_L = expansion_data.get_expanded()  \n\n\n    def run_regression(self):\n        \"\"\"\n        Runs the regression using the specified method and parameters.\n\n        This method initializes a regression instance with the provided\n        parameters and runs the regression to obtain the coefficients and\n        predicted values.\n\n        Attributes:\n            X_T_L (array-like): The transformed feature matrix.\n            Y (array-like): The target variable.\n            method (str): The regression method to use.\n            n_iter (int): The number of iterations for the regression algorithm.\n            verbose (bool): If True, enables verbose output.\n            cv_tol (float): The tolerance for cross-validation.\n            starting_iter (int): The starting iteration for the regression algorithm.\n\n        Returns:\n            None: The method updates the instance attributes `coef_` and `y_pred`\n            with the regression coefficients and predicted values, respectively.\n        \"\"\"\n        regression_instance = regression.regression(\n            X_T_L=self.X_T_L,\n            Y=self.Y,\n            method=self.method,\n            n_iter=self.n_iter,\n            verbose=self.verbose,\n            cv_tol=self.cv_tol,\n            starting_iter=self.starting_iter\n        )\n        self.coef_, self.y_pred = regression_instance.run_regression()\n\n    def stats(self):\n        \"\"\"\n        Calculate and store evaluation statistics for the model.\n\n        This method computes evaluation statistics using the actual target values (self.Y),\n        the predicted values (self.y_pred), and the model coefficients (self.coef_). The\n        results are stored in the instance variable `self.evs`.\n\n        Returns:\n            None\n        \"\"\"\n        self.evs = stats.stats(self.Y, self.y_pred, self.coef_)\n\n    def plot_hdmr(self):\n        \"\"\"\n        Plots the High-Dimensional Model Representation (HDMR) of the model's predictions.\n\n        This method uses the `plot_hdmr` function from the `stats` module to visualize the \n        HDMR of the actual values (`self.Y`) against the predicted values (`self.y_pred`).\n\n        Returns:\n            None\n        \"\"\"\n        stats.plot_hdmr(self.Y, self.y_pred)\n\n\n\n    def eval_all_indices(self):\n        \"\"\"\n        Evaluate all indices and store the results.\n        This method performs the following evaluations:\n        1. Evaluates indices using the provided data and model coefficients.\n        2. Retrieves Sobol indices and stores them in `self.results`.\n        3. Retrieves non-zero coefficients and stores them in `self.non_zero_coefficients`.\n        4. Evaluates Shapley values for the features and stores them in `self.shap`.\n        5. Evaluates the total index for the features and stores it in `self.total`.\n        Returns:\n            None\n        \"\"\"\n        eval_indicies = indicies.eval_indices(self.X_T_L, self.Y, self.coef_, self.evs) \n        self.results = eval_indicies.get_sobol_indicies()\n        self.non_zero_coefficients = eval_indicies.get_non_zero_coefficients() \n\n        self.shap = eval_indicies.eval_shapley(self.X.columns)\n        self.total = eval_indicies.eval_total_index(self.X.columns)\n\n\n    def get_pruned_data(self):\n        \"\"\"\n        Generates a pruned dataset containing only the features with non-zero coefficients.\n\n        This method creates a new DataFrame that includes only the columns from the original\n        dataset (`X_T_L`) that correspond to the labels with non-zero coefficients. Additionally,\n        it includes the target variable (`Y`).\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the pruned data with selected features and the target variable.\n        \"\"\"\n        pruned_data = pd.DataFrame()\n        for label in self.non_zero_coefficients['labels'] :\n            pruned_data[label] = self.X_T_L[label]\n        pruned_data['Y'] = self.Y\n        return pruned_data\n\n    def run_all(self):\n        \"\"\"\n    Execute a complete sequence of steps for RS-HDMR (Random Sampling High-Dimensional Model Representation) analysis.\n\n    This method performs the following steps in sequence:\n    1. Transforms the input data to a unit hypercube.\n    2. Builds basis functions using Legendre polynomials.\n    3. Runs regression analysis to fit the model.\n    4. Calculates and displays RS-HDMR model performance statistics.\n    5. Evaluates Sobol indices to quantify the contribution of each input variable to the output variance.\n    6. Calculates Shapley effects to measure the importance of each input variable.\n    7. Computes the total index to assess the overall impact of input variables.\n    8. If resampling is enabled, performs bootstrap resampling to estimate confidence intervals for Sobol indices and Shapley effects.\n    9. Prints a completion message with a randomly selected quote.\n\n    Returns:\n        tuple: A tuple containing three elements:\n            - sobol_indices (pd.DataFrame): A DataFrame containing Sobol indices for each input variable.\n            - shapley_effects (pd.DataFrame): A DataFrame containing Shapley effects for each input variable.\n            - total_index (pd.DataFrame): A DataFrame containing the total index for each input variable.\n\n    Notes:\n        - The method assumes that the necessary data and configurations are already set in the class instance.\n        - If resampling is enabled (`self.resampling` is True), confidence intervals (CIs) are calculated for Sobol indices and Shapley effects.\n        - The method uses helper functions like `transform_data`, `legendre_expand`, `run_regression`, `stats`, `plot_hdmr`, `eval_sobol_indices`, `get_shapley`, and `get_total_index` to perform specific tasks.\n        - The completion message includes a randomly selected quote for a touch of inspiration.\n\n    Example:\n        &gt;&gt;&gt; sobol_indices, shapley_effects, total_index = instance.run_all()\n        &gt;&gt;&gt; print(sobol_indices)\n        &gt;&gt;&gt; print(shapley_effects)\n        &gt;&gt;&gt; print(total_index)\n    \"\"\"\n        # Define a helper function to print headings\n        def print_step(step_name):\n            print_heading(step_name)\n\n        # Step 1: Transform data to unit hypercube\n        print_step('Transforming data to unit hypercube')\n        self.transform_data()\n\n        # Step 2: Build basis functions\n        print_step('Building basis functions')\n        self.legendre_expand()\n\n        # Step 3: Run regression analysis\n        print_step('Running regression analysis')\n        self.run_regression()\n\n        # Step 4: Calculate RS-HDMR model performance statistics\n        print_step('RS-HDMR model performance statistics')\n        self.stats() \n        print()\n        self.plot_hdmr()\n\n        # Step 5: Evaluate Sobol indices\n        self.eval_all_indices()\n        sobol_indices = self.results.drop(columns=['labels', 'coeff'])\n\n        # Step 6: Calculate Shapley effects\n        shapley_effects = self.shap\n\n        # Step 7: Calculate total index \n        total_index = self.total\n\n        # Step 8: Perform resampling if enabled\n        if self.resampling:\n            print_step(f'Running bootstrap resampling {self.number_of_resamples} samples for {self.CI}% CI') \n            do_resampling = resampling.resampling(self.get_pruned_data(), self.number_of_resamples, self.X.columns)\n            do_resampling.do_resampling() \n            sobol_indices = do_resampling.get_sobol_quantiles(sobol_indices, self.CI)\n            # Calculate quantiles for Shapley effects\n            shapley_effects = do_resampling.get_shap_quantiles(shapley_effects, self.CI) \n            print_step('Completed bootstrap resampling')\n\n        # Step 9: Print completion message with a quote\n        quote = quotes.get_quote() \n        message = (\n            \"                  Completed all analysis\\n\"\n            \"                 ------------------------\\n\\n\"\n            f\"{textwrap.fill(quote, 58)}\"\n        )\n        print_step(message)\n\n        return sobol_indices, shapley_effects, total_index\n\n    def predict(self, X):\n        \"\"\"\n        Predict the output for the given input data using the surrogate model.\n\n        Parameters:\n        X (array-like): Input data for which predictions are to be made.\n\n        Returns:\n        array-like: Predicted output for the input data.\n\n        Notes:\n        If the predictive-surrogate model does not exist, it will be created and trained using\n        the non-zero coefficients and ranges provided during initialization.\n        \"\"\"\n        if not hasattr(self, 'surrogate_model'):\n            self.surrogate_model = predictor.surrogate(self.non_zero_coefficients, self.ranges)\n            self.surrogate_model.fit(self.X, self.Y)\n        return self.surrogate_model.predict(X) \n\n    def get_deltax(self, num_unconditioned: int, delta_samples: int) -&gt; pd.DataFrame:      \n        \"\"\"\n        Calculate delta indices for the given number of unconditioned variables and delta samples.\n\n        This method initializes a DeltaX instance using the provided data and parameters,\n        then computes the delta indices based on the specified number of unconditioned variables\n        and delta samples.\n\n        Args:\n            num_unconditioned (int): The number of unconditioned variables.\n            delta_samples (int): The number of delta samples to generate.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the computed delta indices.\n        \"\"\"\n        self.delta_instance = pawn.DeltaX(self.X, self.Y, self.ranges, self.non_zero_coefficients)\n        delta_indices = self.delta_instance.get_deltax(num_unconditioned, delta_samples)\n        return delta_indices\n\n    def get_hx(self, num_unconditioned: int, delta_samples: int) -&gt; pd.DataFrame:      \n        \"\"\"\n        Calculate delta indices for the given number of unconditioned variables and delta samples.\n\n        This method initializes a DeltaX instance using the provided data and parameters,\n        then computes the delta indices based on the specified number of unconditioned variables\n        and delta samples.\n\n        Args:\n            num_unconditioned (int): The number of unconditioned variables.\n            delta_samples (int): The number of delta samples to generate.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the computed delta indices.\n        \"\"\"\n        self.delta_instance = pawn.hX(self.X, self.Y, self.ranges, self.non_zero_coefficients)\n        delta_indices = self.delta_instance.get_hx(num_unconditioned, delta_samples)\n        return delta_indices\n\n\n    def get_pawnx(self, num_unconditioned: int, num_conditioned: int, num_ks_samples: int, alpha: float = 0.05) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate the PAWN sensitivity indices for the RS-HDMR surrogate model.\n\n        Parameters:\n        -----------\n        num_unconditioned : int\n            The number of unconditioned samples to be used in the PAWN analysis.\n        num_conditioned : int\n            The number of conditioned samples to be used in the PAWN analysis.\n        num_ks_samples : int\n            The number of samples to be used in the Kolmogorov-Smirnov test.\n        alpha : float, optional\n            The significance level for the Kolmogorov-Smirnov test (default is 0.05).\n\n        Returns:\n        --------\n        pd.DataFrame\n            A DataFrame containing the PAWN sensitivity indices.\n        \"\"\"\n        pawn_instance = pawn.pawnx(self.X, self.Y, self.ranges, self.non_zero_coefficients)\n        pawn_indices = pawn_instance.get_pawnx(num_unconditioned, num_conditioned, num_ks_samples, alpha) \n        return pawn_indices \n\n    def get_pawn(self, S=10) :\n        \"\"\"\n        Estimate the PAWN sensitivity indices for the features in the dataset.\n        Parameters:\n        -----------\n        S : int, optional\n            The number of slides to use for the estimation. Default is 10.\n        Returns:\n        --------\n        pawn_results : dict\n            A dictionary containing the PAWN sensitivity indices for each feature.\n        \"\"\"\n\n        num_features = self.X.shape[1] \n        pawn_results = pawn.estimate_pawn(self.X.columns, num_features, self.X.values, self.Y, S=S)\n        return pawn_results\n</code></pre>"},{"location":"reference/api/#shapleyx.rshdmr.eval_all_indices","title":"<code>eval_all_indices()</code>","text":"<p>Evaluate all indices and store the results. This method performs the following evaluations: 1. Evaluates indices using the provided data and model coefficients. 2. Retrieves Sobol indices and stores them in <code>self.results</code>. 3. Retrieves non-zero coefficients and stores them in <code>self.non_zero_coefficients</code>. 4. Evaluates Shapley values for the features and stores them in <code>self.shap</code>. 5. Evaluates the total index for the features and stores it in <code>self.total</code>. Returns:     None</p> Source code in <code>shapleyx\\shapleyx.py</code> <pre><code>def eval_all_indices(self):\n    \"\"\"\n    Evaluate all indices and store the results.\n    This method performs the following evaluations:\n    1. Evaluates indices using the provided data and model coefficients.\n    2. Retrieves Sobol indices and stores them in `self.results`.\n    3. Retrieves non-zero coefficients and stores them in `self.non_zero_coefficients`.\n    4. Evaluates Shapley values for the features and stores them in `self.shap`.\n    5. Evaluates the total index for the features and stores it in `self.total`.\n    Returns:\n        None\n    \"\"\"\n    eval_indicies = indicies.eval_indices(self.X_T_L, self.Y, self.coef_, self.evs) \n    self.results = eval_indicies.get_sobol_indicies()\n    self.non_zero_coefficients = eval_indicies.get_non_zero_coefficients() \n\n    self.shap = eval_indicies.eval_shapley(self.X.columns)\n    self.total = eval_indicies.eval_total_index(self.X.columns)\n</code></pre>"},{"location":"reference/api/#shapleyx.rshdmr.get_deltax","title":"<code>get_deltax(num_unconditioned, delta_samples)</code>","text":"<p>Calculate delta indices for the given number of unconditioned variables and delta samples.</p> <p>This method initializes a DeltaX instance using the provided data and parameters, then computes the delta indices based on the specified number of unconditioned variables and delta samples.</p> <p>Args:     num_unconditioned (int): The number of unconditioned variables.     delta_samples (int): The number of delta samples to generate.</p> <p>Returns:     pd.DataFrame: A DataFrame containing the computed delta indices.</p> Source code in <code>shapleyx\\shapleyx.py</code> <pre><code>def get_deltax(self, num_unconditioned: int, delta_samples: int) -&gt; pd.DataFrame:      \n    \"\"\"\n    Calculate delta indices for the given number of unconditioned variables and delta samples.\n\n    This method initializes a DeltaX instance using the provided data and parameters,\n    then computes the delta indices based on the specified number of unconditioned variables\n    and delta samples.\n\n    Args:\n        num_unconditioned (int): The number of unconditioned variables.\n        delta_samples (int): The number of delta samples to generate.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the computed delta indices.\n    \"\"\"\n    self.delta_instance = pawn.DeltaX(self.X, self.Y, self.ranges, self.non_zero_coefficients)\n    delta_indices = self.delta_instance.get_deltax(num_unconditioned, delta_samples)\n    return delta_indices\n</code></pre>"},{"location":"reference/api/#shapleyx.rshdmr.get_hx","title":"<code>get_hx(num_unconditioned, delta_samples)</code>","text":"<p>Calculate delta indices for the given number of unconditioned variables and delta samples.</p> <p>This method initializes a DeltaX instance using the provided data and parameters, then computes the delta indices based on the specified number of unconditioned variables and delta samples.</p> <p>Args:     num_unconditioned (int): The number of unconditioned variables.     delta_samples (int): The number of delta samples to generate.</p> <p>Returns:     pd.DataFrame: A DataFrame containing the computed delta indices.</p> Source code in <code>shapleyx\\shapleyx.py</code> <pre><code>def get_hx(self, num_unconditioned: int, delta_samples: int) -&gt; pd.DataFrame:      \n    \"\"\"\n    Calculate delta indices for the given number of unconditioned variables and delta samples.\n\n    This method initializes a DeltaX instance using the provided data and parameters,\n    then computes the delta indices based on the specified number of unconditioned variables\n    and delta samples.\n\n    Args:\n        num_unconditioned (int): The number of unconditioned variables.\n        delta_samples (int): The number of delta samples to generate.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the computed delta indices.\n    \"\"\"\n    self.delta_instance = pawn.hX(self.X, self.Y, self.ranges, self.non_zero_coefficients)\n    delta_indices = self.delta_instance.get_hx(num_unconditioned, delta_samples)\n    return delta_indices\n</code></pre>"},{"location":"reference/api/#shapleyx.rshdmr.get_pawn","title":"<code>get_pawn(S=10)</code>","text":"<p>Estimate the PAWN sensitivity indices for the features in the dataset.</p> Parameters: <p>S : int, optional     The number of slides to use for the estimation. Default is 10.</p> Returns: <p>pawn_results : dict     A dictionary containing the PAWN sensitivity indices for each feature.</p> Source code in <code>shapleyx\\shapleyx.py</code> <pre><code>def get_pawn(self, S=10) :\n    \"\"\"\n    Estimate the PAWN sensitivity indices for the features in the dataset.\n    Parameters:\n    -----------\n    S : int, optional\n        The number of slides to use for the estimation. Default is 10.\n    Returns:\n    --------\n    pawn_results : dict\n        A dictionary containing the PAWN sensitivity indices for each feature.\n    \"\"\"\n\n    num_features = self.X.shape[1] \n    pawn_results = pawn.estimate_pawn(self.X.columns, num_features, self.X.values, self.Y, S=S)\n    return pawn_results\n</code></pre>"},{"location":"reference/api/#shapleyx.rshdmr.get_pawnx","title":"<code>get_pawnx(num_unconditioned, num_conditioned, num_ks_samples, alpha=0.05)</code>","text":"<p>Calculate the PAWN sensitivity indices for the RS-HDMR surrogate model.</p> Parameters: <p>num_unconditioned : int     The number of unconditioned samples to be used in the PAWN analysis. num_conditioned : int     The number of conditioned samples to be used in the PAWN analysis. num_ks_samples : int     The number of samples to be used in the Kolmogorov-Smirnov test. alpha : float, optional     The significance level for the Kolmogorov-Smirnov test (default is 0.05).</p> Returns: <p>pd.DataFrame     A DataFrame containing the PAWN sensitivity indices.</p> Source code in <code>shapleyx\\shapleyx.py</code> <pre><code>def get_pawnx(self, num_unconditioned: int, num_conditioned: int, num_ks_samples: int, alpha: float = 0.05) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the PAWN sensitivity indices for the RS-HDMR surrogate model.\n\n    Parameters:\n    -----------\n    num_unconditioned : int\n        The number of unconditioned samples to be used in the PAWN analysis.\n    num_conditioned : int\n        The number of conditioned samples to be used in the PAWN analysis.\n    num_ks_samples : int\n        The number of samples to be used in the Kolmogorov-Smirnov test.\n    alpha : float, optional\n        The significance level for the Kolmogorov-Smirnov test (default is 0.05).\n\n    Returns:\n    --------\n    pd.DataFrame\n        A DataFrame containing the PAWN sensitivity indices.\n    \"\"\"\n    pawn_instance = pawn.pawnx(self.X, self.Y, self.ranges, self.non_zero_coefficients)\n    pawn_indices = pawn_instance.get_pawnx(num_unconditioned, num_conditioned, num_ks_samples, alpha) \n    return pawn_indices \n</code></pre>"},{"location":"reference/api/#shapleyx.rshdmr.get_pruned_data","title":"<code>get_pruned_data()</code>","text":"<p>Generates a pruned dataset containing only the features with non-zero coefficients.</p> <p>This method creates a new DataFrame that includes only the columns from the original dataset (<code>X_T_L</code>) that correspond to the labels with non-zero coefficients. Additionally, it includes the target variable (<code>Y</code>).</p> <p>Returns:     pd.DataFrame: A DataFrame containing the pruned data with selected features and the target variable.</p> Source code in <code>shapleyx\\shapleyx.py</code> <pre><code>def get_pruned_data(self):\n    \"\"\"\n    Generates a pruned dataset containing only the features with non-zero coefficients.\n\n    This method creates a new DataFrame that includes only the columns from the original\n    dataset (`X_T_L`) that correspond to the labels with non-zero coefficients. Additionally,\n    it includes the target variable (`Y`).\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the pruned data with selected features and the target variable.\n    \"\"\"\n    pruned_data = pd.DataFrame()\n    for label in self.non_zero_coefficients['labels'] :\n        pruned_data[label] = self.X_T_L[label]\n    pruned_data['Y'] = self.Y\n    return pruned_data\n</code></pre>"},{"location":"reference/api/#shapleyx.rshdmr.legendre_expand","title":"<code>legendre_expand()</code>","text":"<p>Perform Legendre expansion on the input data. This method uses the <code>legendre_expand</code> function from the <code>legendre</code> module to expand the input data <code>X</code> and <code>X_T</code> up to the specified maximum order <code>max_1st</code> using the provided polynomial basis <code>polys</code> and target values <code>Y</code>. The expanded data is then stored in the instance variables: - <code>primitive_variables</code>: The primitive variables obtained from the expansion. - <code>poly_orders</code>: The polynomial orders used in the expansion. - <code>X_T_L</code>: The expanded data. Returns:     None</p> Source code in <code>shapleyx\\shapleyx.py</code> <pre><code>def legendre_expand(self):\n    \"\"\"\n    Perform Legendre expansion on the input data.\n    This method uses the `legendre_expand` function from the `legendre` module to\n    expand the input data `X` and `X_T` up to the specified maximum order `max_1st`\n    using the provided polynomial basis `polys` and target values `Y`.\n    The expanded data is then stored in the instance variables:\n    - `primitive_variables`: The primitive variables obtained from the expansion.\n    - `poly_orders`: The polynomial orders used in the expansion.\n    - `X_T_L`: The expanded data.\n    Returns:\n        None\n    \"\"\"\n    expansion_data = legendre.legendre_expand(self.X_T, self.polys)\n    expansion_data.build_basis_set() \n\n    self.primitive_variables = expansion_data.get_primitive_variables() \n    self.poly_orders = expansion_data.get_poly_orders()\n    self.X_T_L = expansion_data.get_expanded()  \n</code></pre>"},{"location":"reference/api/#shapleyx.rshdmr.plot_hdmr","title":"<code>plot_hdmr()</code>","text":"<p>Plots the High-Dimensional Model Representation (HDMR) of the model's predictions.</p> <p>This method uses the <code>plot_hdmr</code> function from the <code>stats</code> module to visualize the  HDMR of the actual values (<code>self.Y</code>) against the predicted values (<code>self.y_pred</code>).</p> <p>Returns:     None</p> Source code in <code>shapleyx\\shapleyx.py</code> <pre><code>def plot_hdmr(self):\n    \"\"\"\n    Plots the High-Dimensional Model Representation (HDMR) of the model's predictions.\n\n    This method uses the `plot_hdmr` function from the `stats` module to visualize the \n    HDMR of the actual values (`self.Y`) against the predicted values (`self.y_pred`).\n\n    Returns:\n        None\n    \"\"\"\n    stats.plot_hdmr(self.Y, self.y_pred)\n</code></pre>"},{"location":"reference/api/#shapleyx.rshdmr.predict","title":"<code>predict(X)</code>","text":"<p>Predict the output for the given input data using the surrogate model.</p> <p>Parameters: X (array-like): Input data for which predictions are to be made.</p> <p>Returns: array-like: Predicted output for the input data.</p> <p>Notes: If the predictive-surrogate model does not exist, it will be created and trained using the non-zero coefficients and ranges provided during initialization.</p> Source code in <code>shapleyx\\shapleyx.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Predict the output for the given input data using the surrogate model.\n\n    Parameters:\n    X (array-like): Input data for which predictions are to be made.\n\n    Returns:\n    array-like: Predicted output for the input data.\n\n    Notes:\n    If the predictive-surrogate model does not exist, it will be created and trained using\n    the non-zero coefficients and ranges provided during initialization.\n    \"\"\"\n    if not hasattr(self, 'surrogate_model'):\n        self.surrogate_model = predictor.surrogate(self.non_zero_coefficients, self.ranges)\n        self.surrogate_model.fit(self.X, self.Y)\n    return self.surrogate_model.predict(X) \n</code></pre>"},{"location":"reference/api/#shapleyx.rshdmr.read_data","title":"<code>read_data(data_file)</code>","text":"<p>Reads data from a file or DataFrame and initializes the X and Y attributes.</p> Source code in <code>shapleyx\\shapleyx.py</code> <pre><code>def read_data(self, data_file):\n    \"\"\"\n    Reads data from a file or DataFrame and initializes the X and Y attributes.\n    \"\"\"\n    if isinstance(data_file, pd.DataFrame):\n        print('Found a DataFrame')\n        df = data_file\n    elif isinstance(data_file, str):\n        df = pd.read_csv(data_file)\n    else:\n        raise ValueError(\"data_file must be either a pandas DataFrame or a file path (str).\")\n\n    self.Y = df['Y']\n    self.X = df.drop('Y', axis=1)\n\n    # Clean up the original DataFrame to save memory\n    del df\n</code></pre>"},{"location":"reference/api/#shapleyx.rshdmr.run_all","title":"<code>run_all()</code>","text":"<p>Execute a complete sequence of steps for RS-HDMR (Random Sampling High-Dimensional Model Representation) analysis.</p> <p>This method performs the following steps in sequence: 1. Transforms the input data to a unit hypercube. 2. Builds basis functions using Legendre polynomials. 3. Runs regression analysis to fit the model. 4. Calculates and displays RS-HDMR model performance statistics. 5. Evaluates Sobol indices to quantify the contribution of each input variable to the output variance. 6. Calculates Shapley effects to measure the importance of each input variable. 7. Computes the total index to assess the overall impact of input variables. 8. If resampling is enabled, performs bootstrap resampling to estimate confidence intervals for Sobol indices and Shapley effects. 9. Prints a completion message with a randomly selected quote.</p> <p>Returns:     tuple: A tuple containing three elements:         - sobol_indices (pd.DataFrame): A DataFrame containing Sobol indices for each input variable.         - shapley_effects (pd.DataFrame): A DataFrame containing Shapley effects for each input variable.         - total_index (pd.DataFrame): A DataFrame containing the total index for each input variable.</p> <p>Notes:     - The method assumes that the necessary data and configurations are already set in the class instance.     - If resampling is enabled (<code>self.resampling</code> is True), confidence intervals (CIs) are calculated for Sobol indices and Shapley effects.     - The method uses helper functions like <code>transform_data</code>, <code>legendre_expand</code>, <code>run_regression</code>, <code>stats</code>, <code>plot_hdmr</code>, <code>eval_sobol_indices</code>, <code>get_shapley</code>, and <code>get_total_index</code> to perform specific tasks.     - The completion message includes a randomly selected quote for a touch of inspiration.</p> <p>Example:     &gt;&gt;&gt; sobol_indices, shapley_effects, total_index = instance.run_all()     &gt;&gt;&gt; print(sobol_indices)     &gt;&gt;&gt; print(shapley_effects)     &gt;&gt;&gt; print(total_index)</p> Source code in <code>shapleyx\\shapleyx.py</code> <pre><code>def run_all(self):\n    \"\"\"\nExecute a complete sequence of steps for RS-HDMR (Random Sampling High-Dimensional Model Representation) analysis.\n\nThis method performs the following steps in sequence:\n1. Transforms the input data to a unit hypercube.\n2. Builds basis functions using Legendre polynomials.\n3. Runs regression analysis to fit the model.\n4. Calculates and displays RS-HDMR model performance statistics.\n5. Evaluates Sobol indices to quantify the contribution of each input variable to the output variance.\n6. Calculates Shapley effects to measure the importance of each input variable.\n7. Computes the total index to assess the overall impact of input variables.\n8. If resampling is enabled, performs bootstrap resampling to estimate confidence intervals for Sobol indices and Shapley effects.\n9. Prints a completion message with a randomly selected quote.\n\nReturns:\n    tuple: A tuple containing three elements:\n        - sobol_indices (pd.DataFrame): A DataFrame containing Sobol indices for each input variable.\n        - shapley_effects (pd.DataFrame): A DataFrame containing Shapley effects for each input variable.\n        - total_index (pd.DataFrame): A DataFrame containing the total index for each input variable.\n\nNotes:\n    - The method assumes that the necessary data and configurations are already set in the class instance.\n    - If resampling is enabled (`self.resampling` is True), confidence intervals (CIs) are calculated for Sobol indices and Shapley effects.\n    - The method uses helper functions like `transform_data`, `legendre_expand`, `run_regression`, `stats`, `plot_hdmr`, `eval_sobol_indices`, `get_shapley`, and `get_total_index` to perform specific tasks.\n    - The completion message includes a randomly selected quote for a touch of inspiration.\n\nExample:\n    &gt;&gt;&gt; sobol_indices, shapley_effects, total_index = instance.run_all()\n    &gt;&gt;&gt; print(sobol_indices)\n    &gt;&gt;&gt; print(shapley_effects)\n    &gt;&gt;&gt; print(total_index)\n\"\"\"\n    # Define a helper function to print headings\n    def print_step(step_name):\n        print_heading(step_name)\n\n    # Step 1: Transform data to unit hypercube\n    print_step('Transforming data to unit hypercube')\n    self.transform_data()\n\n    # Step 2: Build basis functions\n    print_step('Building basis functions')\n    self.legendre_expand()\n\n    # Step 3: Run regression analysis\n    print_step('Running regression analysis')\n    self.run_regression()\n\n    # Step 4: Calculate RS-HDMR model performance statistics\n    print_step('RS-HDMR model performance statistics')\n    self.stats() \n    print()\n    self.plot_hdmr()\n\n    # Step 5: Evaluate Sobol indices\n    self.eval_all_indices()\n    sobol_indices = self.results.drop(columns=['labels', 'coeff'])\n\n    # Step 6: Calculate Shapley effects\n    shapley_effects = self.shap\n\n    # Step 7: Calculate total index \n    total_index = self.total\n\n    # Step 8: Perform resampling if enabled\n    if self.resampling:\n        print_step(f'Running bootstrap resampling {self.number_of_resamples} samples for {self.CI}% CI') \n        do_resampling = resampling.resampling(self.get_pruned_data(), self.number_of_resamples, self.X.columns)\n        do_resampling.do_resampling() \n        sobol_indices = do_resampling.get_sobol_quantiles(sobol_indices, self.CI)\n        # Calculate quantiles for Shapley effects\n        shapley_effects = do_resampling.get_shap_quantiles(shapley_effects, self.CI) \n        print_step('Completed bootstrap resampling')\n\n    # Step 9: Print completion message with a quote\n    quote = quotes.get_quote() \n    message = (\n        \"                  Completed all analysis\\n\"\n        \"                 ------------------------\\n\\n\"\n        f\"{textwrap.fill(quote, 58)}\"\n    )\n    print_step(message)\n\n    return sobol_indices, shapley_effects, total_index\n</code></pre>"},{"location":"reference/api/#shapleyx.rshdmr.run_regression","title":"<code>run_regression()</code>","text":"<p>Runs the regression using the specified method and parameters.</p> <p>This method initializes a regression instance with the provided parameters and runs the regression to obtain the coefficients and predicted values.</p> <p>Attributes:     X_T_L (array-like): The transformed feature matrix.     Y (array-like): The target variable.     method (str): The regression method to use.     n_iter (int): The number of iterations for the regression algorithm.     verbose (bool): If True, enables verbose output.     cv_tol (float): The tolerance for cross-validation.     starting_iter (int): The starting iteration for the regression algorithm.</p> <p>Returns:     None: The method updates the instance attributes <code>coef_</code> and <code>y_pred</code>     with the regression coefficients and predicted values, respectively.</p> Source code in <code>shapleyx\\shapleyx.py</code> <pre><code>def run_regression(self):\n    \"\"\"\n    Runs the regression using the specified method and parameters.\n\n    This method initializes a regression instance with the provided\n    parameters and runs the regression to obtain the coefficients and\n    predicted values.\n\n    Attributes:\n        X_T_L (array-like): The transformed feature matrix.\n        Y (array-like): The target variable.\n        method (str): The regression method to use.\n        n_iter (int): The number of iterations for the regression algorithm.\n        verbose (bool): If True, enables verbose output.\n        cv_tol (float): The tolerance for cross-validation.\n        starting_iter (int): The starting iteration for the regression algorithm.\n\n    Returns:\n        None: The method updates the instance attributes `coef_` and `y_pred`\n        with the regression coefficients and predicted values, respectively.\n    \"\"\"\n    regression_instance = regression.regression(\n        X_T_L=self.X_T_L,\n        Y=self.Y,\n        method=self.method,\n        n_iter=self.n_iter,\n        verbose=self.verbose,\n        cv_tol=self.cv_tol,\n        starting_iter=self.starting_iter\n    )\n    self.coef_, self.y_pred = regression_instance.run_regression()\n</code></pre>"},{"location":"reference/api/#shapleyx.rshdmr.stats","title":"<code>stats()</code>","text":"<p>Calculate and store evaluation statistics for the model.</p> <p>This method computes evaluation statistics using the actual target values (self.Y), the predicted values (self.y_pred), and the model coefficients (self.coef_). The results are stored in the instance variable <code>self.evs</code>.</p> <p>Returns:     None</p> Source code in <code>shapleyx\\shapleyx.py</code> <pre><code>def stats(self):\n    \"\"\"\n    Calculate and store evaluation statistics for the model.\n\n    This method computes evaluation statistics using the actual target values (self.Y),\n    the predicted values (self.y_pred), and the model coefficients (self.coef_). The\n    results are stored in the instance variable `self.evs`.\n\n    Returns:\n        None\n    \"\"\"\n    self.evs = stats.stats(self.Y, self.y_pred, self.coef_)\n</code></pre>"},{"location":"reference/api/#shapleyx.rshdmr.transform_data","title":"<code>transform_data()</code>","text":"<p>Transforms the data using into a unit hypercube.</p> <p>This method applies a transformation to the data stored in <code>self.X</code>,  updates the transformed data, and retrieves the ranges and transformed  data matrix.</p> <p>Attributes:     self.X (DataFrame or ndarray): The original data to be transformed.     self.ranges (list): The ranges of the transformed data.     self.X_T (DataFrame or ndarray): The transformed data matrix.</p> <p>Returns:     None</p> Source code in <code>shapleyx\\shapleyx.py</code> <pre><code>def transform_data(self):\n    \"\"\"\n    Transforms the data using into a unit hypercube.\n\n    This method applies a transformation to the data stored in `self.X`, \n    updates the transformed data, and retrieves the ranges and transformed \n    data matrix.\n\n    Attributes:\n        self.X (DataFrame or ndarray): The original data to be transformed.\n        self.ranges (list): The ranges of the transformed data.\n        self.X_T (DataFrame or ndarray): The transformed data matrix.\n\n    Returns:\n        None\n    \"\"\"\n    transformed_data = transformation.transformation(self.X)\n    transformed_data.do_transform()\n    self.ranges = transformed_data.get_ranges()\n    self.X_T = transformed_data.get_X_T()\n</code></pre>"},{"location":"reference/api/#shapleyx.ARD","title":"<code>shapleyx.ARD</code>","text":""},{"location":"reference/api/#shapleyx.ARD.RegressionARD","title":"<code>RegressionARD</code>","text":"<p>               Bases: <code>RegressorMixin</code>, <code>LinearModel</code></p> <p>Regression with Automatic Relevance Determination (ARD) using Sparse Bayesian Learning.</p> <p>This class implements a fast version of ARD regression, which is a Bayesian approach to regression that automatically determines the relevance of each feature. It is based on the Sparse Bayesian Learning (SBL) algorithm, which promotes sparsity in the model by estimating the precision of the coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>n_iter</code> <code>(int, optional(default=300))</code> <p>Maximum number of iterations for the optimization algorithm.</p> <code>300</code> <code>tol</code> <code>(float, optional(default=0.001))</code> <p>Convergence threshold. If the absolute change in the precision parameter for the weights is below this threshold, the algorithm terminates.</p> <code>0.001</code> <code>fit_intercept</code> <code>(bool, optional(default=True))</code> <p>Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (e.g., data is expected to be already centered).</p> <code>True</code> <code>copy_X</code> <code>(bool, optional(default=True))</code> <p>If True, X will be copied; else, it may be overwritten.</p> <code>True</code> <code>verbose</code> <code>(bool, optional(default=False))</code> <p>If True, the algorithm will print progress messages during fitting.</p> <code>False</code> <code>cv_tol</code> <code>(float, optional(default=0.1))</code> <p>Tolerance for cross-validation. If the percentage change in cross-validation score is below this threshold, the algorithm terminates.</p> <code>0.1</code> <code>cv</code> <code>(bool, optional(default=False))</code> <p>If True, cross-validation will be used to determine the optimal number of features.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>coef_</code> <code>(array, shape(n_features))</code> <p>Coefficients of the regression model (mean of the posterior distribution).</p> <code>alpha_</code> <code>float</code> <p>Estimated precision of the noise.</p> <code>active_</code> <code>array, dtype=bool, shape (n_features,)</code> <p>Boolean array indicating which features are active (non-zero coefficients).</p> <code>lambda_</code> <code>(array, shape(n_features))</code> <p>Estimated precisions of the coefficients.</p> <code>sigma_</code> <code>(array, shape(n_features, n_features))</code> <p>Estimated covariance matrix of the weights, computed only for non-zero coefficients.</p> <code>scores_</code> <code>list</code> <p>List of cross-validation scores if <code>cv</code> is True.</p> <p>Methods:</p> Name Description <code>fit</code> <p>Fit the ARD regression model to the data.</p> <code>predict_dist</code> <p>Compute the predictive distribution for the test set.</p> <code>_center_data</code> <p>Center the data by subtracting the mean.</p> <code>_posterior_dist</code> <p>Calculate the mean and covariance matrix of the posterior distribution of coefficients.</p> <code>_sparsity_quality</code> <p>Calculate sparsity and quality parameters for each feature.</p> References <p>[1] Tipping, M. E., &amp; Faul, A. C. (2003). Fast marginal likelihood maximisation for     sparse Bayesian models. In Proceedings of the Ninth International Workshop on     Artificial Intelligence and Statistics (pp. 276-283). [2] Tipping, M. E., &amp; Faul, A. C. (2001). Analysis of sparse Bayesian learning. In     Advances in Neural Information Processing Systems (pp. 383-389).</p> Source code in <code>shapleyx\\ARD.py</code> <pre><code>class RegressionARD(RegressorMixin, LinearModel):\n    '''\n    Regression with Automatic Relevance Determination (ARD) using Sparse Bayesian Learning.\n\n    This class implements a fast version of ARD regression, which is a Bayesian approach\n    to regression that automatically determines the relevance of each feature. It is based\n    on the Sparse Bayesian Learning (SBL) algorithm, which promotes sparsity in the model\n    by estimating the precision of the coefficients.\n\n    Parameters\n    ----------\n    n_iter : int, optional (default=300)\n        Maximum number of iterations for the optimization algorithm.\n\n    tol : float, optional (default=1e-3)\n        Convergence threshold. If the absolute change in the precision parameter for the\n        weights is below this threshold, the algorithm terminates.\n\n    fit_intercept : bool, optional (default=True)\n        Whether to calculate the intercept for this model. If set to False, no intercept\n        will be used in calculations (e.g., data is expected to be already centered).\n\n    copy_X : bool, optional (default=True)\n        If True, X will be copied; else, it may be overwritten.\n\n    verbose : bool, optional (default=False)\n        If True, the algorithm will print progress messages during fitting.\n\n    cv_tol : float, optional (default=0.1)\n        Tolerance for cross-validation. If the percentage change in cross-validation score\n        is below this threshold, the algorithm terminates.\n\n    cv : bool, optional (default=False)\n        If True, cross-validation will be used to determine the optimal number of features.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,)\n        Coefficients of the regression model (mean of the posterior distribution).\n\n    alpha_ : float\n        Estimated precision of the noise.\n\n    active_ : array, dtype=bool, shape (n_features,)\n        Boolean array indicating which features are active (non-zero coefficients).\n\n    lambda_ : array, shape (n_features,)\n        Estimated precisions of the coefficients.\n\n    sigma_ : array, shape (n_features, n_features)\n        Estimated covariance matrix of the weights, computed only for non-zero coefficients.\n\n    scores_ : list\n        List of cross-validation scores if `cv` is True.\n\n    Methods\n    -------\n    fit(X, y)\n        Fit the ARD regression model to the data.\n\n    predict_dist(X)\n        Compute the predictive distribution for the test set.\n\n    _center_data(X, y)\n        Center the data by subtracting the mean.\n\n    _posterior_dist(A, beta, XX, XY, full_covar=False)\n        Calculate the mean and covariance matrix of the posterior distribution of coefficients.\n\n    _sparsity_quality(XX, XXd, XY, XYa, Aa, Ri, active, beta, cholesky)\n        Calculate sparsity and quality parameters for each feature.\n\n    References\n    ----------\n    [1] Tipping, M. E., &amp; Faul, A. C. (2003). Fast marginal likelihood maximisation for\n        sparse Bayesian models. In Proceedings of the Ninth International Workshop on\n        Artificial Intelligence and Statistics (pp. 276-283).\n    [2] Tipping, M. E., &amp; Faul, A. C. (2001). Analysis of sparse Bayesian learning. In\n        Advances in Neural Information Processing Systems (pp. 383-389).\n    '''\n\n    def __init__( self, n_iter = 300, tol = 1e-3, fit_intercept = True, \n                  copy_X = True, verbose = False, cv_tol = 0.1, cv=False):\n        self.n_iter          = n_iter\n        self.tol             = tol\n        self.scores_         = list()\n        self.fit_intercept   = fit_intercept\n        self.copy_X          = copy_X\n        self.verbose         = verbose\n        self.cv              = cv\n        self.cv_tol          = cv_tol\n\n\n    def _center_data(self,X,y):\n        ''' Centers data'''\n        X     = as_float_array(X, copy=self.copy_X)\n        # normalisation should be done in preprocessing!\n        X_std = np.ones(X.shape[1], dtype = X.dtype)\n        if self.fit_intercept:\n            X_mean = np.average(X,axis = 0)\n            y_mean = np.average(y,axis = 0)\n            X     -= X_mean\n            y      = y - y_mean\n        else:\n            X_mean = np.zeros(X.shape[1],dtype = X.dtype)\n            y_mean = 0. if y.ndim == 1 else np.zeros(y.shape[1], dtype=X.dtype)\n        return X,y, X_mean, y_mean, X_std\n\n\n    def fit(self,X,y):\n        '''\n        Fit the ARD regression model to the data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data, matrix of explanatory variables.\n\n        y : array-like, shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        '''\n        X, y = check_X_y(X, y, dtype=np.float64, y_numeric=True)\n        X, y, X_mean, y_mean, X_std = self._center_data(X, y)\n        n_samples, n_features = X.shape\n        cv_list = []\n        cv_score_history = [] \n        current_r = 0\n\n        #  precompute X'*Y , X'*X for faster iterations &amp; allocate memory for\n        #  sparsity &amp; quality vectors\n        XY     = np.dot(X.T,y)\n        XX     = np.dot(X.T,X)\n        XXd    = np.diag(XX)\n\n        #  initialise precision of noise &amp; and coefficients\n        var_y  = np.var(y)\n\n        # check that variance is non zero !!!\n        if var_y == 0 :\n            beta = 1e-2\n        else:\n            beta = 1. / np.var(y)\n\n        A      = np.PINF * np.ones(n_features)\n        active = np.zeros(n_features , dtype = bool)\n\n        # in case of almost perfect multicollinearity between some features\n        # start from feature 0\n        if np.sum( XXd - X_mean**2 &lt; np.finfo(np.float32).eps ) &gt; 0:\n            A[0]       = np.finfo(np.float16).eps\n            active[0]  = True\n        else:\n            # start from a single basis vector with largest projection on targets\n            proj  = XY**2 / XXd\n            start = np.argmax(proj)\n            active[start] = True\n            A[start]      = XXd[start]/( proj[start] - var_y)\n\n        warning_flag = 0\n        for i in range(self.n_iter):\n            XXa     = XX[active,:][:,active]\n            XYa     = XY[active]\n            Aa      =  A[active]\n\n            # mean &amp; covariance of posterior distribution\n            Mn,Ri,cholesky  = self._posterior_dist(Aa,beta,XXa,XYa)\n            if cholesky:\n                Sdiag  = np.sum(Ri**2,0)\n            else:\n                Sdiag  = np.copy(np.diag(Ri)) \n                warning_flag += 1\n\n            # raise warning in case cholesky failes\n            if warning_flag == 1:\n                warnings.warn((\"Cholesky decomposition failed ! Algorithm uses pinvh, \"\n                               \"which is significantly slower, if you use RVR it \"\n                               \"is advised to change parameters of kernel\"))\n\n            # compute quality &amp; sparsity parameters            \n            s,q,S,Q = self._sparsity_quality(XX,XXd,XY,XYa,Aa,Ri,active,beta,cholesky)\n\n            # update precision parameter for noise distribution\n            rss     = np.sum( ( y - np.dot(X[:,active] , Mn) )**2 )\n            beta    = n_samples - np.sum(active) + np.sum(Aa * Sdiag )\n            beta   /= ( rss + np.finfo(np.float32).eps )\n\n            # update precision parameters of coefficients\n            A,converged  = update_precisions(Q,S,q,s,A,active,self.tol,\n                                             n_samples,False)\n# ***************************************            \n            if self.cv:\n                # Select features based on the 'active' mask\n                # Assumes X is a numpy array for efficient slicing\n                X_active = X[:, active]\n\n                # Define the model for cross-validation (instantiated fresh each time)\n                cv_model = linear_model.Ridge()\n\n                try:\n                    # Perform 10-fold cross-validation, explicitly using R^2 scoring\n                    # Ensure 'y' corresponds correctly to 'X_active'\n                    cv_scores = cross_val_score(cv_model, X_active, y, cv=10, scoring='r2')\n                    new_cv_score = np.mean(cv_scores) # Use numpy mean for clarity\n\n                    # Calculate percentage change, handling division by zero\n                    # Assumes current_cv_score is initialized (e.g., to None or 0.0) before the loop\n                    if current_cv_score is not None and current_cv_score != 0:\n                        percentage_change = (new_cv_score - current_cv_score) / current_cv_score * 100\n                    elif new_cv_score == 0 and (current_cv_score is None or current_cv_score == 0):\n                        percentage_change = 0.0 # No change if both old and new scores are zero\n                    else:\n                        # Handle cases where current_cv_score is None (first iteration) or zero\n                        percentage_change = np.inf # Indicate a large change if starting from zero/None\n\n                    # Optional: Replace print with logging for better control in applications\n                    # Assumes 'i' is an iteration counter from an outer loop\n                    print(f\"Iteration {i}: CV Score = {new_cv_score:.4f}, % Change = {percentage_change:.2f}%\")\n\n                    # Check for convergence based on the absolute percentage change\n                    # Assumes cv_tol is a positive threshold for the magnitude of change\n                    # Assumes 'converged' is initialized (e.g., to False) before the loop\n                    if current_cv_score is not None and abs(percentage_change) &lt; self.cv_tol:\n                        converged = True\n                        # Consider adding a 'break' here if the loop should terminate immediately upon convergence\n\n                    # Update the current score and history\n                    # Assumes cv_score_history is initialized (e.g., as []) before the loop\n                    current_cv_score = new_cv_score\n                    cv_score_history.append(new_cv_score)\n\n                except ValueError as ve:\n                    # Catch specific errors, e.g., if X_active becomes empty or has incompatible dimensions\n                    print(f\"Warning: Cross-validation failed at iteration {i} due to ValueError: {ve}\")\n                    # Decide how to handle: stop, skip, assign default score?\n                    # Example: Treat as no improvement or break\n                    percentage_change = np.nan # Mark as invalid\n                    # converged = True # Option: Stop if CV fails\n                except Exception as e:\n                    # Catch other potential errors during cross-validation\n                    print(f\"Warning: Cross-validation failed unexpectedly at iteration {i}: {e}\")\n                    percentage_change = np.nan\n                    # converged = True # Option: Stop if CV fails\n\n\n            # Calculate active features once per iteration\n            num_active_features = np.sum(active)\n\n            if self.verbose:\n                # Use logging (assuming logger is configured) and f-string for iteration progress\n                # import logging  # Ensure logging is imported at the top of the file\n                logging.info(f\"Iteration: {i}, Active Features: {num_active_features}\")\n\n            # Check for convergence or max iterations to terminate\n            if converged or i == self.n_iter - 1:\n                # Construct the final status message\n                final_status = f\"Finished at Iteration: {i}, Active Features: {num_active_features}.\"\n                if converged:\n                    log_level = logging.INFO # Normal convergence\n                    final_status += \" Algorithm converged.\"\n                    # The original code printed \"Algorithm converged !\" only if verbose.\n                    # Logging INFO level covers this sufficiently. Add DEBUG if more detail needed.\n                    # if self.verbose:\n                    #    logging.debug(\"Convergence details: ...\")\n                else: # i == self.n_iter - 1\n                    log_level = logging.WARNING # Reached max iterations without converging\n                    final_status += f\" Reached maximum iterations ({self.n_iter}).\"\n\n                # Log the final status\n                logging.log(log_level, final_status)\n                break # Exit the loop\n\n        #print(('Iteration: {0}, number of features '\n        #               'in the model: {1}').format(i,np.sum(active)))      \n\n        # after last update of alpha &amp; beta update parameters\n        # of posterior distribution\n        XXa,XYa,Aa         = XX[active,:][:,active],XY[active],A[active]\n        Mn, Sn, cholesky   = self._posterior_dist(Aa,beta,XXa,XYa,True)\n        self.coef_         = np.zeros(n_features)\n        self.coef_[active] = Mn\n        self.sigma_        = Sn\n        self.active_       = active\n        self.lambda_       = A\n        self.alpha_        = beta\n        self._set_intercept(X_mean,y_mean,X_std)\n        if self.cv :\n        #    print(max(enumerate(cv_list), key=lambda x: x[1]))\n            print(('Iteration: {0}, number of features '\n                       'in the model: {1}').format(i,np.sum(active))) \n        return self\n\n\n    def _posterior_dist(self,A,beta,XX,XY,full_covar=False):\n        '''\n        Calculate the mean and covariance matrix of the posterior distribution of coefficients.\n\n        Parameters\n        ----------\n        A : array, shape (n_features,)\n            Precision parameters for the coefficients.\n\n        beta : float\n            Precision of the noise.\n\n        XX : array, shape (n_features, n_features)\n            X' * X matrix.\n\n        XY : array, shape (n_features,)\n            X' * y vector.\n\n        full_covar : bool, optional (default=False)\n            If True, return the full covariance matrix; otherwise, return the inverse of the\n            lower triangular matrix from the Cholesky decomposition.\n\n        Returns\n        -------\n        Mn : array, shape (n_features,)\n            Mean of the posterior distribution.\n\n        Sn : array, shape (n_features, n_features)\n            Covariance matrix of the posterior distribution.\n\n        cholesky : bool\n            Whether the Cholesky decomposition was successful.\n        '''\n        # compute precision matrix for active features\n        Sinv = beta * XX\n        np.fill_diagonal(Sinv, np.diag(Sinv) + A)\n        cholesky = True\n        # try cholesky, if it fails go back to pinvh\n        try:\n            # find posterior mean : R*R.T*mean = beta*X.T*Y\n            # solve(R*z = beta*X.T*Y) =&gt; find z =&gt; solve(R.T*mean = z) =&gt; find mean\n            R    = np.linalg.cholesky(Sinv)\n            Z    = solve_triangular(R,beta*XY, check_finite=False, lower = True)\n            Mn   = solve_triangular(R.T,Z, check_finite=False, lower = False)\n\n            # invert lower triangular matrix from cholesky decomposition\n            Ri   = solve_triangular(R,np.eye(A.shape[0]), check_finite=False, lower=True)\n            if full_covar:\n                Sn   = np.dot(Ri.T,Ri)\n                return Mn,Sn,cholesky\n            else:\n                return Mn,Ri,cholesky\n        except LinAlgError:\n            cholesky = False\n            Sn   = pinvh(Sinv)\n            Mn   = beta*np.dot(Sinv,XY)\n            return Mn, Sn, cholesky\n\n\n    def _sparsity_quality(self,XX,XXd,XY,XYa,Aa,Ri,active,beta,cholesky):\n        '''\n        Calculate sparsity and quality parameters for each feature.\n\n        Parameters\n        ----------\n        XX : array, shape (n_features, n_features)\n            X' * X matrix.\n\n        XXd : array, shape (n_features,)\n            Diagonal of X' * X matrix.\n\n        XY : array, shape (n_features,)\n            X' * y vector.\n\n        XYa : array, shape (n_active_features,)\n            X' * y vector for active features.\n\n        Aa : array, shape (n_active_features,)\n            Precision parameters for active features.\n\n        Ri : array, shape (n_active_features, n_active_features)\n            Inverse of the lower triangular matrix from the Cholesky decomposition or the\n            covariance matrix.\n\n        active : array, dtype=bool, shape (n_features,)\n            Boolean array indicating which features are active.\n\n        beta : float\n            Precision of the noise.\n\n        cholesky : bool\n            Whether the Cholesky decomposition was successful.\n\n        Returns\n        -------\n        si : array, shape (n_features,)\n            Sparsity parameters.\n\n        qi : array, shape (n_features,)\n            Quality parameters.\n\n        S : array, shape (n_features,)\n            Intermediate sparsity parameters.\n\n        Q : array, shape (n_features,)\n            Intermediate quality parameters.\n\n        Theoretical Note:\n        -----------------\n        Here we used Woodbury Identity for inverting covariance matrix\n        of target distribution \n        C    = 1/beta + 1/alpha * X' * X\n        C^-1 = beta - beta^2 * X * Sn * X'\n        '''\n\n        bxy        = beta*XY\n        bxx        = beta*XXd\n        if cholesky:\n            # here Ri is inverse of lower triangular matrix obtained from cholesky decomp\n            xxr    = np.dot(XX[:,active],Ri.T)\n            rxy    = np.dot(Ri,XYa)\n            S      = bxx - beta**2 * np.sum( xxr**2, axis=1)\n            Q      = bxy - beta**2 * np.dot( xxr, rxy)\n        else:\n            # here Ri is covariance matrix\n            XXa    = XX[:,active]\n            XS     = np.dot(XXa,Ri)\n            S      = bxx - beta**2 * np.sum(XS*XXa,1)\n            Q      = bxy - beta**2 * np.dot(XS,XYa)\n        # Use following:\n        # (EQ 1) q = A*Q/(A - S) ; s = A*S/(A-S), so if A = np.PINF q = Q, s = S\n        qi         = np.copy(Q)\n        si         = np.copy(S) \n        #  If A is not np.PINF, then it should be 'active' feature =&gt; use (EQ 1)\n        Qa,Sa      = Q[active], S[active]\n        qi[active] = Aa * Qa / (Aa - Sa )\n        si[active] = Aa * Sa / (Aa - Sa )\n        return [si,qi,S,Q]\n\n    def predict_dist(self,X):\n        '''\n        Computes predictive distribution for test set.\n        Predictive distribution for each data point is one dimensional\n        Gaussian and therefore is characterised by mean and variance.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples_test, n_features)\n            Test data, matrix of explanatory variables.\n\n        Returns\n        -------\n        y_hat : array, shape (n_samples_test,)\n            Estimated values of targets on the test set (mean of the predictive distribution).\n\n        var_hat : array, shape (n_samples_test,)\n            Variance of the predictive distribution.\n        '''\n        y_hat     = self._decision_function(X)\n        var_hat   = 1./self.alpha_\n        var_hat  += np.sum( np.dot(X[:,self.active_],self.sigma_) * X[:,self.active_], axis = 1)\n        return y_hat, var_hat\n</code></pre>"},{"location":"reference/api/#shapleyx.ARD.RegressionARD.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the ARD regression model to the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like, sparse matrix</code> <p>Training data, matrix of explanatory variables.</p> <code>array-like</code> <code>y</code> <code>(array - like, shape(n_samples))</code> <p>Target values.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Returns the instance itself.</p> Source code in <code>shapleyx\\ARD.py</code> <pre><code>    def fit(self,X,y):\n        '''\n        Fit the ARD regression model to the data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training data, matrix of explanatory variables.\n\n        y : array-like, shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        '''\n        X, y = check_X_y(X, y, dtype=np.float64, y_numeric=True)\n        X, y, X_mean, y_mean, X_std = self._center_data(X, y)\n        n_samples, n_features = X.shape\n        cv_list = []\n        cv_score_history = [] \n        current_r = 0\n\n        #  precompute X'*Y , X'*X for faster iterations &amp; allocate memory for\n        #  sparsity &amp; quality vectors\n        XY     = np.dot(X.T,y)\n        XX     = np.dot(X.T,X)\n        XXd    = np.diag(XX)\n\n        #  initialise precision of noise &amp; and coefficients\n        var_y  = np.var(y)\n\n        # check that variance is non zero !!!\n        if var_y == 0 :\n            beta = 1e-2\n        else:\n            beta = 1. / np.var(y)\n\n        A      = np.PINF * np.ones(n_features)\n        active = np.zeros(n_features , dtype = bool)\n\n        # in case of almost perfect multicollinearity between some features\n        # start from feature 0\n        if np.sum( XXd - X_mean**2 &lt; np.finfo(np.float32).eps ) &gt; 0:\n            A[0]       = np.finfo(np.float16).eps\n            active[0]  = True\n        else:\n            # start from a single basis vector with largest projection on targets\n            proj  = XY**2 / XXd\n            start = np.argmax(proj)\n            active[start] = True\n            A[start]      = XXd[start]/( proj[start] - var_y)\n\n        warning_flag = 0\n        for i in range(self.n_iter):\n            XXa     = XX[active,:][:,active]\n            XYa     = XY[active]\n            Aa      =  A[active]\n\n            # mean &amp; covariance of posterior distribution\n            Mn,Ri,cholesky  = self._posterior_dist(Aa,beta,XXa,XYa)\n            if cholesky:\n                Sdiag  = np.sum(Ri**2,0)\n            else:\n                Sdiag  = np.copy(np.diag(Ri)) \n                warning_flag += 1\n\n            # raise warning in case cholesky failes\n            if warning_flag == 1:\n                warnings.warn((\"Cholesky decomposition failed ! Algorithm uses pinvh, \"\n                               \"which is significantly slower, if you use RVR it \"\n                               \"is advised to change parameters of kernel\"))\n\n            # compute quality &amp; sparsity parameters            \n            s,q,S,Q = self._sparsity_quality(XX,XXd,XY,XYa,Aa,Ri,active,beta,cholesky)\n\n            # update precision parameter for noise distribution\n            rss     = np.sum( ( y - np.dot(X[:,active] , Mn) )**2 )\n            beta    = n_samples - np.sum(active) + np.sum(Aa * Sdiag )\n            beta   /= ( rss + np.finfo(np.float32).eps )\n\n            # update precision parameters of coefficients\n            A,converged  = update_precisions(Q,S,q,s,A,active,self.tol,\n                                             n_samples,False)\n# ***************************************            \n            if self.cv:\n                # Select features based on the 'active' mask\n                # Assumes X is a numpy array for efficient slicing\n                X_active = X[:, active]\n\n                # Define the model for cross-validation (instantiated fresh each time)\n                cv_model = linear_model.Ridge()\n\n                try:\n                    # Perform 10-fold cross-validation, explicitly using R^2 scoring\n                    # Ensure 'y' corresponds correctly to 'X_active'\n                    cv_scores = cross_val_score(cv_model, X_active, y, cv=10, scoring='r2')\n                    new_cv_score = np.mean(cv_scores) # Use numpy mean for clarity\n\n                    # Calculate percentage change, handling division by zero\n                    # Assumes current_cv_score is initialized (e.g., to None or 0.0) before the loop\n                    if current_cv_score is not None and current_cv_score != 0:\n                        percentage_change = (new_cv_score - current_cv_score) / current_cv_score * 100\n                    elif new_cv_score == 0 and (current_cv_score is None or current_cv_score == 0):\n                        percentage_change = 0.0 # No change if both old and new scores are zero\n                    else:\n                        # Handle cases where current_cv_score is None (first iteration) or zero\n                        percentage_change = np.inf # Indicate a large change if starting from zero/None\n\n                    # Optional: Replace print with logging for better control in applications\n                    # Assumes 'i' is an iteration counter from an outer loop\n                    print(f\"Iteration {i}: CV Score = {new_cv_score:.4f}, % Change = {percentage_change:.2f}%\")\n\n                    # Check for convergence based on the absolute percentage change\n                    # Assumes cv_tol is a positive threshold for the magnitude of change\n                    # Assumes 'converged' is initialized (e.g., to False) before the loop\n                    if current_cv_score is not None and abs(percentage_change) &lt; self.cv_tol:\n                        converged = True\n                        # Consider adding a 'break' here if the loop should terminate immediately upon convergence\n\n                    # Update the current score and history\n                    # Assumes cv_score_history is initialized (e.g., as []) before the loop\n                    current_cv_score = new_cv_score\n                    cv_score_history.append(new_cv_score)\n\n                except ValueError as ve:\n                    # Catch specific errors, e.g., if X_active becomes empty or has incompatible dimensions\n                    print(f\"Warning: Cross-validation failed at iteration {i} due to ValueError: {ve}\")\n                    # Decide how to handle: stop, skip, assign default score?\n                    # Example: Treat as no improvement or break\n                    percentage_change = np.nan # Mark as invalid\n                    # converged = True # Option: Stop if CV fails\n                except Exception as e:\n                    # Catch other potential errors during cross-validation\n                    print(f\"Warning: Cross-validation failed unexpectedly at iteration {i}: {e}\")\n                    percentage_change = np.nan\n                    # converged = True # Option: Stop if CV fails\n\n\n            # Calculate active features once per iteration\n            num_active_features = np.sum(active)\n\n            if self.verbose:\n                # Use logging (assuming logger is configured) and f-string for iteration progress\n                # import logging  # Ensure logging is imported at the top of the file\n                logging.info(f\"Iteration: {i}, Active Features: {num_active_features}\")\n\n            # Check for convergence or max iterations to terminate\n            if converged or i == self.n_iter - 1:\n                # Construct the final status message\n                final_status = f\"Finished at Iteration: {i}, Active Features: {num_active_features}.\"\n                if converged:\n                    log_level = logging.INFO # Normal convergence\n                    final_status += \" Algorithm converged.\"\n                    # The original code printed \"Algorithm converged !\" only if verbose.\n                    # Logging INFO level covers this sufficiently. Add DEBUG if more detail needed.\n                    # if self.verbose:\n                    #    logging.debug(\"Convergence details: ...\")\n                else: # i == self.n_iter - 1\n                    log_level = logging.WARNING # Reached max iterations without converging\n                    final_status += f\" Reached maximum iterations ({self.n_iter}).\"\n\n                # Log the final status\n                logging.log(log_level, final_status)\n                break # Exit the loop\n\n        #print(('Iteration: {0}, number of features '\n        #               'in the model: {1}').format(i,np.sum(active)))      \n\n        # after last update of alpha &amp; beta update parameters\n        # of posterior distribution\n        XXa,XYa,Aa         = XX[active,:][:,active],XY[active],A[active]\n        Mn, Sn, cholesky   = self._posterior_dist(Aa,beta,XXa,XYa,True)\n        self.coef_         = np.zeros(n_features)\n        self.coef_[active] = Mn\n        self.sigma_        = Sn\n        self.active_       = active\n        self.lambda_       = A\n        self.alpha_        = beta\n        self._set_intercept(X_mean,y_mean,X_std)\n        if self.cv :\n        #    print(max(enumerate(cv_list), key=lambda x: x[1]))\n            print(('Iteration: {0}, number of features '\n                       'in the model: {1}').format(i,np.sum(active))) \n        return self\n</code></pre>"},{"location":"reference/api/#shapleyx.ARD.RegressionARD.predict_dist","title":"<code>predict_dist(X)</code>","text":"<p>Computes predictive distribution for test set. Predictive distribution for each data point is one dimensional Gaussian and therefore is characterised by mean and variance.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like, sparse matrix</code> <p>Test data, matrix of explanatory variables.</p> <code>array-like</code> <p>Returns:</p> Name Type Description <code>y_hat</code> <code>(array, shape(n_samples_test))</code> <p>Estimated values of targets on the test set (mean of the predictive distribution).</p> <code>var_hat</code> <code>(array, shape(n_samples_test))</code> <p>Variance of the predictive distribution.</p> Source code in <code>shapleyx\\ARD.py</code> <pre><code>def predict_dist(self,X):\n    '''\n    Computes predictive distribution for test set.\n    Predictive distribution for each data point is one dimensional\n    Gaussian and therefore is characterised by mean and variance.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape (n_samples_test, n_features)\n        Test data, matrix of explanatory variables.\n\n    Returns\n    -------\n    y_hat : array, shape (n_samples_test,)\n        Estimated values of targets on the test set (mean of the predictive distribution).\n\n    var_hat : array, shape (n_samples_test,)\n        Variance of the predictive distribution.\n    '''\n    y_hat     = self._decision_function(X)\n    var_hat   = 1./self.alpha_\n    var_hat  += np.sum( np.dot(X[:,self.active_],self.sigma_) * X[:,self.active_], axis = 1)\n    return y_hat, var_hat\n</code></pre>"},{"location":"reference/api/#shapleyx.ARD.update_precisions","title":"<code>update_precisions(Q, S, q, s, A, active, tol, n_samples, clf_bias)</code>","text":"<p>Updates the precision parameters (alpha) for features in a sparse Bayesian learning model by selecting a feature to add, recompute, or delete based on its impact on the log marginal likelihood. The function also checks for convergence.</p> Parameters: <p>Q : numpy.ndarray     Quality parameters for all features. S : numpy.ndarray     Sparsity parameters for all features. q : numpy.ndarray     Quality parameters for features currently in the model. s : numpy.ndarray     Sparsity parameters for features currently in the model. A : numpy.ndarray     Precision parameters (alpha) for all features. active : numpy.ndarray (bool)     Boolean array indicating whether each feature is currently in the model. tol : float     Tolerance threshold for determining convergence based on changes in precision. n_samples : int     Number of samples in the dataset, used to normalize the change in log marginal likelihood. clf_bias : bool     Flag indicating whether the model includes a bias term (used in classification tasks).</p> Returns: <p>list     A list containing two elements:     - Updated precision parameters (A) for all features.     - A boolean flag indicating whether the model has converged.</p> Notes: <p>The function performs the following steps: 1. Computes the change in log marginal likelihood for adding, recomputing, or deleting features. 2. Identifies the feature that causes the largest change in likelihood. 3. Updates the precision parameter (alpha) for the selected feature. 4. Checks for convergence based on whether no features are added/deleted and changes in precision    are below the specified tolerance. 5. Returns the updated precision parameters and convergence status.</p> <p>Convergence is determined by two conditions: - No features are added or deleted. - The change in precision for features already in the model is below the tolerance threshold.</p> <p>The function ensures that the bias term is not removed in classification tasks.</p> Source code in <code>shapleyx\\ARD.py</code> <pre><code>def update_precisions(Q,S,q,s,A,active,tol,n_samples,clf_bias):\n    '''\n    Updates the precision parameters (alpha) for features in a sparse Bayesian learning model\n    by selecting a feature to add, recompute, or delete based on its impact on the log marginal\n    likelihood. The function also checks for convergence.\n\n    Parameters:\n    -----------\n    Q : numpy.ndarray\n        Quality parameters for all features.\n    S : numpy.ndarray\n        Sparsity parameters for all features.\n    q : numpy.ndarray\n        Quality parameters for features currently in the model.\n    s : numpy.ndarray\n        Sparsity parameters for features currently in the model.\n    A : numpy.ndarray\n        Precision parameters (alpha) for all features.\n    active : numpy.ndarray (bool)\n        Boolean array indicating whether each feature is currently in the model.\n    tol : float\n        Tolerance threshold for determining convergence based on changes in precision.\n    n_samples : int\n        Number of samples in the dataset, used to normalize the change in log marginal likelihood.\n    clf_bias : bool\n        Flag indicating whether the model includes a bias term (used in classification tasks).\n\n    Returns:\n    --------\n    list\n        A list containing two elements:\n        - Updated precision parameters (A) for all features.\n        - A boolean flag indicating whether the model has converged.\n\n    Notes:\n    ------\n    The function performs the following steps:\n    1. Computes the change in log marginal likelihood for adding, recomputing, or deleting features.\n    2. Identifies the feature that causes the largest change in likelihood.\n    3. Updates the precision parameter (alpha) for the selected feature.\n    4. Checks for convergence based on whether no features are added/deleted and changes in precision\n       are below the specified tolerance.\n    5. Returns the updated precision parameters and convergence status.\n\n    Convergence is determined by two conditions:\n    - No features are added or deleted.\n    - The change in precision for features already in the model is below the tolerance threshold.\n\n    The function ensures that the bias term is not removed in classification tasks.\n    '''\n    # initialise vector holding changes in log marginal likelihood\n    deltaL = np.zeros(Q.shape[0])\n\n    # identify features that can be added , recomputed and deleted in model\n    theta        =  q**2 - s \n    add          =  (theta &gt; 0) * (active == False)\n    recompute    =  (theta &gt; 0) * (active == True)\n    delete       = ~(add + recompute)\n\n    # compute sparsity &amp; quality parameters corresponding to features in \n    # three groups identified above\n    Qadd,Sadd      = Q[add], S[add]\n    Qrec,Srec,Arec = Q[recompute], S[recompute], A[recompute]\n    Qdel,Sdel,Adel = Q[delete], S[delete], A[delete]\n\n    # compute new alpha's (precision parameters) for features that are \n    # currently in model and will be recomputed\n    Anew           = s[recompute]**2/ ( theta[recompute] + np.finfo(np.float32).eps)\n    delta_alpha    = (1./Anew - 1./Arec)\n\n    # compute change in log marginal likelihood \n    deltaL[add]       = ( Qadd**2 - Sadd ) / Sadd + np.log(Sadd/Qadd**2 )\n    deltaL[recompute] = Qrec**2 / (Srec + 1. / delta_alpha) - np.log(1 + Srec*delta_alpha)\n    deltaL[delete]    = Qdel**2 / (Sdel - Adel) - np.log(1 - Sdel / Adel)\n    deltaL            = deltaL  / n_samples\n\n    # find feature which caused largest change in likelihood\n    feature_index = np.argmax(deltaL)\n\n    # no deletions or additions\n    same_features  = np.sum( theta[~recompute] &gt; 0) == 0\n\n    # changes in precision for features already in model is below threshold\n    no_delta       = np.sum( abs( Anew - Arec ) &gt; tol ) == 0\n\n    # check convergence: if no features to add or delete and small change in \n    #                    precision for current features then terminate\n    converged = False\n    if same_features and no_delta:\n        converged = True\n        return [A,converged]\n\n    # if not converged update precision parameter of weights and return\n    if theta[feature_index] &gt; 0:\n        A[feature_index] = s[feature_index]**2 / theta[feature_index]\n        if active[feature_index] == False:\n            active[feature_index] = True\n    else:\n        # at least two active features\n        if active[feature_index] == True and np.sum(active) &gt;= 2:\n            # do not remove bias term in classification \n            # (in regression it is factored in through centering)\n            if not (feature_index == 0 and clf_bias):\n               active[feature_index] = False\n               A[feature_index]      = np.PINF\n\n    return [A,converged]\n</code></pre>"},{"location":"reference/api/#shapleyx.xsampler","title":"<code>shapleyx.xsampler</code>","text":""},{"location":"reference/api/#shapleyx.xsampler.xsampler","title":"<code>xsampler(num_samples, ranges)</code>","text":"<p>Generate a Latin Hypercube sample scaled to the specified ranges.</p> <p>Args:     num_samples (int): Number of samples to generate.     ranges (dict): A dictionary where keys are feature names and values are tuples of (lower, upper) bounds.</p> <p>Returns:     np.ndarray: A scaled Latin Hypercube sample of shape (num_samples, num_features).</p> Source code in <code>shapleyx\\xsampler.py</code> <pre><code>def xsampler(num_samples: int, ranges: dict) -&gt; np.ndarray:\n    \"\"\"\n    Generate a Latin Hypercube sample scaled to the specified ranges.\n\n    Args:\n        num_samples (int): Number of samples to generate.\n        ranges (dict): A dictionary where keys are feature names and values are tuples of (lower, upper) bounds.\n\n    Returns:\n        np.ndarray: A scaled Latin Hypercube sample of shape (num_samples, num_features).\n    \"\"\"\n    num_features = len(ranges)\n\n    # Extract lower and upper bounds from the ranges dictionary\n    lower_bounds = [bounds[0] for bounds in ranges.values()]\n    upper_bounds = [bounds[1] for bounds in ranges.values()]\n\n    # Generate Latin Hypercube sample\n    sampler = qmc.LatinHypercube(d=num_features)\n    sample = sampler.random(n=num_samples)\n\n    # Scale the sample to the specified ranges\n    sample_scaled = qmc.scale(sample, lower_bounds, upper_bounds)\n\n    return sample_scaled \n</code></pre>"},{"location":"tutorials/basic-usage/","title":"Basic Usage Tutorial","text":"<p>This tutorial walks through a complete sensitivity analysis workflow with ShapleyX.</p>"},{"location":"tutorials/basic-usage/#step-1-prepare-your-data","title":"Step 1: Prepare Your Data","text":"<pre><code>import pandas as pd\nimport numpy as np\n\n# Generate sample data\nnp.random.seed(42)\nX = np.random.rand(1000, 5)  # 1000 samples, 5 parameters\nY = X[:,0]**2 + 2*X[:,1]*X[:,2] + np.sin(X[:,3]) + X[:,4]\n\n# Create DataFrame\ndata = pd.DataFrame(X, columns=['x1', 'x2', 'x3', 'x4', 'x5'])\ndata['Y'] = Y\ndata.to_csv('sample_data.csv', index=False)\n</code></pre>"},{"location":"tutorials/basic-usage/#step-2-initialize-the-analyzer","title":"Step 2: Initialize the Analyzer","text":"<pre><code>from shapleyx import rshdmr\n\nanalyzer = rshdmr(\n    data_file='sample_data.csv',\n    polys=[10, 5],  # Polynomial orders\n    method='ard',   # Automatic Relevance Determination\n    verbose=True\n)\n</code></pre>"},{"location":"tutorials/basic-usage/#step-3-run-the-analysis","title":"Step 3: Run the Analysis","text":"<pre><code># Run complete analysis pipeline\nsobol_indices, shapley_effects, total_index = analyzer.run_all()\n\n# View results\nprint(\"Sobol Indices:\")\nprint(sobol_indices)\n\nprint(\"\\nShapley Effects:\")\nprint(shapley_effects)\n\nprint(\"\\nTotal Indices:\")\nprint(total_index)\n</code></pre>"},{"location":"tutorials/basic-usage/#step-4-visualize-results","title":"Step 4: Visualize Results","text":"<pre><code># Plot predicted vs actual\nanalyzer.plot_hdmr()\n\n# Plot sensitivity indices\nanalyzer.plot_indices()\n</code></pre>"},{"location":"tutorials/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Try with your own dataset</li> <li>Experiment with different polynomial orders</li> <li>Explore advanced configuration options</li> </ul>"}]}