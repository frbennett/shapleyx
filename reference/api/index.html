
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://frederickbennett.github.io/shapleyx/reference/api/">
      
      
        <link rel="prev" href="../../how-to-guides/common-tasks/">
      
      
        <link rel="next" href="../../explanation/theory/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.11">
    
    
      
        <title>API - ShapleyX Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#api-reference" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="ShapleyX Documentation" class="md-header__button md-logo" aria-label="ShapleyX Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ShapleyX Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              API
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/frederickbennett/shapleyx" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../getting-started/installation/" class="md-tabs__link">
          
  
  
    
  
  Getting Started

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../tutorials/basic-usage/" class="md-tabs__link">
          
  
  
    
  
  Tutorials

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../how-to-guides/common-tasks/" class="md-tabs__link">
          
  
  
    
  
  How-to Guides

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="./" class="md-tabs__link">
          
  
  
    
  
  Reference

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../explanation/theory/" class="md-tabs__link">
          
  
  
    
  
  Explanation

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ShapleyX Documentation" class="md-nav__button md-logo" aria-label="ShapleyX Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    ShapleyX Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/frederickbennett/shapleyx" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Getting Started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Getting Started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-started/installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-started/quickstart/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quickstart
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Tutorials
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/basic-usage/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Basic Usage
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    How-to Guides
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            How-to Guides
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../how-to-guides/common-tasks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Common Tasks
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Reference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    API
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    API
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#shapleyx.rshdmr" class="md-nav__link">
    <span class="md-ellipsis">
      rshdmr
    </span>
  </a>
  
    <nav class="md-nav" aria-label="rshdmr">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#shapleyx.rshdmr.eval_all_indices" class="md-nav__link">
    <span class="md-ellipsis">
      eval_all_indices
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shapleyx.rshdmr.get_deltax" class="md-nav__link">
    <span class="md-ellipsis">
      get_deltax
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shapleyx.rshdmr.get_hx" class="md-nav__link">
    <span class="md-ellipsis">
      get_hx
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shapleyx.rshdmr.get_pawn" class="md-nav__link">
    <span class="md-ellipsis">
      get_pawn
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shapleyx.rshdmr.get_pawnx" class="md-nav__link">
    <span class="md-ellipsis">
      get_pawnx
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shapleyx.rshdmr.get_pruned_data" class="md-nav__link">
    <span class="md-ellipsis">
      get_pruned_data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shapleyx.rshdmr.legendre_expand" class="md-nav__link">
    <span class="md-ellipsis">
      legendre_expand
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shapleyx.rshdmr.plot_hdmr" class="md-nav__link">
    <span class="md-ellipsis">
      plot_hdmr
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shapleyx.rshdmr.predict" class="md-nav__link">
    <span class="md-ellipsis">
      predict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shapleyx.rshdmr.read_data" class="md-nav__link">
    <span class="md-ellipsis">
      read_data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shapleyx.rshdmr.run_all" class="md-nav__link">
    <span class="md-ellipsis">
      run_all
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shapleyx.rshdmr.run_regression" class="md-nav__link">
    <span class="md-ellipsis">
      run_regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shapleyx.rshdmr.stats" class="md-nav__link">
    <span class="md-ellipsis">
      stats
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shapleyx.rshdmr.transform_data" class="md-nav__link">
    <span class="md-ellipsis">
      transform_data
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#shapleyx.ARD" class="md-nav__link">
    <span class="md-ellipsis">
      ARD
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ARD">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#shapleyx.ARD.RegressionARD" class="md-nav__link">
    <span class="md-ellipsis">
      RegressionARD
    </span>
  </a>
  
    <nav class="md-nav" aria-label="RegressionARD">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#shapleyx.ARD.RegressionARD.fit" class="md-nav__link">
    <span class="md-ellipsis">
      fit
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shapleyx.ARD.RegressionARD.predict_dist" class="md-nav__link">
    <span class="md-ellipsis">
      predict_dist
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shapleyx.ARD.update_precisions" class="md-nav__link">
    <span class="md-ellipsis">
      update_precisions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#shapleyx.xsampler" class="md-nav__link">
    <span class="md-ellipsis">
      xsampler
    </span>
  </a>
  
    <nav class="md-nav" aria-label="xsampler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#shapleyx.xsampler.xsampler" class="md-nav__link">
    <span class="md-ellipsis">
      xsampler
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Explanation
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Explanation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../explanation/theory/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Theory
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="api-reference">API Reference</h1>


<div class="doc doc-object doc-class">



<h3 id="shapleyx.rshdmr" class="doc doc-heading">
            <code>shapleyx.rshdmr</code>


</h3>


    <div class="doc doc-contents first">


        <hr />
<p>Global Sensitivity Analysis using Sparse Random Sampling - High Dimensional 
Model Representation (HDMR) with Group Method of Data Handling (GMDH) for 
parameter selection and linear regression for parameter refinement.</p>
<hr />
<p>This module implements a global sensitivity analysis (GSA) framework using 
Sparse Random Sampling (SRS) combined with High Dimensional Model Representation 
(HDMR). The method employs the Group Method of Data Handling (GMDH) for parameter 
selection and linear regression for parameter refinement. The framework is designed 
to analyze the sensitivity of model outputs to input parameters, providing insights 
into the relative importance of each parameter and their interactions.</p>
<p>The module includes functionality for:
- Reading and preprocessing input data.
- Transforming data to a unit hypercube.
- Building basis functions using Legendre polynomials.
- Running regression analysis using various methods (ARD, OMP, etc.).
- Evaluating Sobol indices, Shapley effects, and total indices.
- Performing bootstrap resampling for confidence intervals.
- Calculating PAWN indices for sensitivity analysis.
- Predicting model outputs based on input parameters.</p>
<p>Author: Frederick Bennett</p>
<p>Classes:
    rshdmr: Main class for performing global sensitivity analysis using RS-HDMR.</p>
<p>Methods:
    <strong>init</strong>: Initializes the RS-HDMR object with input data and parameters.
    read_data: Reads and preprocesses input data.
    transform_data: Transforms input data to a unit hypercube.
    legendre_expand: Expands the input data using Legendre polynomials.
    run_regression: Runs regression analysis using specified method.
    stats: Computes and prints model performance statistics.
    plot_hdmr: Plots predicted vs. experimental values.
    eval_sobol_indices: Evaluates Sobol indices for sensitivity analysis.
    get_shapley: Computes Shapley effects for sensitivity analysis.
    get_total_index: Computes total sensitivity indices.
    get_pruned_data: Returns pruned dataset based on non-zero coefficients.
    get_pawn: Computes PAWN indices for sensitivity analysis.
    run_all: Runs the entire RS-HDMR analysis pipeline.
    predict: Predicts model outputs based on input parameters.
    get_pawnx: Computes PAWN indices with additional statistical analysis.</p>
<p>Example:
    # Initialize RS-HDMR object
    analyzer = rshdmr(data_file='input_data.csv', polys=[10, 5], method='ard')</p>
<div class="highlight"><pre><span></span><code># Run the entire analysis pipeline
sobol_indices, shapley_effects, total_index = analyzer.run_all()

# Predict model outputs for new input data
predictions = analyzer.predict(new_input_data)

# Compute PAWN indices
pawn_results = analyzer.get_pawn(S=10)
</code></pre></div>







              <details class="quote">
                <summary>Source code in <code>shapleyx\shapleyx.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">rshdmr</span><span class="p">():</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    *******************************************************************************</span>
<span class="sd">    Global Sensitivity Analysis using Sparse Random Sampling - High Dimensional </span>
<span class="sd">    Model Representation (HDMR) with Group Method of Data Handling (GMDH) for </span>
<span class="sd">    parameter selection and linear regression for parameter refinement.</span>
<span class="sd">    *******************************************************************************</span>

<span class="sd">    This module implements a global sensitivity analysis (GSA) framework using </span>
<span class="sd">    Sparse Random Sampling (SRS) combined with High Dimensional Model Representation </span>
<span class="sd">    (HDMR). The method employs the Group Method of Data Handling (GMDH) for parameter </span>
<span class="sd">    selection and linear regression for parameter refinement. The framework is designed </span>
<span class="sd">    to analyze the sensitivity of model outputs to input parameters, providing insights </span>
<span class="sd">    into the relative importance of each parameter and their interactions.</span>

<span class="sd">    The module includes functionality for:</span>
<span class="sd">    - Reading and preprocessing input data.</span>
<span class="sd">    - Transforming data to a unit hypercube.</span>
<span class="sd">    - Building basis functions using Legendre polynomials.</span>
<span class="sd">    - Running regression analysis using various methods (ARD, OMP, etc.).</span>
<span class="sd">    - Evaluating Sobol indices, Shapley effects, and total indices.</span>
<span class="sd">    - Performing bootstrap resampling for confidence intervals.</span>
<span class="sd">    - Calculating PAWN indices for sensitivity analysis.</span>
<span class="sd">    - Predicting model outputs based on input parameters.</span>

<span class="sd">    Author: Frederick Bennett</span>

<span class="sd">    Classes:</span>
<span class="sd">        rshdmr: Main class for performing global sensitivity analysis using RS-HDMR.</span>

<span class="sd">    Methods:</span>
<span class="sd">        __init__: Initializes the RS-HDMR object with input data and parameters.</span>
<span class="sd">        read_data: Reads and preprocesses input data.</span>
<span class="sd">        transform_data: Transforms input data to a unit hypercube.</span>
<span class="sd">        legendre_expand: Expands the input data using Legendre polynomials.</span>
<span class="sd">        run_regression: Runs regression analysis using specified method.</span>
<span class="sd">        stats: Computes and prints model performance statistics.</span>
<span class="sd">        plot_hdmr: Plots predicted vs. experimental values.</span>
<span class="sd">        eval_sobol_indices: Evaluates Sobol indices for sensitivity analysis.</span>
<span class="sd">        get_shapley: Computes Shapley effects for sensitivity analysis.</span>
<span class="sd">        get_total_index: Computes total sensitivity indices.</span>
<span class="sd">        get_pruned_data: Returns pruned dataset based on non-zero coefficients.</span>
<span class="sd">        get_pawn: Computes PAWN indices for sensitivity analysis.</span>
<span class="sd">        run_all: Runs the entire RS-HDMR analysis pipeline.</span>
<span class="sd">        predict: Predicts model outputs based on input parameters.</span>
<span class="sd">        get_pawnx: Computes PAWN indices with additional statistical analysis.</span>

<span class="sd">    Example:</span>
<span class="sd">        # Initialize RS-HDMR object</span>
<span class="sd">        analyzer = rshdmr(data_file=&#39;input_data.csv&#39;, polys=[10, 5], method=&#39;ard&#39;)</span>

<span class="sd">        # Run the entire analysis pipeline</span>
<span class="sd">        sobol_indices, shapley_effects, total_index = analyzer.run_all()</span>

<span class="sd">        # Predict model outputs for new input data</span>
<span class="sd">        predictions = analyzer.predict(new_input_data)</span>

<span class="sd">        # Compute PAWN indices</span>
<span class="sd">        pawn_results = analyzer.get_pawn(S=10)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Rest of the code...</span>


    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">data_file</span><span class="p">,</span> <span class="n">polys</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
                 <span class="n">n_jobs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
                 <span class="n">limit</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>
                 <span class="n">k_best</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">p_average</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">300</span><span class="p">,</span>
                 <span class="n">verbose</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">method</span> <span class="o">=</span> <span class="s1">&#39;ard&#39;</span><span class="p">,</span>
                 <span class="n">starting_iter</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                 <span class="n">resampling</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                 <span class="n">CI</span><span class="o">=</span><span class="mf">95.0</span><span class="p">,</span>
                 <span class="n">number_of_resamples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                 <span class="n">cv_tol</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">read_data</span><span class="p">(</span><span class="n">data_file</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_size</span> <span class="o">=</span> <span class="n">test_size</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">limit</span> <span class="o">=</span> <span class="n">limit</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">k_best</span> <span class="o">=</span> <span class="n">k_best</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">p_average</span> <span class="o">=</span>  <span class="n">p_average</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">polys</span> <span class="o">=</span>  <span class="n">polys</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_1st</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">polys</span><span class="p">)</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">starting_iter</span> <span class="o">=</span> <span class="n">starting_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resampling</span> <span class="o">=</span> <span class="n">resampling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">CI</span> <span class="o">=</span> <span class="n">CI</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">number_of_resamples</span> <span class="o">=</span> <span class="n">number_of_resamples</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cv_tol</span> <span class="o">=</span> <span class="n">cv_tol</span> 

    <span class="k">def</span><span class="w"> </span><span class="nf">read_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_file</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reads data from a file or DataFrame and initializes the X and Y attributes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data_file</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Found a DataFrame&#39;</span><span class="p">)</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">data_file</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data_file</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_file</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;data_file must be either a pandas DataFrame or a file path (str).&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Clean up the original DataFrame to save memory</span>
        <span class="k">del</span> <span class="n">df</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">transform_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transforms the data using into a unit hypercube.</span>

<span class="sd">        This method applies a transformation to the data stored in `self.X`, </span>
<span class="sd">        updates the transformed data, and retrieves the ranges and transformed </span>
<span class="sd">        data matrix.</span>

<span class="sd">        Attributes:</span>
<span class="sd">            self.X (DataFrame or ndarray): The original data to be transformed.</span>
<span class="sd">            self.ranges (list): The ranges of the transformed data.</span>
<span class="sd">            self.X_T (DataFrame or ndarray): The transformed data matrix.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">transformed_data</span> <span class="o">=</span> <span class="n">transformation</span><span class="o">.</span><span class="n">transformation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
        <span class="n">transformed_data</span><span class="o">.</span><span class="n">do_transform</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ranges</span> <span class="o">=</span> <span class="n">transformed_data</span><span class="o">.</span><span class="n">get_ranges</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_T</span> <span class="o">=</span> <span class="n">transformed_data</span><span class="o">.</span><span class="n">get_X_T</span><span class="p">()</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">legendre_expand</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform Legendre expansion on the input data.</span>
<span class="sd">        This method uses the `legendre_expand` function from the `legendre` module to</span>
<span class="sd">        expand the input data `X` and `X_T` up to the specified maximum order `max_1st`</span>
<span class="sd">        using the provided polynomial basis `polys` and target values `Y`.</span>
<span class="sd">        The expanded data is then stored in the instance variables:</span>
<span class="sd">        - `primitive_variables`: The primitive variables obtained from the expansion.</span>
<span class="sd">        - `poly_orders`: The polynomial orders used in the expansion.</span>
<span class="sd">        - `X_T_L`: The expanded data.</span>
<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">expansion_data</span> <span class="o">=</span> <span class="n">legendre</span><span class="o">.</span><span class="n">legendre_expand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">polys</span><span class="p">)</span>
        <span class="n">expansion_data</span><span class="o">.</span><span class="n">build_basis_set</span><span class="p">()</span> 

        <span class="bp">self</span><span class="o">.</span><span class="n">primitive_variables</span> <span class="o">=</span> <span class="n">expansion_data</span><span class="o">.</span><span class="n">get_primitive_variables</span><span class="p">()</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">poly_orders</span> <span class="o">=</span> <span class="n">expansion_data</span><span class="o">.</span><span class="n">get_poly_orders</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_T_L</span> <span class="o">=</span> <span class="n">expansion_data</span><span class="o">.</span><span class="n">get_expanded</span><span class="p">()</span>  


    <span class="k">def</span><span class="w"> </span><span class="nf">run_regression</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Runs the regression using the specified method and parameters.</span>

<span class="sd">        This method initializes a regression instance with the provided</span>
<span class="sd">        parameters and runs the regression to obtain the coefficients and</span>
<span class="sd">        predicted values.</span>

<span class="sd">        Attributes:</span>
<span class="sd">            X_T_L (array-like): The transformed feature matrix.</span>
<span class="sd">            Y (array-like): The target variable.</span>
<span class="sd">            method (str): The regression method to use.</span>
<span class="sd">            n_iter (int): The number of iterations for the regression algorithm.</span>
<span class="sd">            verbose (bool): If True, enables verbose output.</span>
<span class="sd">            cv_tol (float): The tolerance for cross-validation.</span>
<span class="sd">            starting_iter (int): The starting iteration for the regression algorithm.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None: The method updates the instance attributes `coef_` and `y_pred`</span>
<span class="sd">            with the regression coefficients and predicted values, respectively.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">regression_instance</span> <span class="o">=</span> <span class="n">regression</span><span class="o">.</span><span class="n">regression</span><span class="p">(</span>
            <span class="n">X_T_L</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">X_T_L</span><span class="p">,</span>
            <span class="n">Y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span>
            <span class="n">method</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="p">,</span>
            <span class="n">n_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">cv_tol</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cv_tol</span><span class="p">,</span>
            <span class="n">starting_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">starting_iter</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">regression_instance</span><span class="o">.</span><span class="n">run_regression</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">stats</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate and store evaluation statistics for the model.</span>

<span class="sd">        This method computes evaluation statistics using the actual target values (self.Y),</span>
<span class="sd">        the predicted values (self.y_pred), and the model coefficients (self.coef_). The</span>
<span class="sd">        results are stored in the instance variable `self.evs`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evs</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">plot_hdmr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Plots the High-Dimensional Model Representation (HDMR) of the model&#39;s predictions.</span>

<span class="sd">        This method uses the `plot_hdmr` function from the `stats` module to visualize the </span>
<span class="sd">        HDMR of the actual values (`self.Y`) against the predicted values (`self.y_pred`).</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">stats</span><span class="o">.</span><span class="n">plot_hdmr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="p">)</span>



    <span class="k">def</span><span class="w"> </span><span class="nf">eval_all_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate all indices and store the results.</span>
<span class="sd">        This method performs the following evaluations:</span>
<span class="sd">        1. Evaluates indices using the provided data and model coefficients.</span>
<span class="sd">        2. Retrieves Sobol indices and stores them in `self.results`.</span>
<span class="sd">        3. Retrieves non-zero coefficients and stores them in `self.non_zero_coefficients`.</span>
<span class="sd">        4. Evaluates Shapley values for the features and stores them in `self.shap`.</span>
<span class="sd">        5. Evaluates the total index for the features and stores it in `self.total`.</span>
<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">eval_indicies</span> <span class="o">=</span> <span class="n">indicies</span><span class="o">.</span><span class="n">eval_indices</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_T_L</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">evs</span><span class="p">)</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">results</span> <span class="o">=</span> <span class="n">eval_indicies</span><span class="o">.</span><span class="n">get_sobol_indicies</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">non_zero_coefficients</span> <span class="o">=</span> <span class="n">eval_indicies</span><span class="o">.</span><span class="n">get_non_zero_coefficients</span><span class="p">()</span> 

        <span class="bp">self</span><span class="o">.</span><span class="n">shap</span> <span class="o">=</span> <span class="n">eval_indicies</span><span class="o">.</span><span class="n">eval_shapley</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total</span> <span class="o">=</span> <span class="n">eval_indicies</span><span class="o">.</span><span class="n">eval_total_index</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">get_pruned_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates a pruned dataset containing only the features with non-zero coefficients.</span>

<span class="sd">        This method creates a new DataFrame that includes only the columns from the original</span>
<span class="sd">        dataset (`X_T_L`) that correspond to the labels with non-zero coefficients. Additionally,</span>
<span class="sd">        it includes the target variable (`Y`).</span>

<span class="sd">        Returns:</span>
<span class="sd">            pd.DataFrame: A DataFrame containing the pruned data with selected features and the target variable.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">pruned_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_zero_coefficients</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span> <span class="p">:</span>
            <span class="n">pruned_data</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_T_L</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>
        <span class="n">pruned_data</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span>
        <span class="k">return</span> <span class="n">pruned_data</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">run_all</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Execute a complete sequence of steps for RS-HDMR (Random Sampling High-Dimensional Model Representation) analysis.</span>

<span class="sd">    This method performs the following steps in sequence:</span>
<span class="sd">    1. Transforms the input data to a unit hypercube.</span>
<span class="sd">    2. Builds basis functions using Legendre polynomials.</span>
<span class="sd">    3. Runs regression analysis to fit the model.</span>
<span class="sd">    4. Calculates and displays RS-HDMR model performance statistics.</span>
<span class="sd">    5. Evaluates Sobol indices to quantify the contribution of each input variable to the output variance.</span>
<span class="sd">    6. Calculates Shapley effects to measure the importance of each input variable.</span>
<span class="sd">    7. Computes the total index to assess the overall impact of input variables.</span>
<span class="sd">    8. If resampling is enabled, performs bootstrap resampling to estimate confidence intervals for Sobol indices and Shapley effects.</span>
<span class="sd">    9. Prints a completion message with a randomly selected quote.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple: A tuple containing three elements:</span>
<span class="sd">            - sobol_indices (pd.DataFrame): A DataFrame containing Sobol indices for each input variable.</span>
<span class="sd">            - shapley_effects (pd.DataFrame): A DataFrame containing Shapley effects for each input variable.</span>
<span class="sd">            - total_index (pd.DataFrame): A DataFrame containing the total index for each input variable.</span>

<span class="sd">    Notes:</span>
<span class="sd">        - The method assumes that the necessary data and configurations are already set in the class instance.</span>
<span class="sd">        - If resampling is enabled (`self.resampling` is True), confidence intervals (CIs) are calculated for Sobol indices and Shapley effects.</span>
<span class="sd">        - The method uses helper functions like `transform_data`, `legendre_expand`, `run_regression`, `stats`, `plot_hdmr`, `eval_sobol_indices`, `get_shapley`, and `get_total_index` to perform specific tasks.</span>
<span class="sd">        - The completion message includes a randomly selected quote for a touch of inspiration.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; sobol_indices, shapley_effects, total_index = instance.run_all()</span>
<span class="sd">        &gt;&gt;&gt; print(sobol_indices)</span>
<span class="sd">        &gt;&gt;&gt; print(shapley_effects)</span>
<span class="sd">        &gt;&gt;&gt; print(total_index)</span>
<span class="sd">    &quot;&quot;&quot;</span>
        <span class="c1"># Define a helper function to print headings</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">print_step</span><span class="p">(</span><span class="n">step_name</span><span class="p">):</span>
            <span class="n">print_heading</span><span class="p">(</span><span class="n">step_name</span><span class="p">)</span>

        <span class="c1"># Step 1: Transform data to unit hypercube</span>
        <span class="n">print_step</span><span class="p">(</span><span class="s1">&#39;Transforming data to unit hypercube&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform_data</span><span class="p">()</span>

        <span class="c1"># Step 2: Build basis functions</span>
        <span class="n">print_step</span><span class="p">(</span><span class="s1">&#39;Building basis functions&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">legendre_expand</span><span class="p">()</span>

        <span class="c1"># Step 3: Run regression analysis</span>
        <span class="n">print_step</span><span class="p">(</span><span class="s1">&#39;Running regression analysis&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_regression</span><span class="p">()</span>

        <span class="c1"># Step 4: Calculate RS-HDMR model performance statistics</span>
        <span class="n">print_step</span><span class="p">(</span><span class="s1">&#39;RS-HDMR model performance statistics&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stats</span><span class="p">()</span> 
        <span class="nb">print</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">plot_hdmr</span><span class="p">()</span>

        <span class="c1"># Step 5: Evaluate Sobol indices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_all_indices</span><span class="p">()</span>
        <span class="n">sobol_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="s1">&#39;coeff&#39;</span><span class="p">])</span>

        <span class="c1"># Step 6: Calculate Shapley effects</span>
        <span class="n">shapley_effects</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shap</span>

        <span class="c1"># Step 7: Calculate total index </span>
        <span class="n">total_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total</span>

        <span class="c1"># Step 8: Perform resampling if enabled</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">resampling</span><span class="p">:</span>
            <span class="n">print_step</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Running bootstrap resampling </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">number_of_resamples</span><span class="si">}</span><span class="s1"> samples for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">CI</span><span class="si">}</span><span class="s1">% CI&#39;</span><span class="p">)</span> 
            <span class="n">do_resampling</span> <span class="o">=</span> <span class="n">resampling</span><span class="o">.</span><span class="n">resampling</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_pruned_data</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">number_of_resamples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
            <span class="n">do_resampling</span><span class="o">.</span><span class="n">do_resampling</span><span class="p">()</span> 
            <span class="n">sobol_indices</span> <span class="o">=</span> <span class="n">do_resampling</span><span class="o">.</span><span class="n">get_sobol_quantiles</span><span class="p">(</span><span class="n">sobol_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">CI</span><span class="p">)</span>
            <span class="c1"># Calculate quantiles for Shapley effects</span>
            <span class="n">shapley_effects</span> <span class="o">=</span> <span class="n">do_resampling</span><span class="o">.</span><span class="n">get_shap_quantiles</span><span class="p">(</span><span class="n">shapley_effects</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">CI</span><span class="p">)</span> 
            <span class="n">print_step</span><span class="p">(</span><span class="s1">&#39;Completed bootstrap resampling&#39;</span><span class="p">)</span>

        <span class="c1"># Step 9: Print completion message with a quote</span>
        <span class="n">quote</span> <span class="o">=</span> <span class="n">quotes</span><span class="o">.</span><span class="n">get_quote</span><span class="p">()</span> 
        <span class="n">message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;                  Completed all analysis</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;                 ------------------------</span><span class="se">\n\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">textwrap</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">quote</span><span class="p">,</span><span class="w"> </span><span class="mi">58</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">print_step</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">sobol_indices</span><span class="p">,</span> <span class="n">shapley_effects</span><span class="p">,</span> <span class="n">total_index</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict the output for the given input data using the surrogate model.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        X (array-like): Input data for which predictions are to be made.</span>

<span class="sd">        Returns:</span>
<span class="sd">        array-like: Predicted output for the input data.</span>

<span class="sd">        Notes:</span>
<span class="sd">        If the predictive-surrogate model does not exist, it will be created and trained using</span>
<span class="sd">        the non-zero coefficients and ranges provided during initialization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;surrogate_model&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_model</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">surrogate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">non_zero_coefficients</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ranges</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 

    <span class="k">def</span><span class="w"> </span><span class="nf">get_deltax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_unconditioned</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">delta_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>      
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate delta indices for the given number of unconditioned variables and delta samples.</span>

<span class="sd">        This method initializes a DeltaX instance using the provided data and parameters,</span>
<span class="sd">        then computes the delta indices based on the specified number of unconditioned variables</span>
<span class="sd">        and delta samples.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_unconditioned (int): The number of unconditioned variables.</span>
<span class="sd">            delta_samples (int): The number of delta samples to generate.</span>

<span class="sd">        Returns:</span>
<span class="sd">            pd.DataFrame: A DataFrame containing the computed delta indices.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">delta_instance</span> <span class="o">=</span> <span class="n">pawn</span><span class="o">.</span><span class="n">DeltaX</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ranges</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_zero_coefficients</span><span class="p">)</span>
        <span class="n">delta_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">delta_instance</span><span class="o">.</span><span class="n">get_deltax</span><span class="p">(</span><span class="n">num_unconditioned</span><span class="p">,</span> <span class="n">delta_samples</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">delta_indices</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_hx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_unconditioned</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">delta_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>      
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate delta indices for the given number of unconditioned variables and delta samples.</span>

<span class="sd">        This method initializes a DeltaX instance using the provided data and parameters,</span>
<span class="sd">        then computes the delta indices based on the specified number of unconditioned variables</span>
<span class="sd">        and delta samples.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_unconditioned (int): The number of unconditioned variables.</span>
<span class="sd">            delta_samples (int): The number of delta samples to generate.</span>

<span class="sd">        Returns:</span>
<span class="sd">            pd.DataFrame: A DataFrame containing the computed delta indices.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">delta_instance</span> <span class="o">=</span> <span class="n">pawn</span><span class="o">.</span><span class="n">hX</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ranges</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_zero_coefficients</span><span class="p">)</span>
        <span class="n">delta_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">delta_instance</span><span class="o">.</span><span class="n">get_hx</span><span class="p">(</span><span class="n">num_unconditioned</span><span class="p">,</span> <span class="n">delta_samples</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">delta_indices</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">get_pawnx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_unconditioned</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_conditioned</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_ks_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate the PAWN sensitivity indices for the RS-HDMR surrogate model.</span>

<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        num_unconditioned : int</span>
<span class="sd">            The number of unconditioned samples to be used in the PAWN analysis.</span>
<span class="sd">        num_conditioned : int</span>
<span class="sd">            The number of conditioned samples to be used in the PAWN analysis.</span>
<span class="sd">        num_ks_samples : int</span>
<span class="sd">            The number of samples to be used in the Kolmogorov-Smirnov test.</span>
<span class="sd">        alpha : float, optional</span>
<span class="sd">            The significance level for the Kolmogorov-Smirnov test (default is 0.05).</span>

<span class="sd">        Returns:</span>
<span class="sd">        --------</span>
<span class="sd">        pd.DataFrame</span>
<span class="sd">            A DataFrame containing the PAWN sensitivity indices.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">pawn_instance</span> <span class="o">=</span> <span class="n">pawn</span><span class="o">.</span><span class="n">pawnx</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ranges</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_zero_coefficients</span><span class="p">)</span>
        <span class="n">pawn_indices</span> <span class="o">=</span> <span class="n">pawn_instance</span><span class="o">.</span><span class="n">get_pawnx</span><span class="p">(</span><span class="n">num_unconditioned</span><span class="p">,</span> <span class="n">num_conditioned</span><span class="p">,</span> <span class="n">num_ks_samples</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span> 
        <span class="k">return</span> <span class="n">pawn_indices</span> 

    <span class="k">def</span><span class="w"> </span><span class="nf">get_pawn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">S</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Estimate the PAWN sensitivity indices for the features in the dataset.</span>
<span class="sd">        Parameters:</span>
<span class="sd">        -----------</span>
<span class="sd">        S : int, optional</span>
<span class="sd">            The number of slides to use for the estimation. Default is 10.</span>
<span class="sd">        Returns:</span>
<span class="sd">        --------</span>
<span class="sd">        pawn_results : dict</span>
<span class="sd">            A dictionary containing the PAWN sensitivity indices for each feature.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">num_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> 
        <span class="n">pawn_results</span> <span class="o">=</span> <span class="n">pawn</span><span class="o">.</span><span class="n">estimate_pawn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="n">S</span><span class="o">=</span><span class="n">S</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pawn_results</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="shapleyx.rshdmr.eval_all_indices" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">eval_all_indices</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Evaluate all indices and store the results.
This method performs the following evaluations:
1. Evaluates indices using the provided data and model coefficients.
2. Retrieves Sobol indices and stores them in <code>self.results</code>.
3. Retrieves non-zero coefficients and stores them in <code>self.non_zero_coefficients</code>.
4. Evaluates Shapley values for the features and stores them in <code>self.shap</code>.
5. Evaluates the total index for the features and stores it in <code>self.total</code>.
Returns:
    None</p>


            <details class="quote">
              <summary>Source code in <code>shapleyx\shapleyx.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">eval_all_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate all indices and store the results.</span>
<span class="sd">    This method performs the following evaluations:</span>
<span class="sd">    1. Evaluates indices using the provided data and model coefficients.</span>
<span class="sd">    2. Retrieves Sobol indices and stores them in `self.results`.</span>
<span class="sd">    3. Retrieves non-zero coefficients and stores them in `self.non_zero_coefficients`.</span>
<span class="sd">    4. Evaluates Shapley values for the features and stores them in `self.shap`.</span>
<span class="sd">    5. Evaluates the total index for the features and stores it in `self.total`.</span>
<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">eval_indicies</span> <span class="o">=</span> <span class="n">indicies</span><span class="o">.</span><span class="n">eval_indices</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_T_L</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">evs</span><span class="p">)</span> 
    <span class="bp">self</span><span class="o">.</span><span class="n">results</span> <span class="o">=</span> <span class="n">eval_indicies</span><span class="o">.</span><span class="n">get_sobol_indicies</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">non_zero_coefficients</span> <span class="o">=</span> <span class="n">eval_indicies</span><span class="o">.</span><span class="n">get_non_zero_coefficients</span><span class="p">()</span> 

    <span class="bp">self</span><span class="o">.</span><span class="n">shap</span> <span class="o">=</span> <span class="n">eval_indicies</span><span class="o">.</span><span class="n">eval_shapley</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">total</span> <span class="o">=</span> <span class="n">eval_indicies</span><span class="o">.</span><span class="n">eval_total_index</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="shapleyx.rshdmr.get_deltax" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_deltax</span><span class="p">(</span><span class="n">num_unconditioned</span><span class="p">,</span> <span class="n">delta_samples</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Calculate delta indices for the given number of unconditioned variables and delta samples.</p>
<p>This method initializes a DeltaX instance using the provided data and parameters,
then computes the delta indices based on the specified number of unconditioned variables
and delta samples.</p>
<p>Args:
    num_unconditioned (int): The number of unconditioned variables.
    delta_samples (int): The number of delta samples to generate.</p>
<p>Returns:
    pd.DataFrame: A DataFrame containing the computed delta indices.</p>


            <details class="quote">
              <summary>Source code in <code>shapleyx\shapleyx.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_deltax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_unconditioned</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">delta_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>      
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate delta indices for the given number of unconditioned variables and delta samples.</span>

<span class="sd">    This method initializes a DeltaX instance using the provided data and parameters,</span>
<span class="sd">    then computes the delta indices based on the specified number of unconditioned variables</span>
<span class="sd">    and delta samples.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_unconditioned (int): The number of unconditioned variables.</span>
<span class="sd">        delta_samples (int): The number of delta samples to generate.</span>

<span class="sd">    Returns:</span>
<span class="sd">        pd.DataFrame: A DataFrame containing the computed delta indices.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">delta_instance</span> <span class="o">=</span> <span class="n">pawn</span><span class="o">.</span><span class="n">DeltaX</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ranges</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_zero_coefficients</span><span class="p">)</span>
    <span class="n">delta_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">delta_instance</span><span class="o">.</span><span class="n">get_deltax</span><span class="p">(</span><span class="n">num_unconditioned</span><span class="p">,</span> <span class="n">delta_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">delta_indices</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="shapleyx.rshdmr.get_hx" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_hx</span><span class="p">(</span><span class="n">num_unconditioned</span><span class="p">,</span> <span class="n">delta_samples</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Calculate delta indices for the given number of unconditioned variables and delta samples.</p>
<p>This method initializes a DeltaX instance using the provided data and parameters,
then computes the delta indices based on the specified number of unconditioned variables
and delta samples.</p>
<p>Args:
    num_unconditioned (int): The number of unconditioned variables.
    delta_samples (int): The number of delta samples to generate.</p>
<p>Returns:
    pd.DataFrame: A DataFrame containing the computed delta indices.</p>


            <details class="quote">
              <summary>Source code in <code>shapleyx\shapleyx.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_hx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_unconditioned</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">delta_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>      
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate delta indices for the given number of unconditioned variables and delta samples.</span>

<span class="sd">    This method initializes a DeltaX instance using the provided data and parameters,</span>
<span class="sd">    then computes the delta indices based on the specified number of unconditioned variables</span>
<span class="sd">    and delta samples.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_unconditioned (int): The number of unconditioned variables.</span>
<span class="sd">        delta_samples (int): The number of delta samples to generate.</span>

<span class="sd">    Returns:</span>
<span class="sd">        pd.DataFrame: A DataFrame containing the computed delta indices.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">delta_instance</span> <span class="o">=</span> <span class="n">pawn</span><span class="o">.</span><span class="n">hX</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ranges</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_zero_coefficients</span><span class="p">)</span>
    <span class="n">delta_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">delta_instance</span><span class="o">.</span><span class="n">get_hx</span><span class="p">(</span><span class="n">num_unconditioned</span><span class="p">,</span> <span class="n">delta_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">delta_indices</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="shapleyx.rshdmr.get_pawn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_pawn</span><span class="p">(</span><span class="n">S</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Estimate the PAWN sensitivity indices for the features in the dataset.</p>


<details class="parameters:" open>
  <summary>Parameters:</summary>
  <p>S : int, optional
    The number of slides to use for the estimation. Default is 10.</p>
</details>

<details class="returns:" open>
  <summary>Returns:</summary>
  <p>pawn_results : dict
    A dictionary containing the PAWN sensitivity indices for each feature.</p>
</details>

            <details class="quote">
              <summary>Source code in <code>shapleyx\shapleyx.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_pawn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">S</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate the PAWN sensitivity indices for the features in the dataset.</span>
<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    S : int, optional</span>
<span class="sd">        The number of slides to use for the estimation. Default is 10.</span>
<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    pawn_results : dict</span>
<span class="sd">        A dictionary containing the PAWN sensitivity indices for each feature.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">num_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> 
    <span class="n">pawn_results</span> <span class="o">=</span> <span class="n">pawn</span><span class="o">.</span><span class="n">estimate_pawn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="n">S</span><span class="o">=</span><span class="n">S</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pawn_results</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="shapleyx.rshdmr.get_pawnx" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_pawnx</span><span class="p">(</span><span class="n">num_unconditioned</span><span class="p">,</span> <span class="n">num_conditioned</span><span class="p">,</span> <span class="n">num_ks_samples</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Calculate the PAWN sensitivity indices for the RS-HDMR surrogate model.</p>


<details class="parameters:" open>
  <summary>Parameters:</summary>
  <p>num_unconditioned : int
    The number of unconditioned samples to be used in the PAWN analysis.
num_conditioned : int
    The number of conditioned samples to be used in the PAWN analysis.
num_ks_samples : int
    The number of samples to be used in the Kolmogorov-Smirnov test.
alpha : float, optional
    The significance level for the Kolmogorov-Smirnov test (default is 0.05).</p>
</details>

<details class="returns:" open>
  <summary>Returns:</summary>
  <p>pd.DataFrame
    A DataFrame containing the PAWN sensitivity indices.</p>
</details>

            <details class="quote">
              <summary>Source code in <code>shapleyx\shapleyx.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_pawnx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_unconditioned</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_conditioned</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_ks_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the PAWN sensitivity indices for the RS-HDMR surrogate model.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    num_unconditioned : int</span>
<span class="sd">        The number of unconditioned samples to be used in the PAWN analysis.</span>
<span class="sd">    num_conditioned : int</span>
<span class="sd">        The number of conditioned samples to be used in the PAWN analysis.</span>
<span class="sd">    num_ks_samples : int</span>
<span class="sd">        The number of samples to be used in the Kolmogorov-Smirnov test.</span>
<span class="sd">    alpha : float, optional</span>
<span class="sd">        The significance level for the Kolmogorov-Smirnov test (default is 0.05).</span>

<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    pd.DataFrame</span>
<span class="sd">        A DataFrame containing the PAWN sensitivity indices.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pawn_instance</span> <span class="o">=</span> <span class="n">pawn</span><span class="o">.</span><span class="n">pawnx</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ranges</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_zero_coefficients</span><span class="p">)</span>
    <span class="n">pawn_indices</span> <span class="o">=</span> <span class="n">pawn_instance</span><span class="o">.</span><span class="n">get_pawnx</span><span class="p">(</span><span class="n">num_unconditioned</span><span class="p">,</span> <span class="n">num_conditioned</span><span class="p">,</span> <span class="n">num_ks_samples</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">pawn_indices</span> 
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="shapleyx.rshdmr.get_pruned_data" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_pruned_data</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Generates a pruned dataset containing only the features with non-zero coefficients.</p>
<p>This method creates a new DataFrame that includes only the columns from the original
dataset (<code>X_T_L</code>) that correspond to the labels with non-zero coefficients. Additionally,
it includes the target variable (<code>Y</code>).</p>
<p>Returns:
    pd.DataFrame: A DataFrame containing the pruned data with selected features and the target variable.</p>


            <details class="quote">
              <summary>Source code in <code>shapleyx\shapleyx.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_pruned_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates a pruned dataset containing only the features with non-zero coefficients.</span>

<span class="sd">    This method creates a new DataFrame that includes only the columns from the original</span>
<span class="sd">    dataset (`X_T_L`) that correspond to the labels with non-zero coefficients. Additionally,</span>
<span class="sd">    it includes the target variable (`Y`).</span>

<span class="sd">    Returns:</span>
<span class="sd">        pd.DataFrame: A DataFrame containing the pruned data with selected features and the target variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pruned_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_zero_coefficients</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span> <span class="p">:</span>
        <span class="n">pruned_data</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_T_L</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>
    <span class="n">pruned_data</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span>
    <span class="k">return</span> <span class="n">pruned_data</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="shapleyx.rshdmr.legendre_expand" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">legendre_expand</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Perform Legendre expansion on the input data.
This method uses the <code>legendre_expand</code> function from the <code>legendre</code> module to
expand the input data <code>X</code> and <code>X_T</code> up to the specified maximum order <code>max_1st</code>
using the provided polynomial basis <code>polys</code> and target values <code>Y</code>.
The expanded data is then stored in the instance variables:
- <code>primitive_variables</code>: The primitive variables obtained from the expansion.
- <code>poly_orders</code>: The polynomial orders used in the expansion.
- <code>X_T_L</code>: The expanded data.
Returns:
    None</p>


            <details class="quote">
              <summary>Source code in <code>shapleyx\shapleyx.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">legendre_expand</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform Legendre expansion on the input data.</span>
<span class="sd">    This method uses the `legendre_expand` function from the `legendre` module to</span>
<span class="sd">    expand the input data `X` and `X_T` up to the specified maximum order `max_1st`</span>
<span class="sd">    using the provided polynomial basis `polys` and target values `Y`.</span>
<span class="sd">    The expanded data is then stored in the instance variables:</span>
<span class="sd">    - `primitive_variables`: The primitive variables obtained from the expansion.</span>
<span class="sd">    - `poly_orders`: The polynomial orders used in the expansion.</span>
<span class="sd">    - `X_T_L`: The expanded data.</span>
<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">expansion_data</span> <span class="o">=</span> <span class="n">legendre</span><span class="o">.</span><span class="n">legendre_expand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">polys</span><span class="p">)</span>
    <span class="n">expansion_data</span><span class="o">.</span><span class="n">build_basis_set</span><span class="p">()</span> 

    <span class="bp">self</span><span class="o">.</span><span class="n">primitive_variables</span> <span class="o">=</span> <span class="n">expansion_data</span><span class="o">.</span><span class="n">get_primitive_variables</span><span class="p">()</span> 
    <span class="bp">self</span><span class="o">.</span><span class="n">poly_orders</span> <span class="o">=</span> <span class="n">expansion_data</span><span class="o">.</span><span class="n">get_poly_orders</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">X_T_L</span> <span class="o">=</span> <span class="n">expansion_data</span><span class="o">.</span><span class="n">get_expanded</span><span class="p">()</span>  
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="shapleyx.rshdmr.plot_hdmr" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">plot_hdmr</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Plots the High-Dimensional Model Representation (HDMR) of the model's predictions.</p>
<p>This method uses the <code>plot_hdmr</code> function from the <code>stats</code> module to visualize the 
HDMR of the actual values (<code>self.Y</code>) against the predicted values (<code>self.y_pred</code>).</p>
<p>Returns:
    None</p>


            <details class="quote">
              <summary>Source code in <code>shapleyx\shapleyx.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">plot_hdmr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plots the High-Dimensional Model Representation (HDMR) of the model&#39;s predictions.</span>

<span class="sd">    This method uses the `plot_hdmr` function from the `stats` module to visualize the </span>
<span class="sd">    HDMR of the actual values (`self.Y`) against the predicted values (`self.y_pred`).</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">stats</span><span class="o">.</span><span class="n">plot_hdmr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="shapleyx.rshdmr.predict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Predict the output for the given input data using the surrogate model.</p>
<p>Parameters:
X (array-like): Input data for which predictions are to be made.</p>
<p>Returns:
array-like: Predicted output for the input data.</p>
<p>Notes:
If the predictive-surrogate model does not exist, it will be created and trained using
the non-zero coefficients and ranges provided during initialization.</p>


            <details class="quote">
              <summary>Source code in <code>shapleyx\shapleyx.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Predict the output for the given input data using the surrogate model.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    X (array-like): Input data for which predictions are to be made.</span>

<span class="sd">    Returns:</span>
<span class="sd">    array-like: Predicted output for the input data.</span>

<span class="sd">    Notes:</span>
<span class="sd">    If the predictive-surrogate model does not exist, it will be created and trained using</span>
<span class="sd">    the non-zero coefficients and ranges provided during initialization.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;surrogate_model&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_model</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">surrogate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">non_zero_coefficients</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ranges</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">surrogate_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="shapleyx.rshdmr.read_data" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">read_data</span><span class="p">(</span><span class="n">data_file</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Reads data from a file or DataFrame and initializes the X and Y attributes.</p>


            <details class="quote">
              <summary>Source code in <code>shapleyx\shapleyx.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">read_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_file</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reads data from a file or DataFrame and initializes the X and Y attributes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data_file</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Found a DataFrame&#39;</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">data_file</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data_file</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_file</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;data_file must be either a pandas DataFrame or a file path (str).&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Clean up the original DataFrame to save memory</span>
    <span class="k">del</span> <span class="n">df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="shapleyx.rshdmr.run_all" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">run_all</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Execute a complete sequence of steps for RS-HDMR (Random Sampling High-Dimensional Model Representation) analysis.</p>
<p>This method performs the following steps in sequence:
1. Transforms the input data to a unit hypercube.
2. Builds basis functions using Legendre polynomials.
3. Runs regression analysis to fit the model.
4. Calculates and displays RS-HDMR model performance statistics.
5. Evaluates Sobol indices to quantify the contribution of each input variable to the output variance.
6. Calculates Shapley effects to measure the importance of each input variable.
7. Computes the total index to assess the overall impact of input variables.
8. If resampling is enabled, performs bootstrap resampling to estimate confidence intervals for Sobol indices and Shapley effects.
9. Prints a completion message with a randomly selected quote.</p>
<p>Returns:
    tuple: A tuple containing three elements:
        - sobol_indices (pd.DataFrame): A DataFrame containing Sobol indices for each input variable.
        - shapley_effects (pd.DataFrame): A DataFrame containing Shapley effects for each input variable.
        - total_index (pd.DataFrame): A DataFrame containing the total index for each input variable.</p>
<p>Notes:
    - The method assumes that the necessary data and configurations are already set in the class instance.
    - If resampling is enabled (<code>self.resampling</code> is True), confidence intervals (CIs) are calculated for Sobol indices and Shapley effects.
    - The method uses helper functions like <code>transform_data</code>, <code>legendre_expand</code>, <code>run_regression</code>, <code>stats</code>, <code>plot_hdmr</code>, <code>eval_sobol_indices</code>, <code>get_shapley</code>, and <code>get_total_index</code> to perform specific tasks.
    - The completion message includes a randomly selected quote for a touch of inspiration.</p>
<p>Example:
    &gt;&gt;&gt; sobol_indices, shapley_effects, total_index = instance.run_all()
    &gt;&gt;&gt; print(sobol_indices)
    &gt;&gt;&gt; print(shapley_effects)
    &gt;&gt;&gt; print(total_index)</p>


            <details class="quote">
              <summary>Source code in <code>shapleyx\shapleyx.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">run_all</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Execute a complete sequence of steps for RS-HDMR (Random Sampling High-Dimensional Model Representation) analysis.</span>

<span class="sd">This method performs the following steps in sequence:</span>
<span class="sd">1. Transforms the input data to a unit hypercube.</span>
<span class="sd">2. Builds basis functions using Legendre polynomials.</span>
<span class="sd">3. Runs regression analysis to fit the model.</span>
<span class="sd">4. Calculates and displays RS-HDMR model performance statistics.</span>
<span class="sd">5. Evaluates Sobol indices to quantify the contribution of each input variable to the output variance.</span>
<span class="sd">6. Calculates Shapley effects to measure the importance of each input variable.</span>
<span class="sd">7. Computes the total index to assess the overall impact of input variables.</span>
<span class="sd">8. If resampling is enabled, performs bootstrap resampling to estimate confidence intervals for Sobol indices and Shapley effects.</span>
<span class="sd">9. Prints a completion message with a randomly selected quote.</span>

<span class="sd">Returns:</span>
<span class="sd">    tuple: A tuple containing three elements:</span>
<span class="sd">        - sobol_indices (pd.DataFrame): A DataFrame containing Sobol indices for each input variable.</span>
<span class="sd">        - shapley_effects (pd.DataFrame): A DataFrame containing Shapley effects for each input variable.</span>
<span class="sd">        - total_index (pd.DataFrame): A DataFrame containing the total index for each input variable.</span>

<span class="sd">Notes:</span>
<span class="sd">    - The method assumes that the necessary data and configurations are already set in the class instance.</span>
<span class="sd">    - If resampling is enabled (`self.resampling` is True), confidence intervals (CIs) are calculated for Sobol indices and Shapley effects.</span>
<span class="sd">    - The method uses helper functions like `transform_data`, `legendre_expand`, `run_regression`, `stats`, `plot_hdmr`, `eval_sobol_indices`, `get_shapley`, and `get_total_index` to perform specific tasks.</span>
<span class="sd">    - The completion message includes a randomly selected quote for a touch of inspiration.</span>

<span class="sd">Example:</span>
<span class="sd">    &gt;&gt;&gt; sobol_indices, shapley_effects, total_index = instance.run_all()</span>
<span class="sd">    &gt;&gt;&gt; print(sobol_indices)</span>
<span class="sd">    &gt;&gt;&gt; print(shapley_effects)</span>
<span class="sd">    &gt;&gt;&gt; print(total_index)</span>
<span class="sd">&quot;&quot;&quot;</span>
    <span class="c1"># Define a helper function to print headings</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">print_step</span><span class="p">(</span><span class="n">step_name</span><span class="p">):</span>
        <span class="n">print_heading</span><span class="p">(</span><span class="n">step_name</span><span class="p">)</span>

    <span class="c1"># Step 1: Transform data to unit hypercube</span>
    <span class="n">print_step</span><span class="p">(</span><span class="s1">&#39;Transforming data to unit hypercube&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transform_data</span><span class="p">()</span>

    <span class="c1"># Step 2: Build basis functions</span>
    <span class="n">print_step</span><span class="p">(</span><span class="s1">&#39;Building basis functions&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">legendre_expand</span><span class="p">()</span>

    <span class="c1"># Step 3: Run regression analysis</span>
    <span class="n">print_step</span><span class="p">(</span><span class="s1">&#39;Running regression analysis&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">run_regression</span><span class="p">()</span>

    <span class="c1"># Step 4: Calculate RS-HDMR model performance statistics</span>
    <span class="n">print_step</span><span class="p">(</span><span class="s1">&#39;RS-HDMR model performance statistics&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stats</span><span class="p">()</span> 
    <span class="nb">print</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">plot_hdmr</span><span class="p">()</span>

    <span class="c1"># Step 5: Evaluate Sobol indices</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">eval_all_indices</span><span class="p">()</span>
    <span class="n">sobol_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="s1">&#39;coeff&#39;</span><span class="p">])</span>

    <span class="c1"># Step 6: Calculate Shapley effects</span>
    <span class="n">shapley_effects</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shap</span>

    <span class="c1"># Step 7: Calculate total index </span>
    <span class="n">total_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total</span>

    <span class="c1"># Step 8: Perform resampling if enabled</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">resampling</span><span class="p">:</span>
        <span class="n">print_step</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Running bootstrap resampling </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">number_of_resamples</span><span class="si">}</span><span class="s1"> samples for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">CI</span><span class="si">}</span><span class="s1">% CI&#39;</span><span class="p">)</span> 
        <span class="n">do_resampling</span> <span class="o">=</span> <span class="n">resampling</span><span class="o">.</span><span class="n">resampling</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_pruned_data</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">number_of_resamples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
        <span class="n">do_resampling</span><span class="o">.</span><span class="n">do_resampling</span><span class="p">()</span> 
        <span class="n">sobol_indices</span> <span class="o">=</span> <span class="n">do_resampling</span><span class="o">.</span><span class="n">get_sobol_quantiles</span><span class="p">(</span><span class="n">sobol_indices</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">CI</span><span class="p">)</span>
        <span class="c1"># Calculate quantiles for Shapley effects</span>
        <span class="n">shapley_effects</span> <span class="o">=</span> <span class="n">do_resampling</span><span class="o">.</span><span class="n">get_shap_quantiles</span><span class="p">(</span><span class="n">shapley_effects</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">CI</span><span class="p">)</span> 
        <span class="n">print_step</span><span class="p">(</span><span class="s1">&#39;Completed bootstrap resampling&#39;</span><span class="p">)</span>

    <span class="c1"># Step 9: Print completion message with a quote</span>
    <span class="n">quote</span> <span class="o">=</span> <span class="n">quotes</span><span class="o">.</span><span class="n">get_quote</span><span class="p">()</span> 
    <span class="n">message</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;                  Completed all analysis</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;                 ------------------------</span><span class="se">\n\n</span><span class="s2">&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">textwrap</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">quote</span><span class="p">,</span><span class="w"> </span><span class="mi">58</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">print_step</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">sobol_indices</span><span class="p">,</span> <span class="n">shapley_effects</span><span class="p">,</span> <span class="n">total_index</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="shapleyx.rshdmr.run_regression" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">run_regression</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Runs the regression using the specified method and parameters.</p>
<p>This method initializes a regression instance with the provided
parameters and runs the regression to obtain the coefficients and
predicted values.</p>
<p>Attributes:
    X_T_L (array-like): The transformed feature matrix.
    Y (array-like): The target variable.
    method (str): The regression method to use.
    n_iter (int): The number of iterations for the regression algorithm.
    verbose (bool): If True, enables verbose output.
    cv_tol (float): The tolerance for cross-validation.
    starting_iter (int): The starting iteration for the regression algorithm.</p>
<p>Returns:
    None: The method updates the instance attributes <code>coef_</code> and <code>y_pred</code>
    with the regression coefficients and predicted values, respectively.</p>


            <details class="quote">
              <summary>Source code in <code>shapleyx\shapleyx.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">run_regression</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Runs the regression using the specified method and parameters.</span>

<span class="sd">    This method initializes a regression instance with the provided</span>
<span class="sd">    parameters and runs the regression to obtain the coefficients and</span>
<span class="sd">    predicted values.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        X_T_L (array-like): The transformed feature matrix.</span>
<span class="sd">        Y (array-like): The target variable.</span>
<span class="sd">        method (str): The regression method to use.</span>
<span class="sd">        n_iter (int): The number of iterations for the regression algorithm.</span>
<span class="sd">        verbose (bool): If True, enables verbose output.</span>
<span class="sd">        cv_tol (float): The tolerance for cross-validation.</span>
<span class="sd">        starting_iter (int): The starting iteration for the regression algorithm.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None: The method updates the instance attributes `coef_` and `y_pred`</span>
<span class="sd">        with the regression coefficients and predicted values, respectively.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">regression_instance</span> <span class="o">=</span> <span class="n">regression</span><span class="o">.</span><span class="n">regression</span><span class="p">(</span>
        <span class="n">X_T_L</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">X_T_L</span><span class="p">,</span>
        <span class="n">Y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span>
        <span class="n">method</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="p">,</span>
        <span class="n">n_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
        <span class="n">cv_tol</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cv_tol</span><span class="p">,</span>
        <span class="n">starting_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">starting_iter</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">regression_instance</span><span class="o">.</span><span class="n">run_regression</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="shapleyx.rshdmr.stats" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">stats</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Calculate and store evaluation statistics for the model.</p>
<p>This method computes evaluation statistics using the actual target values (self.Y),
the predicted values (self.y_pred), and the model coefficients (self.coef_). The
results are stored in the instance variable <code>self.evs</code>.</p>
<p>Returns:
    None</p>


            <details class="quote">
              <summary>Source code in <code>shapleyx\shapleyx.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">stats</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate and store evaluation statistics for the model.</span>

<span class="sd">    This method computes evaluation statistics using the actual target values (self.Y),</span>
<span class="sd">    the predicted values (self.y_pred), and the model coefficients (self.coef_). The</span>
<span class="sd">    results are stored in the instance variable `self.evs`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">evs</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="shapleyx.rshdmr.transform_data" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">transform_data</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Transforms the data using into a unit hypercube.</p>
<p>This method applies a transformation to the data stored in <code>self.X</code>, 
updates the transformed data, and retrieves the ranges and transformed 
data matrix.</p>
<p>Attributes:
    self.X (DataFrame or ndarray): The original data to be transformed.
    self.ranges (list): The ranges of the transformed data.
    self.X_T (DataFrame or ndarray): The transformed data matrix.</p>
<p>Returns:
    None</p>


            <details class="quote">
              <summary>Source code in <code>shapleyx\shapleyx.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">transform_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transforms the data using into a unit hypercube.</span>

<span class="sd">    This method applies a transformation to the data stored in `self.X`, </span>
<span class="sd">    updates the transformed data, and retrieves the ranges and transformed </span>
<span class="sd">    data matrix.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        self.X (DataFrame or ndarray): The original data to be transformed.</span>
<span class="sd">        self.ranges (list): The ranges of the transformed data.</span>
<span class="sd">        self.X_T (DataFrame or ndarray): The transformed data matrix.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">transformed_data</span> <span class="o">=</span> <span class="n">transformation</span><span class="o">.</span><span class="n">transformation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
    <span class="n">transformed_data</span><span class="o">.</span><span class="n">do_transform</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ranges</span> <span class="o">=</span> <span class="n">transformed_data</span><span class="o">.</span><span class="n">get_ranges</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">X_T</span> <span class="o">=</span> <span class="n">transformed_data</span><span class="o">.</span><span class="n">get_X_T</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="shapleyx.ARD" class="doc doc-heading">
            <code>shapleyx.ARD</code>


</h3>

    <div class="doc doc-contents first">









  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h4 id="shapleyx.ARD.RegressionARD" class="doc doc-heading">
            <code>RegressionARD</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="sklearn.base.RegressorMixin">RegressorMixin</span></code>, <code><span title="sklearn.linear_model._base.LinearModel">LinearModel</span></code></p>


        <p>Regression with Automatic Relevance Determination (ARD) using Sparse Bayesian Learning.</p>
<p>This class implements a fast version of ARD regression, which is a Bayesian approach
to regression that automatically determines the relevance of each feature. It is based
on the Sparse Bayesian Learning (SBL) algorithm, which promotes sparsity in the model
by estimating the precision of the coefficients.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>n_iter</code>
            </td>
            <td>
                  <code>(<span title="int">int</span>, <span title="optional">optional</span>(default=300))</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Maximum number of iterations for the optimization algorithm.</p>
              </div>
            </td>
            <td>
                  <code>300</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tol</code>
            </td>
            <td>
                  <code>(<span title="float">float</span>, <span title="optional">optional</span>(default=0.001))</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Convergence threshold. If the absolute change in the precision parameter for the
weights is below this threshold, the algorithm terminates.</p>
              </div>
            </td>
            <td>
                  <code>0.001</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>fit_intercept</code>
            </td>
            <td>
                  <code>(<span title="bool">bool</span>, <span title="optional">optional</span>(default=True))</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to calculate the intercept for this model. If set to False, no intercept
will be used in calculations (e.g., data is expected to be already centered).</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>copy_X</code>
            </td>
            <td>
                  <code>(<span title="bool">bool</span>, <span title="optional">optional</span>(default=True))</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If True, X will be copied; else, it may be overwritten.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>verbose</code>
            </td>
            <td>
                  <code>(<span title="bool">bool</span>, <span title="optional">optional</span>(default=False))</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If True, the algorithm will print progress messages during fitting.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cv_tol</code>
            </td>
            <td>
                  <code>(<span title="float">float</span>, <span title="optional">optional</span>(default=0.1))</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Tolerance for cross-validation. If the percentage change in cross-validation score
is below this threshold, the algorithm terminates.</p>
              </div>
            </td>
            <td>
                  <code>0.1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cv</code>
            </td>
            <td>
                  <code>(<span title="bool">bool</span>, <span title="optional">optional</span>(default=False))</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If True, cross-validation will be used to determine the optimal number of features.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Attributes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="shapleyx.ARD.RegressionARD.coef_">coef_</span></code></td>
            <td>
                  <code>(<span title="array">array</span>, <span title="shape">shape</span>(<span title="n_features">n_features</span>))</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Coefficients of the regression model (mean of the posterior distribution).</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="shapleyx.ARD.RegressionARD.alpha_">alpha_</span></code></td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Estimated precision of the noise.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="shapleyx.ARD.RegressionARD.active_">active_</span></code></td>
            <td>
                  <code>array, dtype=bool, shape (n_features,)</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Boolean array indicating which features are active (non-zero coefficients).</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="shapleyx.ARD.RegressionARD.lambda_">lambda_</span></code></td>
            <td>
                  <code>(<span title="array">array</span>, <span title="shape">shape</span>(<span title="n_features">n_features</span>))</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Estimated precisions of the coefficients.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="shapleyx.ARD.RegressionARD.sigma_">sigma_</span></code></td>
            <td>
                  <code>(<span title="array">array</span>, <span title="shape">shape</span>(<span title="n_features">n_features</span>, <span title="n_features">n_features</span>))</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Estimated covariance matrix of the weights, computed only for non-zero coefficients.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="shapleyx.ARD.RegressionARD.scores_">scores_</span></code></td>
            <td>
                  <code><span title="list">list</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of cross-validation scores if <code>cv</code> is True.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="fit(X, y) (shapleyx.ARD.RegressionARD.fit)" href="#shapleyx.ARD.RegressionARD.fit">fit</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Fit the ARD regression model to the data.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="predict_dist(X) (shapleyx.ARD.RegressionARD.predict_dist)" href="#shapleyx.ARD.RegressionARD.predict_dist">predict_dist</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Compute the predictive distribution for the test set.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><span title="shapleyx.ARD.RegressionARD._center_data">_center_data</span></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Center the data by subtracting the mean.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><span title="shapleyx.ARD.RegressionARD._posterior_dist">_posterior_dist</span></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Calculate the mean and covariance matrix of the posterior distribution of coefficients.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><span title="shapleyx.ARD.RegressionARD._sparsity_quality">_sparsity_quality</span></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Calculate sparsity and quality parameters for each feature.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>


<details class="references" open>
  <summary>References</summary>
  <p>[1] Tipping, M. E., &amp; Faul, A. C. (2003). Fast marginal likelihood maximisation for
    sparse Bayesian models. In Proceedings of the Ninth International Workshop on
    Artificial Intelligence and Statistics (pp. 276-283).
[2] Tipping, M. E., &amp; Faul, A. C. (2001). Analysis of sparse Bayesian learning. In
    Advances in Neural Information Processing Systems (pp. 383-389).</p>
</details>






              <details class="quote">
                <summary>Source code in <code>shapleyx\ARD.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">RegressionARD</span><span class="p">(</span><span class="n">RegressorMixin</span><span class="p">,</span> <span class="n">LinearModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Regression with Automatic Relevance Determination (ARD) using Sparse Bayesian Learning.</span>

<span class="sd">    This class implements a fast version of ARD regression, which is a Bayesian approach</span>
<span class="sd">    to regression that automatically determines the relevance of each feature. It is based</span>
<span class="sd">    on the Sparse Bayesian Learning (SBL) algorithm, which promotes sparsity in the model</span>
<span class="sd">    by estimating the precision of the coefficients.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_iter : int, optional (default=300)</span>
<span class="sd">        Maximum number of iterations for the optimization algorithm.</span>

<span class="sd">    tol : float, optional (default=1e-3)</span>
<span class="sd">        Convergence threshold. If the absolute change in the precision parameter for the</span>
<span class="sd">        weights is below this threshold, the algorithm terminates.</span>

<span class="sd">    fit_intercept : bool, optional (default=True)</span>
<span class="sd">        Whether to calculate the intercept for this model. If set to False, no intercept</span>
<span class="sd">        will be used in calculations (e.g., data is expected to be already centered).</span>

<span class="sd">    copy_X : bool, optional (default=True)</span>
<span class="sd">        If True, X will be copied; else, it may be overwritten.</span>

<span class="sd">    verbose : bool, optional (default=False)</span>
<span class="sd">        If True, the algorithm will print progress messages during fitting.</span>

<span class="sd">    cv_tol : float, optional (default=0.1)</span>
<span class="sd">        Tolerance for cross-validation. If the percentage change in cross-validation score</span>
<span class="sd">        is below this threshold, the algorithm terminates.</span>

<span class="sd">    cv : bool, optional (default=False)</span>
<span class="sd">        If True, cross-validation will be used to determine the optimal number of features.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    coef_ : array, shape (n_features,)</span>
<span class="sd">        Coefficients of the regression model (mean of the posterior distribution).</span>

<span class="sd">    alpha_ : float</span>
<span class="sd">        Estimated precision of the noise.</span>

<span class="sd">    active_ : array, dtype=bool, shape (n_features,)</span>
<span class="sd">        Boolean array indicating which features are active (non-zero coefficients).</span>

<span class="sd">    lambda_ : array, shape (n_features,)</span>
<span class="sd">        Estimated precisions of the coefficients.</span>

<span class="sd">    sigma_ : array, shape (n_features, n_features)</span>
<span class="sd">        Estimated covariance matrix of the weights, computed only for non-zero coefficients.</span>

<span class="sd">    scores_ : list</span>
<span class="sd">        List of cross-validation scores if `cv` is True.</span>

<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    fit(X, y)</span>
<span class="sd">        Fit the ARD regression model to the data.</span>

<span class="sd">    predict_dist(X)</span>
<span class="sd">        Compute the predictive distribution for the test set.</span>

<span class="sd">    _center_data(X, y)</span>
<span class="sd">        Center the data by subtracting the mean.</span>

<span class="sd">    _posterior_dist(A, beta, XX, XY, full_covar=False)</span>
<span class="sd">        Calculate the mean and covariance matrix of the posterior distribution of coefficients.</span>

<span class="sd">    _sparsity_quality(XX, XXd, XY, XYa, Aa, Ri, active, beta, cholesky)</span>
<span class="sd">        Calculate sparsity and quality parameters for each feature.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    [1] Tipping, M. E., &amp; Faul, A. C. (2003). Fast marginal likelihood maximisation for</span>
<span class="sd">        sparse Bayesian models. In Proceedings of the Ninth International Workshop on</span>
<span class="sd">        Artificial Intelligence and Statistics (pp. 276-283).</span>
<span class="sd">    [2] Tipping, M. E., &amp; Faul, A. C. (2001). Analysis of sparse Bayesian learning. In</span>
<span class="sd">        Advances in Neural Information Processing Systems (pp. 383-389).</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span> <span class="bp">self</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">300</span><span class="p">,</span> <span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="n">fit_intercept</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> 
                  <span class="n">copy_X</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">cv_tol</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span>          <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span>             <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scores_</span>         <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span>   <span class="o">=</span> <span class="n">fit_intercept</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">copy_X</span>          <span class="o">=</span> <span class="n">copy_X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span>         <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cv</span>              <span class="o">=</span> <span class="n">cv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cv_tol</span>          <span class="o">=</span> <span class="n">cv_tol</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">_center_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39; Centers data&#39;&#39;&#39;</span>
        <span class="n">X</span>     <span class="o">=</span> <span class="n">as_float_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">copy_X</span><span class="p">)</span>
        <span class="c1"># normalisation should be done in preprocessing!</span>
        <span class="n">X_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">X_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">X</span>     <span class="o">-=</span> <span class="n">X_mean</span>
            <span class="n">y</span>      <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_mean</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">y_mean</span> <span class="o">=</span> <span class="mf">0.</span> <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">X_mean</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">X_std</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Fit the ARD regression model to the data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            Training data, matrix of explanatory variables.</span>

<span class="sd">        y : array-like, shape (n_samples,)</span>
<span class="sd">            Target values.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns the instance itself.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">y_numeric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X_mean</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">X_std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_center_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">cv_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">cv_score_history</span> <span class="o">=</span> <span class="p">[]</span> 
        <span class="n">current_r</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1">#  precompute X&#39;*Y , X&#39;*X for faster iterations &amp; allocate memory for</span>
        <span class="c1">#  sparsity &amp; quality vectors</span>
        <span class="n">XY</span>     <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="n">XX</span>     <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">X</span><span class="p">)</span>
        <span class="n">XXd</span>    <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">XX</span><span class="p">)</span>

        <span class="c1">#  initialise precision of noise &amp; and coefficients</span>
        <span class="n">var_y</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="c1"># check that variance is non zero !!!</span>
        <span class="k">if</span> <span class="n">var_y</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="mf">1e-2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="n">A</span>      <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">PINF</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span>
        <span class="n">active</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span> <span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">)</span>

        <span class="c1"># in case of almost perfect multicollinearity between some features</span>
        <span class="c1"># start from feature 0</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="n">XXd</span> <span class="o">-</span> <span class="n">X_mean</span><span class="o">**</span><span class="mi">2</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span> <span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>       <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
            <span class="n">active</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># start from a single basis vector with largest projection on targets</span>
            <span class="n">proj</span>  <span class="o">=</span> <span class="n">XY</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">XXd</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">proj</span><span class="p">)</span>
            <span class="n">active</span><span class="p">[</span><span class="n">start</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">A</span><span class="p">[</span><span class="n">start</span><span class="p">]</span>      <span class="o">=</span> <span class="n">XXd</span><span class="p">[</span><span class="n">start</span><span class="p">]</span><span class="o">/</span><span class="p">(</span> <span class="n">proj</span><span class="p">[</span><span class="n">start</span><span class="p">]</span> <span class="o">-</span> <span class="n">var_y</span><span class="p">)</span>

        <span class="n">warning_flag</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">):</span>
            <span class="n">XXa</span>     <span class="o">=</span> <span class="n">XX</span><span class="p">[</span><span class="n">active</span><span class="p">,:][:,</span><span class="n">active</span><span class="p">]</span>
            <span class="n">XYa</span>     <span class="o">=</span> <span class="n">XY</span><span class="p">[</span><span class="n">active</span><span class="p">]</span>
            <span class="n">Aa</span>      <span class="o">=</span>  <span class="n">A</span><span class="p">[</span><span class="n">active</span><span class="p">]</span>

            <span class="c1"># mean &amp; covariance of posterior distribution</span>
            <span class="n">Mn</span><span class="p">,</span><span class="n">Ri</span><span class="p">,</span><span class="n">cholesky</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_dist</span><span class="p">(</span><span class="n">Aa</span><span class="p">,</span><span class="n">beta</span><span class="p">,</span><span class="n">XXa</span><span class="p">,</span><span class="n">XYa</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">cholesky</span><span class="p">:</span>
                <span class="n">Sdiag</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Ri</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">Sdiag</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">Ri</span><span class="p">))</span> 
                <span class="n">warning_flag</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># raise warning in case cholesky failes</span>
            <span class="k">if</span> <span class="n">warning_flag</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">((</span><span class="s2">&quot;Cholesky decomposition failed ! Algorithm uses pinvh, &quot;</span>
                               <span class="s2">&quot;which is significantly slower, if you use RVR it &quot;</span>
                               <span class="s2">&quot;is advised to change parameters of kernel&quot;</span><span class="p">))</span>

            <span class="c1"># compute quality &amp; sparsity parameters            </span>
            <span class="n">s</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">S</span><span class="p">,</span><span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparsity_quality</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span><span class="n">XXd</span><span class="p">,</span><span class="n">XY</span><span class="p">,</span><span class="n">XYa</span><span class="p">,</span><span class="n">Aa</span><span class="p">,</span><span class="n">Ri</span><span class="p">,</span><span class="n">active</span><span class="p">,</span><span class="n">beta</span><span class="p">,</span><span class="n">cholesky</span><span class="p">)</span>

            <span class="c1"># update precision parameter for noise distribution</span>
            <span class="n">rss</span>     <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="p">(</span> <span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="n">active</span><span class="p">]</span> <span class="p">,</span> <span class="n">Mn</span><span class="p">)</span> <span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
            <span class="n">beta</span>    <span class="o">=</span> <span class="n">n_samples</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">active</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Aa</span> <span class="o">*</span> <span class="n">Sdiag</span> <span class="p">)</span>
            <span class="n">beta</span>   <span class="o">/=</span> <span class="p">(</span> <span class="n">rss</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span> <span class="p">)</span>

            <span class="c1"># update precision parameters of coefficients</span>
            <span class="n">A</span><span class="p">,</span><span class="n">converged</span>  <span class="o">=</span> <span class="n">update_precisions</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span><span class="n">S</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">A</span><span class="p">,</span><span class="n">active</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
                                             <span class="n">n_samples</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># ***************************************            </span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">:</span>
                <span class="c1"># Select features based on the &#39;active&#39; mask</span>
                <span class="c1"># Assumes X is a numpy array for efficient slicing</span>
                <span class="n">X_active</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">active</span><span class="p">]</span>

                <span class="c1"># Define the model for cross-validation (instantiated fresh each time)</span>
                <span class="n">cv_model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Ridge</span><span class="p">()</span>

                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># Perform 10-fold cross-validation, explicitly using R^2 scoring</span>
                    <span class="c1"># Ensure &#39;y&#39; corresponds correctly to &#39;X_active&#39;</span>
                    <span class="n">cv_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">cv_model</span><span class="p">,</span> <span class="n">X_active</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;r2&#39;</span><span class="p">)</span>
                    <span class="n">new_cv_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">)</span> <span class="c1"># Use numpy mean for clarity</span>

                    <span class="c1"># Calculate percentage change, handling division by zero</span>
                    <span class="c1"># Assumes current_cv_score is initialized (e.g., to None or 0.0) before the loop</span>
                    <span class="k">if</span> <span class="n">current_cv_score</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">current_cv_score</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">percentage_change</span> <span class="o">=</span> <span class="p">(</span><span class="n">new_cv_score</span> <span class="o">-</span> <span class="n">current_cv_score</span><span class="p">)</span> <span class="o">/</span> <span class="n">current_cv_score</span> <span class="o">*</span> <span class="mi">100</span>
                    <span class="k">elif</span> <span class="n">new_cv_score</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="p">(</span><span class="n">current_cv_score</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">current_cv_score</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
                        <span class="n">percentage_change</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="c1"># No change if both old and new scores are zero</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Handle cases where current_cv_score is None (first iteration) or zero</span>
                        <span class="n">percentage_change</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span> <span class="c1"># Indicate a large change if starting from zero/None</span>

                    <span class="c1"># Optional: Replace print with logging for better control in applications</span>
                    <span class="c1"># Assumes &#39;i&#39; is an iteration counter from an outer loop</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: CV Score = </span><span class="si">{</span><span class="n">new_cv_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, % Change = </span><span class="si">{</span><span class="n">percentage_change</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

                    <span class="c1"># Check for convergence based on the absolute percentage change</span>
                    <span class="c1"># Assumes cv_tol is a positive threshold for the magnitude of change</span>
                    <span class="c1"># Assumes &#39;converged&#39; is initialized (e.g., to False) before the loop</span>
                    <span class="k">if</span> <span class="n">current_cv_score</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">abs</span><span class="p">(</span><span class="n">percentage_change</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv_tol</span><span class="p">:</span>
                        <span class="n">converged</span> <span class="o">=</span> <span class="kc">True</span>
                        <span class="c1"># Consider adding a &#39;break&#39; here if the loop should terminate immediately upon convergence</span>

                    <span class="c1"># Update the current score and history</span>
                    <span class="c1"># Assumes cv_score_history is initialized (e.g., as []) before the loop</span>
                    <span class="n">current_cv_score</span> <span class="o">=</span> <span class="n">new_cv_score</span>
                    <span class="n">cv_score_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_cv_score</span><span class="p">)</span>

                <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">ve</span><span class="p">:</span>
                    <span class="c1"># Catch specific errors, e.g., if X_active becomes empty or has incompatible dimensions</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Cross-validation failed at iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> due to ValueError: </span><span class="si">{</span><span class="n">ve</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="c1"># Decide how to handle: stop, skip, assign default score?</span>
                    <span class="c1"># Example: Treat as no improvement or break</span>
                    <span class="n">percentage_change</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span> <span class="c1"># Mark as invalid</span>
                    <span class="c1"># converged = True # Option: Stop if CV fails</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="c1"># Catch other potential errors during cross-validation</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Cross-validation failed unexpectedly at iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="n">percentage_change</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
                    <span class="c1"># converged = True # Option: Stop if CV fails</span>


            <span class="c1"># Calculate active features once per iteration</span>
            <span class="n">num_active_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">active</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="c1"># Use logging (assuming logger is configured) and f-string for iteration progress</span>
                <span class="c1"># import logging  # Ensure logging is imported at the top of the file</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, Active Features: </span><span class="si">{</span><span class="n">num_active_features</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="c1"># Check for convergence or max iterations to terminate</span>
            <span class="k">if</span> <span class="n">converged</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Construct the final status message</span>
                <span class="n">final_status</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Finished at Iteration: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, Active Features: </span><span class="si">{</span><span class="n">num_active_features</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="k">if</span> <span class="n">converged</span><span class="p">:</span>
                    <span class="n">log_level</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">INFO</span> <span class="c1"># Normal convergence</span>
                    <span class="n">final_status</span> <span class="o">+=</span> <span class="s2">&quot; Algorithm converged.&quot;</span>
                    <span class="c1"># The original code printed &quot;Algorithm converged !&quot; only if verbose.</span>
                    <span class="c1"># Logging INFO level covers this sufficiently. Add DEBUG if more detail needed.</span>
                    <span class="c1"># if self.verbose:</span>
                    <span class="c1">#    logging.debug(&quot;Convergence details: ...&quot;)</span>
                <span class="k">else</span><span class="p">:</span> <span class="c1"># i == self.n_iter - 1</span>
                    <span class="n">log_level</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span> <span class="c1"># Reached max iterations without converging</span>
                    <span class="n">final_status</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot; Reached maximum iterations (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="si">}</span><span class="s2">).&quot;</span>

                <span class="c1"># Log the final status</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">log_level</span><span class="p">,</span> <span class="n">final_status</span><span class="p">)</span>
                <span class="k">break</span> <span class="c1"># Exit the loop</span>

        <span class="c1">#print((&#39;Iteration: {0}, number of features &#39;</span>
        <span class="c1">#               &#39;in the model: {1}&#39;).format(i,np.sum(active)))      </span>

        <span class="c1"># after last update of alpha &amp; beta update parameters</span>
        <span class="c1"># of posterior distribution</span>
        <span class="n">XXa</span><span class="p">,</span><span class="n">XYa</span><span class="p">,</span><span class="n">Aa</span>         <span class="o">=</span> <span class="n">XX</span><span class="p">[</span><span class="n">active</span><span class="p">,:][:,</span><span class="n">active</span><span class="p">],</span><span class="n">XY</span><span class="p">[</span><span class="n">active</span><span class="p">],</span><span class="n">A</span><span class="p">[</span><span class="n">active</span><span class="p">]</span>
        <span class="n">Mn</span><span class="p">,</span> <span class="n">Sn</span><span class="p">,</span> <span class="n">cholesky</span>   <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_dist</span><span class="p">(</span><span class="n">Aa</span><span class="p">,</span><span class="n">beta</span><span class="p">,</span><span class="n">XXa</span><span class="p">,</span><span class="n">XYa</span><span class="p">,</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span>         <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="n">active</span><span class="p">]</span> <span class="o">=</span> <span class="n">Mn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma_</span>        <span class="o">=</span> <span class="n">Sn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">active_</span>       <span class="o">=</span> <span class="n">active</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span>       <span class="o">=</span> <span class="n">A</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_</span>        <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_intercept</span><span class="p">(</span><span class="n">X_mean</span><span class="p">,</span><span class="n">y_mean</span><span class="p">,</span><span class="n">X_std</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv</span> <span class="p">:</span>
        <span class="c1">#    print(max(enumerate(cv_list), key=lambda x: x[1]))</span>
            <span class="nb">print</span><span class="p">((</span><span class="s1">&#39;Iteration: </span><span class="si">{0}</span><span class="s1">, number of features &#39;</span>
                       <span class="s1">&#39;in the model: </span><span class="si">{1}</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">active</span><span class="p">)))</span> 
        <span class="k">return</span> <span class="bp">self</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">_posterior_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">A</span><span class="p">,</span><span class="n">beta</span><span class="p">,</span><span class="n">XX</span><span class="p">,</span><span class="n">XY</span><span class="p">,</span><span class="n">full_covar</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Calculate the mean and covariance matrix of the posterior distribution of coefficients.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        A : array, shape (n_features,)</span>
<span class="sd">            Precision parameters for the coefficients.</span>

<span class="sd">        beta : float</span>
<span class="sd">            Precision of the noise.</span>

<span class="sd">        XX : array, shape (n_features, n_features)</span>
<span class="sd">            X&#39; * X matrix.</span>

<span class="sd">        XY : array, shape (n_features,)</span>
<span class="sd">            X&#39; * y vector.</span>

<span class="sd">        full_covar : bool, optional (default=False)</span>
<span class="sd">            If True, return the full covariance matrix; otherwise, return the inverse of the</span>
<span class="sd">            lower triangular matrix from the Cholesky decomposition.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Mn : array, shape (n_features,)</span>
<span class="sd">            Mean of the posterior distribution.</span>

<span class="sd">        Sn : array, shape (n_features, n_features)</span>
<span class="sd">            Covariance matrix of the posterior distribution.</span>

<span class="sd">        cholesky : bool</span>
<span class="sd">            Whether the Cholesky decomposition was successful.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="c1"># compute precision matrix for active features</span>
        <span class="n">Sinv</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">XX</span>
        <span class="n">np</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">Sinv</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">Sinv</span><span class="p">)</span> <span class="o">+</span> <span class="n">A</span><span class="p">)</span>
        <span class="n">cholesky</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># try cholesky, if it fails go back to pinvh</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># find posterior mean : R*R.T*mean = beta*X.T*Y</span>
            <span class="c1"># solve(R*z = beta*X.T*Y) =&gt; find z =&gt; solve(R.T*mean = z) =&gt; find mean</span>
            <span class="n">R</span>    <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">Sinv</span><span class="p">)</span>
            <span class="n">Z</span>    <span class="o">=</span> <span class="n">solve_triangular</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="n">beta</span><span class="o">*</span><span class="n">XY</span><span class="p">,</span> <span class="n">check_finite</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lower</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
            <span class="n">Mn</span>   <span class="o">=</span> <span class="n">solve_triangular</span><span class="p">(</span><span class="n">R</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">Z</span><span class="p">,</span> <span class="n">check_finite</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lower</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

            <span class="c1"># invert lower triangular matrix from cholesky decomposition</span>
            <span class="n">Ri</span>   <span class="o">=</span> <span class="n">solve_triangular</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">check_finite</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">full_covar</span><span class="p">:</span>
                <span class="n">Sn</span>   <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Ri</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">Ri</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">Mn</span><span class="p">,</span><span class="n">Sn</span><span class="p">,</span><span class="n">cholesky</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Mn</span><span class="p">,</span><span class="n">Ri</span><span class="p">,</span><span class="n">cholesky</span>
        <span class="k">except</span> <span class="n">LinAlgError</span><span class="p">:</span>
            <span class="n">cholesky</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">Sn</span>   <span class="o">=</span> <span class="n">pinvh</span><span class="p">(</span><span class="n">Sinv</span><span class="p">)</span>
            <span class="n">Mn</span>   <span class="o">=</span> <span class="n">beta</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Sinv</span><span class="p">,</span><span class="n">XY</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Mn</span><span class="p">,</span> <span class="n">Sn</span><span class="p">,</span> <span class="n">cholesky</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">_sparsity_quality</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">XX</span><span class="p">,</span><span class="n">XXd</span><span class="p">,</span><span class="n">XY</span><span class="p">,</span><span class="n">XYa</span><span class="p">,</span><span class="n">Aa</span><span class="p">,</span><span class="n">Ri</span><span class="p">,</span><span class="n">active</span><span class="p">,</span><span class="n">beta</span><span class="p">,</span><span class="n">cholesky</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Calculate sparsity and quality parameters for each feature.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        XX : array, shape (n_features, n_features)</span>
<span class="sd">            X&#39; * X matrix.</span>

<span class="sd">        XXd : array, shape (n_features,)</span>
<span class="sd">            Diagonal of X&#39; * X matrix.</span>

<span class="sd">        XY : array, shape (n_features,)</span>
<span class="sd">            X&#39; * y vector.</span>

<span class="sd">        XYa : array, shape (n_active_features,)</span>
<span class="sd">            X&#39; * y vector for active features.</span>

<span class="sd">        Aa : array, shape (n_active_features,)</span>
<span class="sd">            Precision parameters for active features.</span>

<span class="sd">        Ri : array, shape (n_active_features, n_active_features)</span>
<span class="sd">            Inverse of the lower triangular matrix from the Cholesky decomposition or the</span>
<span class="sd">            covariance matrix.</span>

<span class="sd">        active : array, dtype=bool, shape (n_features,)</span>
<span class="sd">            Boolean array indicating which features are active.</span>

<span class="sd">        beta : float</span>
<span class="sd">            Precision of the noise.</span>

<span class="sd">        cholesky : bool</span>
<span class="sd">            Whether the Cholesky decomposition was successful.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        si : array, shape (n_features,)</span>
<span class="sd">            Sparsity parameters.</span>

<span class="sd">        qi : array, shape (n_features,)</span>
<span class="sd">            Quality parameters.</span>

<span class="sd">        S : array, shape (n_features,)</span>
<span class="sd">            Intermediate sparsity parameters.</span>

<span class="sd">        Q : array, shape (n_features,)</span>
<span class="sd">            Intermediate quality parameters.</span>

<span class="sd">        Theoretical Note:</span>
<span class="sd">        -----------------</span>
<span class="sd">        Here we used Woodbury Identity for inverting covariance matrix</span>
<span class="sd">        of target distribution </span>
<span class="sd">        C    = 1/beta + 1/alpha * X&#39; * X</span>
<span class="sd">        C^-1 = beta - beta^2 * X * Sn * X&#39;</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="n">bxy</span>        <span class="o">=</span> <span class="n">beta</span><span class="o">*</span><span class="n">XY</span>
        <span class="n">bxx</span>        <span class="o">=</span> <span class="n">beta</span><span class="o">*</span><span class="n">XXd</span>
        <span class="k">if</span> <span class="n">cholesky</span><span class="p">:</span>
            <span class="c1"># here Ri is inverse of lower triangular matrix obtained from cholesky decomp</span>
            <span class="n">xxr</span>    <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XX</span><span class="p">[:,</span><span class="n">active</span><span class="p">],</span><span class="n">Ri</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
            <span class="n">rxy</span>    <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Ri</span><span class="p">,</span><span class="n">XYa</span><span class="p">)</span>
            <span class="n">S</span>      <span class="o">=</span> <span class="n">bxx</span> <span class="o">-</span> <span class="n">beta</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="n">xxr</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">Q</span>      <span class="o">=</span> <span class="n">bxy</span> <span class="o">-</span> <span class="n">beta</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span> <span class="n">xxr</span><span class="p">,</span> <span class="n">rxy</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># here Ri is covariance matrix</span>
            <span class="n">XXa</span>    <span class="o">=</span> <span class="n">XX</span><span class="p">[:,</span><span class="n">active</span><span class="p">]</span>
            <span class="n">XS</span>     <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XXa</span><span class="p">,</span><span class="n">Ri</span><span class="p">)</span>
            <span class="n">S</span>      <span class="o">=</span> <span class="n">bxx</span> <span class="o">-</span> <span class="n">beta</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">XS</span><span class="o">*</span><span class="n">XXa</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">Q</span>      <span class="o">=</span> <span class="n">bxy</span> <span class="o">-</span> <span class="n">beta</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XS</span><span class="p">,</span><span class="n">XYa</span><span class="p">)</span>
        <span class="c1"># Use following:</span>
        <span class="c1"># (EQ 1) q = A*Q/(A - S) ; s = A*S/(A-S), so if A = np.PINF q = Q, s = S</span>
        <span class="n">qi</span>         <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
        <span class="n">si</span>         <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">S</span><span class="p">)</span> 
        <span class="c1">#  If A is not np.PINF, then it should be &#39;active&#39; feature =&gt; use (EQ 1)</span>
        <span class="n">Qa</span><span class="p">,</span><span class="n">Sa</span>      <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">active</span><span class="p">],</span> <span class="n">S</span><span class="p">[</span><span class="n">active</span><span class="p">]</span>
        <span class="n">qi</span><span class="p">[</span><span class="n">active</span><span class="p">]</span> <span class="o">=</span> <span class="n">Aa</span> <span class="o">*</span> <span class="n">Qa</span> <span class="o">/</span> <span class="p">(</span><span class="n">Aa</span> <span class="o">-</span> <span class="n">Sa</span> <span class="p">)</span>
        <span class="n">si</span><span class="p">[</span><span class="n">active</span><span class="p">]</span> <span class="o">=</span> <span class="n">Aa</span> <span class="o">*</span> <span class="n">Sa</span> <span class="o">/</span> <span class="p">(</span><span class="n">Aa</span> <span class="o">-</span> <span class="n">Sa</span> <span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">si</span><span class="p">,</span><span class="n">qi</span><span class="p">,</span><span class="n">S</span><span class="p">,</span><span class="n">Q</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Computes predictive distribution for test set.</span>
<span class="sd">        Predictive distribution for each data point is one dimensional</span>
<span class="sd">        Gaussian and therefore is characterised by mean and variance.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples_test, n_features)</span>
<span class="sd">            Test data, matrix of explanatory variables.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y_hat : array, shape (n_samples_test,)</span>
<span class="sd">            Estimated values of targets on the test set (mean of the predictive distribution).</span>

<span class="sd">        var_hat : array, shape (n_samples_test,)</span>
<span class="sd">            Variance of the predictive distribution.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">y_hat</span>     <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">var_hat</span>   <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha_</span>
        <span class="n">var_hat</span>  <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="bp">self</span><span class="o">.</span><span class="n">active_</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma_</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span><span class="bp">self</span><span class="o">.</span><span class="n">active_</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_hat</span><span class="p">,</span> <span class="n">var_hat</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h5 id="shapleyx.ARD.RegressionARD.fit" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Fit the ARD regression model to the data.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>X</code>
            </td>
            <td>
                  <code>array-like, sparse matrix</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Training data, matrix of explanatory variables.</p>
              </div>
            </td>
            <td>
                  <code>array-like</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>y</code>
            </td>
            <td>
                  <code>(<span title="array">array</span> - <span title="like">like</span>, <span title="shape">shape</span>(<span title="n_samples">n_samples</span>))</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Target values.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>self</code></td>            <td>
                  <code><span title="object">object</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Returns the instance itself.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>shapleyx\ARD.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span></pre></div></td><td class="code"><div><pre><span></span><code>    <span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Fit the ARD regression model to the data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            Training data, matrix of explanatory variables.</span>

<span class="sd">        y : array-like, shape (n_samples,)</span>
<span class="sd">            Target values.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns the instance itself.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">y_numeric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X_mean</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">X_std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_center_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">cv_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">cv_score_history</span> <span class="o">=</span> <span class="p">[]</span> 
        <span class="n">current_r</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1">#  precompute X&#39;*Y , X&#39;*X for faster iterations &amp; allocate memory for</span>
        <span class="c1">#  sparsity &amp; quality vectors</span>
        <span class="n">XY</span>     <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="n">XX</span>     <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">X</span><span class="p">)</span>
        <span class="n">XXd</span>    <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">XX</span><span class="p">)</span>

        <span class="c1">#  initialise precision of noise &amp; and coefficients</span>
        <span class="n">var_y</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="c1"># check that variance is non zero !!!</span>
        <span class="k">if</span> <span class="n">var_y</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="mf">1e-2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="n">A</span>      <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">PINF</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span>
        <span class="n">active</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span> <span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">)</span>

        <span class="c1"># in case of almost perfect multicollinearity between some features</span>
        <span class="c1"># start from feature 0</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="n">XXd</span> <span class="o">-</span> <span class="n">X_mean</span><span class="o">**</span><span class="mi">2</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span> <span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>       <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
            <span class="n">active</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># start from a single basis vector with largest projection on targets</span>
            <span class="n">proj</span>  <span class="o">=</span> <span class="n">XY</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">XXd</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">proj</span><span class="p">)</span>
            <span class="n">active</span><span class="p">[</span><span class="n">start</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">A</span><span class="p">[</span><span class="n">start</span><span class="p">]</span>      <span class="o">=</span> <span class="n">XXd</span><span class="p">[</span><span class="n">start</span><span class="p">]</span><span class="o">/</span><span class="p">(</span> <span class="n">proj</span><span class="p">[</span><span class="n">start</span><span class="p">]</span> <span class="o">-</span> <span class="n">var_y</span><span class="p">)</span>

        <span class="n">warning_flag</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">):</span>
            <span class="n">XXa</span>     <span class="o">=</span> <span class="n">XX</span><span class="p">[</span><span class="n">active</span><span class="p">,:][:,</span><span class="n">active</span><span class="p">]</span>
            <span class="n">XYa</span>     <span class="o">=</span> <span class="n">XY</span><span class="p">[</span><span class="n">active</span><span class="p">]</span>
            <span class="n">Aa</span>      <span class="o">=</span>  <span class="n">A</span><span class="p">[</span><span class="n">active</span><span class="p">]</span>

            <span class="c1"># mean &amp; covariance of posterior distribution</span>
            <span class="n">Mn</span><span class="p">,</span><span class="n">Ri</span><span class="p">,</span><span class="n">cholesky</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_dist</span><span class="p">(</span><span class="n">Aa</span><span class="p">,</span><span class="n">beta</span><span class="p">,</span><span class="n">XXa</span><span class="p">,</span><span class="n">XYa</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">cholesky</span><span class="p">:</span>
                <span class="n">Sdiag</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Ri</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">Sdiag</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">Ri</span><span class="p">))</span> 
                <span class="n">warning_flag</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># raise warning in case cholesky failes</span>
            <span class="k">if</span> <span class="n">warning_flag</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">((</span><span class="s2">&quot;Cholesky decomposition failed ! Algorithm uses pinvh, &quot;</span>
                               <span class="s2">&quot;which is significantly slower, if you use RVR it &quot;</span>
                               <span class="s2">&quot;is advised to change parameters of kernel&quot;</span><span class="p">))</span>

            <span class="c1"># compute quality &amp; sparsity parameters            </span>
            <span class="n">s</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">S</span><span class="p">,</span><span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sparsity_quality</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span><span class="n">XXd</span><span class="p">,</span><span class="n">XY</span><span class="p">,</span><span class="n">XYa</span><span class="p">,</span><span class="n">Aa</span><span class="p">,</span><span class="n">Ri</span><span class="p">,</span><span class="n">active</span><span class="p">,</span><span class="n">beta</span><span class="p">,</span><span class="n">cholesky</span><span class="p">)</span>

            <span class="c1"># update precision parameter for noise distribution</span>
            <span class="n">rss</span>     <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="p">(</span> <span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="n">active</span><span class="p">]</span> <span class="p">,</span> <span class="n">Mn</span><span class="p">)</span> <span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
            <span class="n">beta</span>    <span class="o">=</span> <span class="n">n_samples</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">active</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Aa</span> <span class="o">*</span> <span class="n">Sdiag</span> <span class="p">)</span>
            <span class="n">beta</span>   <span class="o">/=</span> <span class="p">(</span> <span class="n">rss</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span> <span class="p">)</span>

            <span class="c1"># update precision parameters of coefficients</span>
            <span class="n">A</span><span class="p">,</span><span class="n">converged</span>  <span class="o">=</span> <span class="n">update_precisions</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span><span class="n">S</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">A</span><span class="p">,</span><span class="n">active</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
                                             <span class="n">n_samples</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># ***************************************            </span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">:</span>
                <span class="c1"># Select features based on the &#39;active&#39; mask</span>
                <span class="c1"># Assumes X is a numpy array for efficient slicing</span>
                <span class="n">X_active</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">active</span><span class="p">]</span>

                <span class="c1"># Define the model for cross-validation (instantiated fresh each time)</span>
                <span class="n">cv_model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Ridge</span><span class="p">()</span>

                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># Perform 10-fold cross-validation, explicitly using R^2 scoring</span>
                    <span class="c1"># Ensure &#39;y&#39; corresponds correctly to &#39;X_active&#39;</span>
                    <span class="n">cv_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">cv_model</span><span class="p">,</span> <span class="n">X_active</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;r2&#39;</span><span class="p">)</span>
                    <span class="n">new_cv_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">)</span> <span class="c1"># Use numpy mean for clarity</span>

                    <span class="c1"># Calculate percentage change, handling division by zero</span>
                    <span class="c1"># Assumes current_cv_score is initialized (e.g., to None or 0.0) before the loop</span>
                    <span class="k">if</span> <span class="n">current_cv_score</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">current_cv_score</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">percentage_change</span> <span class="o">=</span> <span class="p">(</span><span class="n">new_cv_score</span> <span class="o">-</span> <span class="n">current_cv_score</span><span class="p">)</span> <span class="o">/</span> <span class="n">current_cv_score</span> <span class="o">*</span> <span class="mi">100</span>
                    <span class="k">elif</span> <span class="n">new_cv_score</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="p">(</span><span class="n">current_cv_score</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">current_cv_score</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
                        <span class="n">percentage_change</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="c1"># No change if both old and new scores are zero</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Handle cases where current_cv_score is None (first iteration) or zero</span>
                        <span class="n">percentage_change</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span> <span class="c1"># Indicate a large change if starting from zero/None</span>

                    <span class="c1"># Optional: Replace print with logging for better control in applications</span>
                    <span class="c1"># Assumes &#39;i&#39; is an iteration counter from an outer loop</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: CV Score = </span><span class="si">{</span><span class="n">new_cv_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, % Change = </span><span class="si">{</span><span class="n">percentage_change</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

                    <span class="c1"># Check for convergence based on the absolute percentage change</span>
                    <span class="c1"># Assumes cv_tol is a positive threshold for the magnitude of change</span>
                    <span class="c1"># Assumes &#39;converged&#39; is initialized (e.g., to False) before the loop</span>
                    <span class="k">if</span> <span class="n">current_cv_score</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">abs</span><span class="p">(</span><span class="n">percentage_change</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv_tol</span><span class="p">:</span>
                        <span class="n">converged</span> <span class="o">=</span> <span class="kc">True</span>
                        <span class="c1"># Consider adding a &#39;break&#39; here if the loop should terminate immediately upon convergence</span>

                    <span class="c1"># Update the current score and history</span>
                    <span class="c1"># Assumes cv_score_history is initialized (e.g., as []) before the loop</span>
                    <span class="n">current_cv_score</span> <span class="o">=</span> <span class="n">new_cv_score</span>
                    <span class="n">cv_score_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_cv_score</span><span class="p">)</span>

                <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">ve</span><span class="p">:</span>
                    <span class="c1"># Catch specific errors, e.g., if X_active becomes empty or has incompatible dimensions</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Cross-validation failed at iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> due to ValueError: </span><span class="si">{</span><span class="n">ve</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="c1"># Decide how to handle: stop, skip, assign default score?</span>
                    <span class="c1"># Example: Treat as no improvement or break</span>
                    <span class="n">percentage_change</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span> <span class="c1"># Mark as invalid</span>
                    <span class="c1"># converged = True # Option: Stop if CV fails</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="c1"># Catch other potential errors during cross-validation</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Cross-validation failed unexpectedly at iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="n">percentage_change</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
                    <span class="c1"># converged = True # Option: Stop if CV fails</span>


            <span class="c1"># Calculate active features once per iteration</span>
            <span class="n">num_active_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">active</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="c1"># Use logging (assuming logger is configured) and f-string for iteration progress</span>
                <span class="c1"># import logging  # Ensure logging is imported at the top of the file</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, Active Features: </span><span class="si">{</span><span class="n">num_active_features</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="c1"># Check for convergence or max iterations to terminate</span>
            <span class="k">if</span> <span class="n">converged</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Construct the final status message</span>
                <span class="n">final_status</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Finished at Iteration: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, Active Features: </span><span class="si">{</span><span class="n">num_active_features</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="k">if</span> <span class="n">converged</span><span class="p">:</span>
                    <span class="n">log_level</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">INFO</span> <span class="c1"># Normal convergence</span>
                    <span class="n">final_status</span> <span class="o">+=</span> <span class="s2">&quot; Algorithm converged.&quot;</span>
                    <span class="c1"># The original code printed &quot;Algorithm converged !&quot; only if verbose.</span>
                    <span class="c1"># Logging INFO level covers this sufficiently. Add DEBUG if more detail needed.</span>
                    <span class="c1"># if self.verbose:</span>
                    <span class="c1">#    logging.debug(&quot;Convergence details: ...&quot;)</span>
                <span class="k">else</span><span class="p">:</span> <span class="c1"># i == self.n_iter - 1</span>
                    <span class="n">log_level</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span> <span class="c1"># Reached max iterations without converging</span>
                    <span class="n">final_status</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot; Reached maximum iterations (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="si">}</span><span class="s2">).&quot;</span>

                <span class="c1"># Log the final status</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">log_level</span><span class="p">,</span> <span class="n">final_status</span><span class="p">)</span>
                <span class="k">break</span> <span class="c1"># Exit the loop</span>

        <span class="c1">#print((&#39;Iteration: {0}, number of features &#39;</span>
        <span class="c1">#               &#39;in the model: {1}&#39;).format(i,np.sum(active)))      </span>

        <span class="c1"># after last update of alpha &amp; beta update parameters</span>
        <span class="c1"># of posterior distribution</span>
        <span class="n">XXa</span><span class="p">,</span><span class="n">XYa</span><span class="p">,</span><span class="n">Aa</span>         <span class="o">=</span> <span class="n">XX</span><span class="p">[</span><span class="n">active</span><span class="p">,:][:,</span><span class="n">active</span><span class="p">],</span><span class="n">XY</span><span class="p">[</span><span class="n">active</span><span class="p">],</span><span class="n">A</span><span class="p">[</span><span class="n">active</span><span class="p">]</span>
        <span class="n">Mn</span><span class="p">,</span> <span class="n">Sn</span><span class="p">,</span> <span class="n">cholesky</span>   <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_posterior_dist</span><span class="p">(</span><span class="n">Aa</span><span class="p">,</span><span class="n">beta</span><span class="p">,</span><span class="n">XXa</span><span class="p">,</span><span class="n">XYa</span><span class="p">,</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span>         <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="n">active</span><span class="p">]</span> <span class="o">=</span> <span class="n">Mn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma_</span>        <span class="o">=</span> <span class="n">Sn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">active_</span>       <span class="o">=</span> <span class="n">active</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span>       <span class="o">=</span> <span class="n">A</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_</span>        <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_intercept</span><span class="p">(</span><span class="n">X_mean</span><span class="p">,</span><span class="n">y_mean</span><span class="p">,</span><span class="n">X_std</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv</span> <span class="p">:</span>
        <span class="c1">#    print(max(enumerate(cv_list), key=lambda x: x[1]))</span>
            <span class="nb">print</span><span class="p">((</span><span class="s1">&#39;Iteration: </span><span class="si">{0}</span><span class="s1">, number of features &#39;</span>
                       <span class="s1">&#39;in the model: </span><span class="si">{1}</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">active</span><span class="p">)))</span> 
        <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="shapleyx.ARD.RegressionARD.predict_dist" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict_dist</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Computes predictive distribution for test set.
Predictive distribution for each data point is one dimensional
Gaussian and therefore is characterised by mean and variance.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>X</code>
            </td>
            <td>
                  <code>array-like, sparse matrix</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Test data, matrix of explanatory variables.</p>
              </div>
            </td>
            <td>
                  <code>array-like</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>y_hat</code></td>            <td>
                  <code>(<span title="array">array</span>, <span title="shape">shape</span>(<span title="n_samples_test">n_samples_test</span>))</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Estimated values of targets on the test set (mean of the predictive distribution).</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
<td><code>var_hat</code></td>            <td>
                  <code>(<span title="array">array</span>, <span title="shape">shape</span>(<span title="n_samples_test">n_samples_test</span>))</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Variance of the predictive distribution.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>shapleyx\ARD.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">predict_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Computes predictive distribution for test set.</span>
<span class="sd">    Predictive distribution for each data point is one dimensional</span>
<span class="sd">    Gaussian and therefore is characterised by mean and variance.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix}, shape (n_samples_test, n_features)</span>
<span class="sd">        Test data, matrix of explanatory variables.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    y_hat : array, shape (n_samples_test,)</span>
<span class="sd">        Estimated values of targets on the test set (mean of the predictive distribution).</span>

<span class="sd">    var_hat : array, shape (n_samples_test,)</span>
<span class="sd">        Variance of the predictive distribution.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">y_hat</span>     <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">var_hat</span>   <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha_</span>
    <span class="n">var_hat</span>  <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="bp">self</span><span class="o">.</span><span class="n">active_</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma_</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span><span class="bp">self</span><span class="o">.</span><span class="n">active_</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y_hat</span><span class="p">,</span> <span class="n">var_hat</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h4 id="shapleyx.ARD.update_precisions" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">update_precisions</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">active</span><span class="p">,</span> <span class="n">tol</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">clf_bias</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Updates the precision parameters (alpha) for features in a sparse Bayesian learning model
by selecting a feature to add, recompute, or delete based on its impact on the log marginal
likelihood. The function also checks for convergence.</p>


<details class="parameters:" open>
  <summary>Parameters:</summary>
  <p>Q : numpy.ndarray
    Quality parameters for all features.
S : numpy.ndarray
    Sparsity parameters for all features.
q : numpy.ndarray
    Quality parameters for features currently in the model.
s : numpy.ndarray
    Sparsity parameters for features currently in the model.
A : numpy.ndarray
    Precision parameters (alpha) for all features.
active : numpy.ndarray (bool)
    Boolean array indicating whether each feature is currently in the model.
tol : float
    Tolerance threshold for determining convergence based on changes in precision.
n_samples : int
    Number of samples in the dataset, used to normalize the change in log marginal likelihood.
clf_bias : bool
    Flag indicating whether the model includes a bias term (used in classification tasks).</p>
</details>

<details class="returns:" open>
  <summary>Returns:</summary>
  <p>list
    A list containing two elements:
    - Updated precision parameters (A) for all features.
    - A boolean flag indicating whether the model has converged.</p>
</details>

<details class="notes:" open>
  <summary>Notes:</summary>
  <p>The function performs the following steps:
1. Computes the change in log marginal likelihood for adding, recomputing, or deleting features.
2. Identifies the feature that causes the largest change in likelihood.
3. Updates the precision parameter (alpha) for the selected feature.
4. Checks for convergence based on whether no features are added/deleted and changes in precision
   are below the specified tolerance.
5. Returns the updated precision parameters and convergence status.</p>
<p>Convergence is determined by two conditions:
- No features are added or deleted.
- The change in precision for features already in the model is below the tolerance threshold.</p>
<p>The function ensures that the bias term is not removed in classification tasks.</p>
</details>

            <details class="quote">
              <summary>Source code in <code>shapleyx\ARD.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">update_precisions</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span><span class="n">S</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">A</span><span class="p">,</span><span class="n">active</span><span class="p">,</span><span class="n">tol</span><span class="p">,</span><span class="n">n_samples</span><span class="p">,</span><span class="n">clf_bias</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Updates the precision parameters (alpha) for features in a sparse Bayesian learning model</span>
<span class="sd">    by selecting a feature to add, recompute, or delete based on its impact on the log marginal</span>
<span class="sd">    likelihood. The function also checks for convergence.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    Q : numpy.ndarray</span>
<span class="sd">        Quality parameters for all features.</span>
<span class="sd">    S : numpy.ndarray</span>
<span class="sd">        Sparsity parameters for all features.</span>
<span class="sd">    q : numpy.ndarray</span>
<span class="sd">        Quality parameters for features currently in the model.</span>
<span class="sd">    s : numpy.ndarray</span>
<span class="sd">        Sparsity parameters for features currently in the model.</span>
<span class="sd">    A : numpy.ndarray</span>
<span class="sd">        Precision parameters (alpha) for all features.</span>
<span class="sd">    active : numpy.ndarray (bool)</span>
<span class="sd">        Boolean array indicating whether each feature is currently in the model.</span>
<span class="sd">    tol : float</span>
<span class="sd">        Tolerance threshold for determining convergence based on changes in precision.</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples in the dataset, used to normalize the change in log marginal likelihood.</span>
<span class="sd">    clf_bias : bool</span>
<span class="sd">        Flag indicating whether the model includes a bias term (used in classification tasks).</span>

<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    list</span>
<span class="sd">        A list containing two elements:</span>
<span class="sd">        - Updated precision parameters (A) for all features.</span>
<span class="sd">        - A boolean flag indicating whether the model has converged.</span>

<span class="sd">    Notes:</span>
<span class="sd">    ------</span>
<span class="sd">    The function performs the following steps:</span>
<span class="sd">    1. Computes the change in log marginal likelihood for adding, recomputing, or deleting features.</span>
<span class="sd">    2. Identifies the feature that causes the largest change in likelihood.</span>
<span class="sd">    3. Updates the precision parameter (alpha) for the selected feature.</span>
<span class="sd">    4. Checks for convergence based on whether no features are added/deleted and changes in precision</span>
<span class="sd">       are below the specified tolerance.</span>
<span class="sd">    5. Returns the updated precision parameters and convergence status.</span>

<span class="sd">    Convergence is determined by two conditions:</span>
<span class="sd">    - No features are added or deleted.</span>
<span class="sd">    - The change in precision for features already in the model is below the tolerance threshold.</span>

<span class="sd">    The function ensures that the bias term is not removed in classification tasks.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># initialise vector holding changes in log marginal likelihood</span>
    <span class="n">deltaL</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># identify features that can be added , recomputed and deleted in model</span>
    <span class="n">theta</span>        <span class="o">=</span>  <span class="n">q</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">s</span> 
    <span class="n">add</span>          <span class="o">=</span>  <span class="p">(</span><span class="n">theta</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">active</span> <span class="o">==</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">recompute</span>    <span class="o">=</span>  <span class="p">(</span><span class="n">theta</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">active</span> <span class="o">==</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">delete</span>       <span class="o">=</span> <span class="o">~</span><span class="p">(</span><span class="n">add</span> <span class="o">+</span> <span class="n">recompute</span><span class="p">)</span>

    <span class="c1"># compute sparsity &amp; quality parameters corresponding to features in </span>
    <span class="c1"># three groups identified above</span>
    <span class="n">Qadd</span><span class="p">,</span><span class="n">Sadd</span>      <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">add</span><span class="p">],</span> <span class="n">S</span><span class="p">[</span><span class="n">add</span><span class="p">]</span>
    <span class="n">Qrec</span><span class="p">,</span><span class="n">Srec</span><span class="p">,</span><span class="n">Arec</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">recompute</span><span class="p">],</span> <span class="n">S</span><span class="p">[</span><span class="n">recompute</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="n">recompute</span><span class="p">]</span>
    <span class="n">Qdel</span><span class="p">,</span><span class="n">Sdel</span><span class="p">,</span><span class="n">Adel</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">delete</span><span class="p">],</span> <span class="n">S</span><span class="p">[</span><span class="n">delete</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="n">delete</span><span class="p">]</span>

    <span class="c1"># compute new alpha&#39;s (precision parameters) for features that are </span>
    <span class="c1"># currently in model and will be recomputed</span>
    <span class="n">Anew</span>           <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">recompute</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span> <span class="p">(</span> <span class="n">theta</span><span class="p">[</span><span class="n">recompute</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
    <span class="n">delta_alpha</span>    <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">Anew</span> <span class="o">-</span> <span class="mf">1.</span><span class="o">/</span><span class="n">Arec</span><span class="p">)</span>

    <span class="c1"># compute change in log marginal likelihood </span>
    <span class="n">deltaL</span><span class="p">[</span><span class="n">add</span><span class="p">]</span>       <span class="o">=</span> <span class="p">(</span> <span class="n">Qadd</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">Sadd</span> <span class="p">)</span> <span class="o">/</span> <span class="n">Sadd</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Sadd</span><span class="o">/</span><span class="n">Qadd</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
    <span class="n">deltaL</span><span class="p">[</span><span class="n">recompute</span><span class="p">]</span> <span class="o">=</span> <span class="n">Qrec</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">Srec</span> <span class="o">+</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">delta_alpha</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">Srec</span><span class="o">*</span><span class="n">delta_alpha</span><span class="p">)</span>
    <span class="n">deltaL</span><span class="p">[</span><span class="n">delete</span><span class="p">]</span>    <span class="o">=</span> <span class="n">Qdel</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">Sdel</span> <span class="o">-</span> <span class="n">Adel</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Sdel</span> <span class="o">/</span> <span class="n">Adel</span><span class="p">)</span>
    <span class="n">deltaL</span>            <span class="o">=</span> <span class="n">deltaL</span>  <span class="o">/</span> <span class="n">n_samples</span>

    <span class="c1"># find feature which caused largest change in likelihood</span>
    <span class="n">feature_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">deltaL</span><span class="p">)</span>

    <span class="c1"># no deletions or additions</span>
    <span class="n">same_features</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="n">theta</span><span class="p">[</span><span class="o">~</span><span class="n">recompute</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="c1"># changes in precision for features already in model is below threshold</span>
    <span class="n">no_delta</span>       <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="nb">abs</span><span class="p">(</span> <span class="n">Anew</span> <span class="o">-</span> <span class="n">Arec</span> <span class="p">)</span> <span class="o">&gt;</span> <span class="n">tol</span> <span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="c1"># check convergence: if no features to add or delete and small change in </span>
    <span class="c1">#                    precision for current features then terminate</span>
    <span class="n">converged</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">same_features</span> <span class="ow">and</span> <span class="n">no_delta</span><span class="p">:</span>
        <span class="n">converged</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span><span class="n">converged</span><span class="p">]</span>

    <span class="c1"># if not converged update precision parameter of weights and return</span>
    <span class="k">if</span> <span class="n">theta</span><span class="p">[</span><span class="n">feature_index</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">A</span><span class="p">[</span><span class="n">feature_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">feature_index</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">theta</span><span class="p">[</span><span class="n">feature_index</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">active</span><span class="p">[</span><span class="n">feature_index</span><span class="p">]</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">active</span><span class="p">[</span><span class="n">feature_index</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># at least two active features</span>
        <span class="k">if</span> <span class="n">active</span><span class="p">[</span><span class="n">feature_index</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">active</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># do not remove bias term in classification </span>
            <span class="c1"># (in regression it is factored in through centering)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">feature_index</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">clf_bias</span><span class="p">):</span>
               <span class="n">active</span><span class="p">[</span><span class="n">feature_index</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
               <span class="n">A</span><span class="p">[</span><span class="n">feature_index</span><span class="p">]</span>      <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">PINF</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span><span class="n">converged</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="shapleyx.xsampler" class="doc doc-heading">
            <code>shapleyx.xsampler</code>


</h3>

    <div class="doc doc-contents first">









  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="shapleyx.xsampler.xsampler" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">xsampler</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">ranges</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Generate a Latin Hypercube sample scaled to the specified ranges.</p>
<p>Args:
    num_samples (int): Number of samples to generate.
    ranges (dict): A dictionary where keys are feature names and values are tuples of (lower, upper) bounds.</p>
<p>Returns:
    np.ndarray: A scaled Latin Hypercube sample of shape (num_samples, num_features).</p>


            <details class="quote">
              <summary>Source code in <code>shapleyx\xsampler.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">xsampler</span><span class="p">(</span><span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">ranges</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate a Latin Hypercube sample scaled to the specified ranges.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_samples (int): Number of samples to generate.</span>
<span class="sd">        ranges (dict): A dictionary where keys are feature names and values are tuples of (lower, upper) bounds.</span>

<span class="sd">    Returns:</span>
<span class="sd">        np.ndarray: A scaled Latin Hypercube sample of shape (num_samples, num_features).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_features</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ranges</span><span class="p">)</span>

    <span class="c1"># Extract lower and upper bounds from the ranges dictionary</span>
    <span class="n">lower_bounds</span> <span class="o">=</span> <span class="p">[</span><span class="n">bounds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">bounds</span> <span class="ow">in</span> <span class="n">ranges</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
    <span class="n">upper_bounds</span> <span class="o">=</span> <span class="p">[</span><span class="n">bounds</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">bounds</span> <span class="ow">in</span> <span class="n">ranges</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>

    <span class="c1"># Generate Latin Hypercube sample</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="n">qmc</span><span class="o">.</span><span class="n">LatinHypercube</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="n">num_features</span><span class="p">)</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">num_samples</span><span class="p">)</span>

    <span class="c1"># Scale the sample to the specified ranges</span>
    <span class="n">sample_scaled</span> <span class="o">=</span> <span class="n">qmc</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">lower_bounds</span><span class="p">,</span> <span class="n">upper_bounds</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">sample_scaled</span> 
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.indexes", "navigation.sections", "navigation.top", "toc.integrate", "content.tabs.link"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>